{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Hyperparameters for Pretraining of NeuMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to pick:\n",
    "- layers\n",
    "- reg_mf\n",
    "- reg_layers\n",
    "- learning rate \n",
    "\n",
    "- batch-size \n",
    "\n",
    "Use library hyperas to carry out the parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "seed(1)\n",
    "set_random_seed(1)\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Input, Dense, merge, Flatten, concatenate, multiply, dot, Reshape, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "import keras.callbacks\n",
    "from time import time\n",
    "import pdb\n",
    "from scipy import sparse\n",
    "\n",
    "import sqlalchemy as db\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from matplotlib import pyplot as plt\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions for data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    data_bundle = pd.read_pickle(\"data/training_data/sample_100000.pkl\")\n",
    "    max_length = 200\n",
    "    #data_bundle = data_bundle.dropna().reset_index()\n",
    "    # context\n",
    "    month = np.nan_to_num(data_bundle.month_enc.values.reshape(-1,1))\n",
    "    days_online = np.nan_to_num(data_bundle.days_online_log_std.values.reshape(-1,1))#, dtype = \"float32\")\n",
    "    # item\n",
    "    item_anbieter = np.nan_to_num(data_bundle.anbieterid_enc.values.reshape(-1,1)) # nur weil es noch Nan gab - fix ich noch\n",
    "    item_mkt= np.nan_to_num(data_bundle.anbietermarktplatz_enc.values.reshape(-1,1))\n",
    "    item_wg = np.nan_to_num(data_bundle.warengruppe_enc.values.reshape(-1,1))\n",
    "    item_preis = np.nan_to_num(data_bundle.preis_log_std.values.reshape(-1,1))\n",
    "    item_ve = np.nan_to_num(data_bundle.minve_log_std.values.reshape(-1,1))\n",
    "    list_text = []\n",
    "    list_text_user = []\n",
    "    for i in range(len(data_bundle)):\n",
    "        list_text.append(data_bundle.text_vec[i])\n",
    "        list_text_user.append(data_bundle.text_vec_user[i])\n",
    "    item_text = np.array(list_text, ndmin = 2)\n",
    "\n",
    "    # user\n",
    "    user_text = np.array(list_text_user, ndmin = 2)\n",
    "    user_mkt = np.nan_to_num(data_bundle.usermkt_enc.values.reshape(-1,1))\n",
    "\n",
    "    user_anbieter = pad_sequences(data_bundle.anbieterid_enc_user, maxlen = max_length, padding = \"pre\")\n",
    "    user_anbietermkt = pad_sequences(data_bundle.anbietermarktplatz_enc_user, maxlen = max_length, padding = \"pre\")\n",
    "    user_wg = pad_sequences(data_bundle.warengruppe_enc_user, maxlen = max_length, padding = \"pre\")\n",
    "    user_preis = np.nan_to_num(data_bundle.preis_log_std_user.values.reshape(-1,1))\n",
    "    user_ve = np.nan_to_num(data_bundle.minve_log_std_user.values.reshape(-1,1))\n",
    "    \n",
    "    # features \n",
    "    x_train = [month[:90000], days_online[:90000], item_anbieter[:90000], item_mkt[:90000], item_wg[:90000], item_preis[:90000], item_ve[:90000], item_text[:90000], user_mkt[:90000], user_anbietermkt[:90000], user_wg[:90000], user_anbieter[:90000], user_preis[:90000], user_ve[:90000], user_text[:90000]]\n",
    "    x_test = [month[90000:], days_online[90000:], item_anbieter[90000:], item_mkt[90000:], item_wg[90000:], item_preis[90000:], item_ve[90000:], item_text[90000:], user_mkt[90000:], user_anbietermkt[90000:], user_wg[90000:], user_anbieter[90000:], user_preis[90000:], user_ve[90000:], user_text[90000:]]\n",
    "    # target\n",
    "    y_train = data_bundle.pick.values.reshape(-1,1)[:90000]\n",
    "    y_test = data_bundle.pick.values.reshape(-1,1)[90000:]\n",
    "\n",
    "    return (x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changed input in here - results of all runs are at the end of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    layers = {{choice([[128,64,32,16] , [256, 128,64,32], [512, 256, 128,64]])}}\n",
    "    reg_mlp = 0.001\n",
    "    reg_lay = 0.001\n",
    "    reg_layers = [reg_mlp, reg_lay, reg_lay, reg_lay]\n",
    "    reg_mf = 0.001 \n",
    "    \n",
    "    def mask_aware_mean(x):\n",
    "    # recreate the masks - all zero rows have been masked\n",
    "        mask = K.not_equal(K.sum(K.abs(x), axis=2, keepdims=True), 0)\n",
    "        # number of that rows are not all zeros\n",
    "        n = K.sum(K.cast(mask, 'float32'), axis=1, keepdims=False)\n",
    "        # compute mask-aware mean of x\n",
    "        x_mean = K.sum(x, axis=1, keepdims=False) / n\n",
    "        return x_mean \n",
    "\n",
    "    lrelu = lambda x: tf.keras.activations.relu(x, alpha=0.1)\n",
    "    \n",
    "    def get_model(layers, reg_layers, reg_mf):\n",
    "        assert len(layers) == len(reg_layers)\n",
    "        num_layer = len(layers) #Number of layers in the MLP\n",
    "        ### Input variables\n",
    "        max_length = 200\n",
    "        months = 13\n",
    "        supplier = 563\n",
    "        wgs = 230\n",
    "        mkt = 9\n",
    "\n",
    "        months_emb = round(months ** 0.25)\n",
    "        supplier_emb = round(supplier ** 0.25)\n",
    "        wgs_emb = round(wgs ** 0.25)\n",
    "        mkt_emb = round(mkt ** 0.25)\n",
    "        \n",
    "        # Context\n",
    "        month = Input(shape = (1,), dtype = \"float32\", name = \"month\")\n",
    "        day_online = Input(shape = (1,), dtype = \"float32\", name = \"days_online\")\n",
    "        # Item\n",
    "        item_anbieter = Input(shape = (1,), dtype = \"float32\", name = \"item_anbieter\")\n",
    "        item_mkt = Input(shape = (1,), dtype = \"float32\", name = \"item_mkt\")\n",
    "        item_wg = Input(shape = (1,), dtype = \"float32\", name = \"item_wg\")\n",
    "        item_preis = Input(shape = (1,), dtype = \"float32\", name = \"item_preis\")\n",
    "        item_ve = Input(shape = (1,), dtype = \"float32\", name = \"item_ve\")\n",
    "        item_text = Input(shape = (150,), dtype = \"float32\", name = \"item_text\")\n",
    "\n",
    "        # User\n",
    "        user_mkt = Input(shape = (1,), dtype = \"float32\", name = \"user_mkt\")\n",
    "        user_anbieter = Input(shape = (max_length,), dtype = \"float32\", name = \"user_anbieter\")\n",
    "        user_anbietermkt = Input(shape = (max_length,), dtype = \"float32\", name = \"user_anbietermkt\")\n",
    "        user_wg = Input(shape = (max_length,), dtype = \"float32\", name = \"user_wg\")\n",
    "        user_preis = Input(shape = (1,), dtype = \"float32\", name = \"user_preis\")\n",
    "        user_ve = Input(shape = (1,), dtype = \"float32\", name = \"user_ve\")\n",
    "        user_text = Input(shape = (150,), dtype = \"float32\", name = \"user_text\")\n",
    "\n",
    "\n",
    "\n",
    "        ### Embedding layer\n",
    "        # MF\n",
    "        # Context\n",
    "        MF_Embedding_Context_month = Embedding(input_dim=months, output_dim=months_emb, name='mf_embedding_month',\n",
    "                                      embeddings_initializer=initializers.random_normal(),\n",
    "                                      embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "\n",
    "        # Item\n",
    "\n",
    "        MF_Embedding_Item_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mf_embedding_item_anbieter',\n",
    "                                      embeddings_initializer=initializers.random_normal(),\n",
    "                                      embeddings_regularizer=l2(reg_mf),input_length=1)\n",
    "        MF_Embedding_Item_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mf_embedding_item_mkt',\n",
    "                                      embeddings_initializer=initializers.random_normal(),\n",
    "                                      embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "\n",
    "        MF_Embedding_Item_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mf_embedding_item_wg',\n",
    "                                      embeddings_initializer=initializers.random_normal(),\n",
    "                                      embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "\n",
    "\n",
    "        # User\n",
    "\n",
    "        MF_Embedding_User_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mf_embedding_user_mkt',\n",
    "                                          embeddings_initializer=initializers.random_normal(),\n",
    "                                          embeddings_regularizer=l2(reg_mf),\n",
    "                                          #mask_zero = True,\n",
    "                                          input_length=1)\n",
    "        MF_Embedding_User_anbietermkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mf_embedding_user_anbietermkt',\n",
    "                                                  embeddings_initializer=initializers.random_normal(),\n",
    "                                                  embeddings_regularizer=l2(reg_mf),\n",
    "                                                  mask_zero = True,\n",
    "                                                  input_length=max_length)\n",
    "        MF_Embedding_User_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mf_embedding_user_wg',\n",
    "                                         embeddings_initializer=initializers.random_normal(),\n",
    "                                         embeddings_regularizer=l2(reg_mf),\n",
    "                                         mask_zero = True,\n",
    "                                         input_length=max_length)\n",
    "        MF_Embedding_User_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mf_embedding_user_anbieter',\n",
    "                                               embeddings_initializer=initializers.random_normal(),\n",
    "                                               embeddings_regularizer=l2(reg_mf),\n",
    "                                               mask_zero = True,\n",
    "                                               input_length=max_length)\n",
    "\n",
    "\n",
    "        # MLP\n",
    "        #Context\n",
    "\n",
    "        MLP_Embedding_Context_month = Embedding(input_dim=months, output_dim=months_emb, name='mlp_embedding_month',\n",
    "                                      embeddings_initializer=initializers.random_normal(),\n",
    "                                      embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
    "\n",
    "\n",
    "        # Item\n",
    "\n",
    "        MLP_Embedding_Item_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mlp_embedding_item_anbieter',\n",
    "                                      embeddings_initializer=initializers.random_normal(),\n",
    "                                      embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
    "        MLP_Embedding_Item_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mlp_embedding_item_mkt',\n",
    "                                      embeddings_initializer=initializers.random_normal(),\n",
    "                                      embeddings_regularizer=l2(reg_layers[0]),  \n",
    "                                           input_length=1)\n",
    "\n",
    "        MLP_Embedding_Item_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mlp_embedding_item_wg',\n",
    "                                      embeddings_initializer=initializers.random_normal(),\n",
    "                                      embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
    "\n",
    "\n",
    "        # User\n",
    "\n",
    "        MLP_Embedding_User_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mlp_embedding_user_mkt',\n",
    "                                          embeddings_initializer=initializers.random_normal(),\n",
    "                                          embeddings_regularizer=l2(reg_layers[0]),\n",
    "                                          #mask_zero = True,\n",
    "                                          input_length=1)\n",
    "        MLP_Embedding_User_anbietermkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mlp_embedding_user_anbietermkt',\n",
    "                                                  embeddings_initializer=initializers.random_normal(),\n",
    "                                                  embeddings_regularizer=l2(reg_layers[0]),\n",
    "                                                  mask_zero = True,\n",
    "                                                  input_length=max_length)\n",
    "        MLP_Embedding_User_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mlp_embedding_user_wg',\n",
    "                                         embeddings_initializer=initializers.random_normal(),\n",
    "                                         embeddings_regularizer=l2(reg_layers[0]),\n",
    "                                         mask_zero = True,\n",
    "                                         input_length=max_length)\n",
    "        MLP_Embedding_User_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mlp_embedding_user_anbieter',\n",
    "                                               embeddings_initializer=initializers.random_normal(),\n",
    "                                               embeddings_regularizer=l2(reg_layers[0]),\n",
    "                                               #mask_zero = True,\n",
    "                                               input_length=max_length)\n",
    "\n",
    "        ## all user ones zb. and then concenate / multiply in MF \n",
    "        # MF part\n",
    "        ## context \n",
    "        emb_item_month = Flatten(name = \"mf_flat_month\")(MF_Embedding_Context_month(month))\n",
    "\n",
    "        ## user\n",
    "        emb_user_mkt = Flatten(name =\"mf_flat_user_mkt\")(MF_Embedding_User_mkt(user_mkt))\n",
    "        #     emb_user_anbietermkt = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mf_avg_user_anbietermkt\"))(MF_Embedding_User_anbietermkt(user_anbietermkt))\n",
    "        emb_user_wg = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mf_avg_user_wg\"))(MF_Embedding_User_wg(user_wg))\n",
    "        emb_user_anbieter = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mf_avg_user_anbieter\"))(MF_Embedding_User_anbieter(user_anbieter))\n",
    "\n",
    "        ## item\n",
    "        emb_item_anbieter = Flatten(name = \"mf_flat_item_anbieter\")(MF_Embedding_Item_anbieter(item_anbieter))\n",
    "        emb_item_mkt = Flatten(name = \"mf_flat_item_mkt\")(MF_Embedding_Item_mkt(item_mkt))\n",
    "        emb_item_wg = Flatten(name = \"mf_flat_item_wg\")(MF_Embedding_Item_wg(item_wg))\n",
    "\n",
    "        # MF connect\n",
    "        mf_vector = multiply([concatenate([emb_user_mkt, emb_user_wg, emb_user_anbieter, user_preis, user_ve, user_text,\n",
    "                                           day_online, emb_item_month], name = \"mf_user\"), \n",
    "                              concatenate([emb_item_mkt, emb_item_wg, emb_item_anbieter, item_preis, item_ve, item_text,\n",
    "                                           day_online, emb_item_month], name = \"mf_item\")], \n",
    "                             name = \"mf_multiply\")\n",
    "\n",
    "\n",
    "        # MLP part\n",
    "        ## context \n",
    "        emb_item_month2 = Flatten(name = \"mlp_flat_month\")(MLP_Embedding_Context_month(month))\n",
    "        ## user\n",
    "        emb_user_mkt2 = Flatten(name =\"mlp_flat_user_mkt\")(MLP_Embedding_User_mkt(user_mkt))\n",
    "        emb_user_anbietermkt2 = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mlp_avg_user_anbietermkt\"))(MLP_Embedding_User_anbietermkt(user_anbietermkt))\n",
    "        emb_user_wg2 = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mlp_avg_user_wg\"))(MLP_Embedding_User_wg(user_wg))\n",
    "        emb_user_anbieter2 = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mlp_avg_user_anbieten\"))(MLP_Embedding_User_anbieter(user_anbieter))\n",
    "\n",
    "        ## item\n",
    "        emb_item_anbieter2 = Flatten(name = \"mlp_flat_item_anbieter\")(MLP_Embedding_Item_anbieter(item_anbieter))\n",
    "        emb_item_mkt2 = Flatten(name = \"mlp_flat_item_mkt\")(MLP_Embedding_Item_mkt(item_mkt))\n",
    "        emb_item_wg2 = Flatten(name = \"mlp_flat_item_wg\")(MLP_Embedding_Item_wg(item_wg))\n",
    "\n",
    "        mlp_vector = concatenate([emb_item_month2, day_online,\n",
    "                                  emb_user_mkt2, emb_user_anbietermkt2, emb_user_wg2, emb_user_anbieter2, user_preis, user_ve, user_text,\n",
    "                                  emb_item_anbieter2, emb_item_mkt2, emb_item_wg2, item_preis, item_ve, item_text], name = \"mlp_conc\")\n",
    "\n",
    "        for idx in range(1, num_layer):\n",
    "            layer = Dense(layers[idx], kernel_regularizer=l2(reg_layers[idx]), activation=lrelu, name=\"layer%d\" % idx)\n",
    "            mlp_vector = layer(mlp_vector)\n",
    "\n",
    "        # Concatenate MF and MLP parts\n",
    "        predict_vector = concatenate([mf_vector, mlp_vector])\n",
    "\n",
    "        # Final prediction layer\n",
    "        prediction = Dense(1, activation='sigmoid', kernel_initializer=initializers.lecun_normal(),\n",
    "                           name=\"prediction\")(predict_vector)\n",
    "\n",
    "        model_ = Model(inputs=[month, day_online,\n",
    "                               item_anbieter, item_mkt, item_wg, item_preis, item_ve, user_text,\n",
    "                               user_mkt, user_anbietermkt, user_wg, user_anbieter, user_preis, user_ve, item_text],\n",
    "                       outputs=prediction)\n",
    "\n",
    "        return model_\n",
    "\n",
    "    \n",
    "    # load functino to evaluate performance      \n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    def auroc(y_true, y_pred):\n",
    "        return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "                  \n",
    "\n",
    "\n",
    "    # create Model \n",
    "    \n",
    "    if 'results' not in globals():\n",
    "        global results\n",
    "        results = []\n",
    "        \n",
    "    model = get_model(layers, reg_layers, reg_mf)\n",
    "    model.compile(optimizer=Adam(lr={{loguniform(-9, -2)}}), loss='binary_crossentropy', metrics = [auroc])\n",
    "    result = model.fit(x_train, y_train, batch_size = {{choice([128, 256, 512])}}, epochs = 10, verbose = 2, validation_split=0.1)\n",
    "    validation_auc =np.amax(result.history['val_auroc'])\n",
    "    \n",
    "    valLoss = result.history['val_loss'][-1]\n",
    "    parameters = space\n",
    "    parameters[\"val_loss\"] = valLoss\n",
    "    parameters[\"best_val_auc\"] = validation_auc\n",
    "    results.append(parameters)\n",
    "    with open(\"data/models/tuning/test_5.pkl\", 'wb') as fp:\n",
    "        pickle.dump(results, fp)\n",
    "    print('Best validation auc of epoch:', validation_auc)\n",
    "    return {'loss': -validation_auc, 'status': STATUS_OK, 'model': model} # minimizes based on validation_auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine best Hyperparamter Combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look in outputs to see which parameters were compared and which results could be achieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from numpy.random import seed\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow import set_random_seed\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pickle\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.sequence import pad_sequences\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import initializers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Embedding, Input, Dense, merge, Flatten, concatenate, multiply, dot, Reshape, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras.callbacks\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from time import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pdb\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from scipy import sparse\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sqlalchemy as db\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sqlalchemy import create_engine\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import psycopg2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from matplotlib import pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import roc_auc_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from matplotlib import pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'layers': hp.choice('layers', [[128,64,32,16] , [256, 128,64,32], [512, 256, 128,64]]),\n",
      "        'lr': hp.loguniform('lr', -9, -2),\n",
      "        'batch_size': hp.choice('batch_size', [128, 256, 512]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: data_bundle = pd.read_pickle(\"training_data/sample_100000.pkl\")\n",
      "   3: max_length = 200\n",
      "   4: #data_bundle = data_bundle.dropna().reset_index()\n",
      "   5: # context\n",
      "   6: month = np.nan_to_num(data_bundle.month_enc.values.reshape(-1,1))\n",
      "   7: days_online = np.nan_to_num(data_bundle.days_online_log_std.values.reshape(-1,1))#, dtype = \"float32\")\n",
      "   8: # item\n",
      "   9: item_anbieter = np.nan_to_num(data_bundle.anbieterid_enc.values.reshape(-1,1)) # nur weil es noch Nan gab - fix ich noch\n",
      "  10: item_mkt= np.nan_to_num(data_bundle.anbietermarktplatz_enc.values.reshape(-1,1))\n",
      "  11: item_wg = np.nan_to_num(data_bundle.warengruppe_enc.values.reshape(-1,1))\n",
      "  12: item_preis = np.nan_to_num(data_bundle.preis_log_std.values.reshape(-1,1))\n",
      "  13: item_ve = np.nan_to_num(data_bundle.minve_log_std.values.reshape(-1,1))\n",
      "  14: list_text = []\n",
      "  15: list_text_user = []\n",
      "  16: for i in range(len(data_bundle)):\n",
      "  17:     list_text.append(data_bundle.text_vec[i])\n",
      "  18:     list_text_user.append(data_bundle.text_vec_user[i])\n",
      "  19: item_text = np.array(list_text, ndmin = 2)\n",
      "  20: \n",
      "  21: # user\n",
      "  22: user_text = np.array(list_text_user, ndmin = 2)\n",
      "  23: user_mkt = np.nan_to_num(data_bundle.usermkt_enc.values.reshape(-1,1))\n",
      "  24: \n",
      "  25: user_anbieter = pad_sequences(data_bundle.anbieterid_enc_user, maxlen = max_length, padding = \"pre\")\n",
      "  26: user_anbietermkt = pad_sequences(data_bundle.anbietermarktplatz_enc_user, maxlen = max_length, padding = \"pre\")\n",
      "  27: user_wg = pad_sequences(data_bundle.warengruppe_enc_user, maxlen = max_length, padding = \"pre\")\n",
      "  28: user_preis = np.nan_to_num(data_bundle.preis_log_std_user.values.reshape(-1,1))\n",
      "  29: user_ve = np.nan_to_num(data_bundle.minve_log_std_user.values.reshape(-1,1))\n",
      "  30: \n",
      "  31: # features \n",
      "  32: x_train = [month[:90000], days_online[:90000], item_anbieter[:90000], item_mkt[:90000], item_wg[:90000], item_preis[:90000], item_ve[:90000], item_text[:90000], user_mkt[:90000], user_anbietermkt[:90000], user_wg[:90000], user_anbieter[:90000], user_preis[:90000], user_ve[:90000], user_text[:90000]]\n",
      "  33: x_test = [month[90000:], days_online[90000:], item_anbieter[90000:], item_mkt[90000:], item_wg[90000:], item_preis[90000:], item_ve[90000:], item_text[90000:], user_mkt[90000:], user_anbietermkt[90000:], user_wg[90000:], user_anbieter[90000:], user_preis[90000:], user_ve[90000:], user_text[90000:]]\n",
      "  34: # target\n",
      "  35: y_train = data_bundle.pick.values.reshape(-1,1)[:90000]\n",
      "  36: y_test = data_bundle.pick.values.reshape(-1,1)[90000:]\n",
      "  37: \n",
      "  38: \n",
      "  39: \n",
      "  40: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "    1: def keras_fmin_fnct(space):\n",
      "    2: \n",
      "    3:     layers = space['layers']\n",
      "    4:     reg_mlp = 0.001\n",
      "    5:     reg_lay = 0.001\n",
      "    6:     reg_layers = [reg_mlp, reg_lay, reg_lay, reg_lay]\n",
      "    7:     reg_mf = 0.001 \n",
      "    8:     \n",
      "    9:     def mask_aware_mean(x):\n",
      "   10:     # recreate the masks - all zero rows have been masked\n",
      "   11:         mask = K.not_equal(K.sum(K.abs(x), axis=2, keepdims=True), 0)\n",
      "   12:         # number of that rows are not all zeros\n",
      "   13:         n = K.sum(K.cast(mask, 'float32'), axis=1, keepdims=False)\n",
      "   14:         # compute mask-aware mean of x\n",
      "   15:         x_mean = K.sum(x, axis=1, keepdims=False) / n\n",
      "   16:         return x_mean \n",
      "   17: \n",
      "   18:     lrelu = lambda x: tf.keras.activations.relu(x, alpha=0.1)\n",
      "   19:     \n",
      "   20:     def get_model(layers, reg_layers, reg_mf):\n",
      "   21:         assert len(layers) == len(reg_layers)\n",
      "   22:         num_layer = len(layers) #Number of layers in the MLP\n",
      "   23:         ### Input variables\n",
      "   24:         max_length = 200\n",
      "   25:         months = 13\n",
      "   26:         supplier = 563\n",
      "   27:         wgs = 230\n",
      "   28:         mkt = 9\n",
      "   29: \n",
      "   30:         months_emb = round(months ** 0.25)\n",
      "   31:         supplier_emb = round(supplier ** 0.25)\n",
      "   32:         wgs_emb = round(wgs ** 0.25)\n",
      "   33:         mkt_emb = round(mkt ** 0.25)\n",
      "   34:         \n",
      "   35:         # Context\n",
      "   36:         month = Input(shape = (1,), dtype = \"float32\", name = \"month\")\n",
      "   37:         day_online = Input(shape = (1,), dtype = \"float32\", name = \"days_online\")\n",
      "   38:         # Item\n",
      "   39:         item_anbieter = Input(shape = (1,), dtype = \"float32\", name = \"item_anbieter\")\n",
      "   40:         item_mkt = Input(shape = (1,), dtype = \"float32\", name = \"item_mkt\")\n",
      "   41:         item_wg = Input(shape = (1,), dtype = \"float32\", name = \"item_wg\")\n",
      "   42:         item_preis = Input(shape = (1,), dtype = \"float32\", name = \"item_preis\")\n",
      "   43:         item_ve = Input(shape = (1,), dtype = \"float32\", name = \"item_ve\")\n",
      "   44:         item_text = Input(shape = (150,), dtype = \"float32\", name = \"item_text\")\n",
      "   45: \n",
      "   46:         # User\n",
      "   47:         user_mkt = Input(shape = (1,), dtype = \"float32\", name = \"user_mkt\")\n",
      "   48:         user_anbieter = Input(shape = (max_length,), dtype = \"float32\", name = \"user_anbieter\")\n",
      "   49:         user_anbietermkt = Input(shape = (max_length,), dtype = \"float32\", name = \"user_anbietermkt\")\n",
      "   50:         user_wg = Input(shape = (max_length,), dtype = \"float32\", name = \"user_wg\")\n",
      "   51:         user_preis = Input(shape = (1,), dtype = \"float32\", name = \"user_preis\")\n",
      "   52:         user_ve = Input(shape = (1,), dtype = \"float32\", name = \"user_ve\")\n",
      "   53:         user_text = Input(shape = (150,), dtype = \"float32\", name = \"user_text\")\n",
      "   54: \n",
      "   55: \n",
      "   56: \n",
      "   57:         ### Embedding layer\n",
      "   58:         # MF\n",
      "   59:         # Context\n",
      "   60:         MF_Embedding_Context_month = Embedding(input_dim=months, output_dim=months_emb, name='mf_embedding_month',\n",
      "   61:                                       embeddings_initializer=initializers.random_normal(),\n",
      "   62:                                       embeddings_regularizer=l2(reg_mf), input_length=1)\n",
      "   63: \n",
      "   64:         # Item\n",
      "   65: \n",
      "   66:         MF_Embedding_Item_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mf_embedding_item_anbieter',\n",
      "   67:                                       embeddings_initializer=initializers.random_normal(),\n",
      "   68:                                       embeddings_regularizer=l2(reg_mf),input_length=1)\n",
      "   69:         MF_Embedding_Item_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mf_embedding_item_mkt',\n",
      "   70:                                       embeddings_initializer=initializers.random_normal(),\n",
      "   71:                                       embeddings_regularizer=l2(reg_mf), input_length=1)\n",
      "   72: \n",
      "   73:         MF_Embedding_Item_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mf_embedding_item_wg',\n",
      "   74:                                       embeddings_initializer=initializers.random_normal(),\n",
      "   75:                                       embeddings_regularizer=l2(reg_mf), input_length=1)\n",
      "   76: \n",
      "   77: \n",
      "   78:         # User\n",
      "   79: \n",
      "   80:         MF_Embedding_User_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mf_embedding_user_mkt',\n",
      "   81:                                           embeddings_initializer=initializers.random_normal(),\n",
      "   82:                                           embeddings_regularizer=l2(reg_mf),\n",
      "   83:                                           #mask_zero = True,\n",
      "   84:                                           input_length=1)\n",
      "   85:         MF_Embedding_User_anbietermkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mf_embedding_user_anbietermkt',\n",
      "   86:                                                   embeddings_initializer=initializers.random_normal(),\n",
      "   87:                                                   embeddings_regularizer=l2(reg_mf),\n",
      "   88:                                                   mask_zero = True,\n",
      "   89:                                                   input_length=max_length)\n",
      "   90:         MF_Embedding_User_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mf_embedding_user_wg',\n",
      "   91:                                          embeddings_initializer=initializers.random_normal(),\n",
      "   92:                                          embeddings_regularizer=l2(reg_mf),\n",
      "   93:                                          mask_zero = True,\n",
      "   94:                                          input_length=max_length)\n",
      "   95:         MF_Embedding_User_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mf_embedding_user_anbieter',\n",
      "   96:                                                embeddings_initializer=initializers.random_normal(),\n",
      "   97:                                                embeddings_regularizer=l2(reg_mf),\n",
      "   98:                                                mask_zero = True,\n",
      "   99:                                                input_length=max_length)\n",
      "  100: \n",
      "  101: \n",
      "  102:         # MLP\n",
      "  103:         #Context\n",
      "  104: \n",
      "  105:         MLP_Embedding_Context_month = Embedding(input_dim=months, output_dim=months_emb, name='mlp_embedding_month',\n",
      "  106:                                       embeddings_initializer=initializers.random_normal(),\n",
      "  107:                                       embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
      "  108: \n",
      "  109: \n",
      "  110:         # Item\n",
      "  111: \n",
      "  112:         MLP_Embedding_Item_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mlp_embedding_item_anbieter',\n",
      "  113:                                       embeddings_initializer=initializers.random_normal(),\n",
      "  114:                                       embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
      "  115:         MLP_Embedding_Item_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mlp_embedding_item_mkt',\n",
      "  116:                                       embeddings_initializer=initializers.random_normal(),\n",
      "  117:                                       embeddings_regularizer=l2(reg_layers[0]),  \n",
      "  118:                                            input_length=1)\n",
      "  119: \n",
      "  120:         MLP_Embedding_Item_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mlp_embedding_item_wg',\n",
      "  121:                                       embeddings_initializer=initializers.random_normal(),\n",
      "  122:                                       embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
      "  123: \n",
      "  124: \n",
      "  125:         # User\n",
      "  126: \n",
      "  127:         MLP_Embedding_User_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mlp_embedding_user_mkt',\n",
      "  128:                                           embeddings_initializer=initializers.random_normal(),\n",
      "  129:                                           embeddings_regularizer=l2(reg_layers[0]),\n",
      "  130:                                           #mask_zero = True,\n",
      "  131:                                           input_length=1)\n",
      "  132:         MLP_Embedding_User_anbietermkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mlp_embedding_user_anbietermkt',\n",
      "  133:                                                   embeddings_initializer=initializers.random_normal(),\n",
      "  134:                                                   embeddings_regularizer=l2(reg_layers[0]),\n",
      "  135:                                                   mask_zero = True,\n",
      "  136:                                                   input_length=max_length)\n",
      "  137:         MLP_Embedding_User_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mlp_embedding_user_wg',\n",
      "  138:                                          embeddings_initializer=initializers.random_normal(),\n",
      "  139:                                          embeddings_regularizer=l2(reg_layers[0]),\n",
      "  140:                                          mask_zero = True,\n",
      "  141:                                          input_length=max_length)\n",
      "  142:         MLP_Embedding_User_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mlp_embedding_user_anbieter',\n",
      "  143:                                                embeddings_initializer=initializers.random_normal(),\n",
      "  144:                                                embeddings_regularizer=l2(reg_layers[0]),\n",
      "  145:                                                #mask_zero = True,\n",
      "  146:                                                input_length=max_length)\n",
      "  147: \n",
      "  148:         ## all user ones zb. and then concenate / multiply in MF \n",
      "  149:         # MF part\n",
      "  150:         ## context \n",
      "  151:         emb_item_month = Flatten(name = \"mf_flat_month\")(MF_Embedding_Context_month(month))\n",
      "  152: \n",
      "  153:         ## user\n",
      "  154:         emb_user_mkt = Flatten(name =\"mf_flat_user_mkt\")(MF_Embedding_User_mkt(user_mkt))\n",
      "  155:         #     emb_user_anbietermkt = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mf_avg_user_anbietermkt\"))(MF_Embedding_User_anbietermkt(user_anbietermkt))\n",
      "  156:         emb_user_wg = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mf_avg_user_wg\"))(MF_Embedding_User_wg(user_wg))\n",
      "  157:         emb_user_anbieter = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mf_avg_user_anbieter\"))(MF_Embedding_User_anbieter(user_anbieter))\n",
      "  158: \n",
      "  159:         ## item\n",
      "  160:         emb_item_anbieter = Flatten(name = \"mf_flat_item_anbieter\")(MF_Embedding_Item_anbieter(item_anbieter))\n",
      "  161:         emb_item_mkt = Flatten(name = \"mf_flat_item_mkt\")(MF_Embedding_Item_mkt(item_mkt))\n",
      "  162:         emb_item_wg = Flatten(name = \"mf_flat_item_wg\")(MF_Embedding_Item_wg(item_wg))\n",
      "  163: \n",
      "  164:         # MF connect\n",
      "  165:         mf_vector = multiply([concatenate([emb_user_mkt, emb_user_wg, emb_user_anbieter, user_preis, user_ve, user_text,\n",
      "  166:                                            day_online, emb_item_month], name = \"mf_user\"), \n",
      "  167:                               concatenate([emb_item_mkt, emb_item_wg, emb_item_anbieter, item_preis, item_ve, item_text,\n",
      "  168:                                            day_online, emb_item_month], name = \"mf_item\")], \n",
      "  169:                              name = \"mf_multiply\")\n",
      "  170: \n",
      "  171: \n",
      "  172:         # MLP part\n",
      "  173:         ## context \n",
      "  174:         emb_item_month2 = Flatten(name = \"mlp_flat_month\")(MLP_Embedding_Context_month(month))\n",
      "  175:         ## user\n",
      "  176:         emb_user_mkt2 = Flatten(name =\"mlp_flat_user_mkt\")(MLP_Embedding_User_mkt(user_mkt))\n",
      "  177:         emb_user_anbietermkt2 = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mlp_avg_user_anbietermkt\"))(MLP_Embedding_User_anbietermkt(user_anbietermkt))\n",
      "  178:         emb_user_wg2 = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mlp_avg_user_wg\"))(MLP_Embedding_User_wg(user_wg))\n",
      "  179:         emb_user_anbieter2 = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mlp_avg_user_anbieten\"))(MLP_Embedding_User_anbieter(user_anbieter))\n",
      "  180: \n",
      "  181:         ## item\n",
      "  182:         emb_item_anbieter2 = Flatten(name = \"mlp_flat_item_anbieter\")(MLP_Embedding_Item_anbieter(item_anbieter))\n",
      "  183:         emb_item_mkt2 = Flatten(name = \"mlp_flat_item_mkt\")(MLP_Embedding_Item_mkt(item_mkt))\n",
      "  184:         emb_item_wg2 = Flatten(name = \"mlp_flat_item_wg\")(MLP_Embedding_Item_wg(item_wg))\n",
      "  185: \n",
      "  186:         mlp_vector = concatenate([emb_item_month2, day_online,\n",
      "  187:                                   emb_user_mkt2, emb_user_anbietermkt2, emb_user_wg2, emb_user_anbieter2, user_preis, user_ve, user_text,\n",
      "  188:                                   emb_item_anbieter2, emb_item_mkt2, emb_item_wg2, item_preis, item_ve, item_text], name = \"mlp_conc\")\n",
      "  189: \n",
      "  190:         for idx in range(1, num_layer):\n",
      "  191:             layer = Dense(layers[idx], kernel_regularizer=l2(reg_layers[idx]), activation=lrelu, name=\"layer%d\" % idx)\n",
      "  192:             mlp_vector = layer(mlp_vector)\n",
      "  193: \n",
      "  194:         # Concatenate MF and MLP parts\n",
      "  195:         predict_vector = concatenate([mf_vector, mlp_vector])\n",
      "  196: \n",
      "  197:         # Final prediction layer\n",
      "  198:         prediction = Dense(1, activation='sigmoid', kernel_initializer=initializers.lecun_normal(),\n",
      "  199:                            name=\"prediction\")(predict_vector)\n",
      "  200: \n",
      "  201:         model_ = Model(inputs=[month, day_online,\n",
      "  202:                                item_anbieter, item_mkt, item_wg, item_preis, item_ve, user_text,\n",
      "  203:                                user_mkt, user_anbietermkt, user_wg, user_anbieter, user_preis, user_ve, item_text],\n",
      "  204:                        outputs=prediction)\n",
      "  205: \n",
      "  206:         return model_\n",
      "  207: \n",
      "  208:     \n",
      "  209:     # load functino to evaluate performance      \n",
      "  210:     def auroc(y_true, y_pred):\n",
      "  211:         return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)\n",
      "  212:                   \n",
      "  213: \n",
      "  214: \n",
      "  215:     # create Model \n",
      "  216:     \n",
      "  217:     if 'results' not in globals():\n",
      "  218:         global results\n",
      "  219:         results = []\n",
      "  220:         \n",
      "  221:     model = get_model(layers, reg_layers, reg_mf)\n",
      "  222:     model.compile(optimizer=Adam(lr=space['lr']), loss='binary_crossentropy', metrics = [auroc])\n",
      "  223:     result = model.fit(x_train, y_train, batch_size = space['batch_size'], epochs = 10, verbose = 2, validation_split=0.1)\n",
      "  224:     validation_auc =np.amax(result.history['val_auroc'])\n",
      "  225:     \n",
      "  226:     valLoss = result.history['val_loss'][-1]\n",
      "  227:     parameters = space\n",
      "  228:     parameters[\"val_loss\"] = valLoss\n",
      "  229:     parameters[\"best_val_auc\"] = validation_auc\n",
      "  230:     results.append(parameters)\n",
      "  231:     with open(\"models/tuning/test_5.pkl\", 'wb') as fp:\n",
      "  232:         pickle.dump(results, fp)\n",
      "  233:     print('Best validation auc of epoch:', validation_auc)\n",
      "  234:     return {'loss': -validation_auc, 'status': STATUS_OK, 'model': model} # minimizes based on validation_auc\n",
      "  235: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 81000 samples, validate on 9000 samples    \n",
      "Epoch 1/10                                          \n",
      " - 3s - loss: 0.6387 - auroc: 0.6087 - val_loss: 0.5938 - val_auroc: 0.6385\n",
      "\n",
      "Epoch 2/10                                          \n",
      " - 3s - loss: 0.5754 - auroc: 0.6585 - val_loss: 0.5858 - val_auroc: 0.6421\n",
      "\n",
      "Epoch 3/10                                          \n",
      " - 3s - loss: 0.5714 - auroc: 0.6609 - val_loss: 0.5826 - val_auroc: 0.6477\n",
      "\n",
      "Epoch 4/10                                          \n",
      " - 3s - loss: 0.5690 - auroc: 0.6637 - val_loss: 0.5826 - val_auroc: 0.6468\n",
      "\n",
      "Epoch 5/10                                          \n",
      " - 3s - loss: 0.5682 - auroc: 0.6648 - val_loss: 0.5803 - val_auroc: 0.6498\n",
      "\n",
      "Epoch 6/10                                          \n",
      " - 3s - loss: 0.5672 - auroc: 0.6656 - val_loss: 0.5808 - val_auroc: 0.6502\n",
      "\n",
      "Epoch 7/10                                          \n",
      " - 3s - loss: 0.5667 - auroc: 0.6660 - val_loss: 0.5814 - val_auroc: 0.6489\n",
      "\n",
      "Epoch 8/10                                          \n",
      " - 3s - loss: 0.5663 - auroc: 0.6668 - val_loss: 0.5791 - val_auroc: 0.6530\n",
      "\n",
      "Epoch 9/10                                          \n",
      " - 3s - loss: 0.5664 - auroc: 0.6683 - val_loss: 0.5803 - val_auroc: 0.6524\n",
      "\n",
      "Epoch 10/10                                         \n",
      " - 4s - loss: 0.5668 - auroc: 0.6706 - val_loss: 0.5797 - val_auroc: 0.6555\n",
      "\n",
      "Best validation auc of epoch:                       \n",
      "0.6554561853408813                                  \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 8s - loss: 0.6135 - auroc: 0.6406 - val_loss: 0.5859 - val_auroc: 0.6446   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 5s - loss: 0.5727 - auroc: 0.6582 - val_loss: 0.5826 - val_auroc: 0.6484   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 5s - loss: 0.5702 - auroc: 0.6618 - val_loss: 0.5833 - val_auroc: 0.6499   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 8s - loss: 0.5693 - auroc: 0.6646 - val_loss: 0.5809 - val_auroc: 0.6541   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 5s - loss: 0.5679 - auroc: 0.6654 - val_loss: 0.5811 - val_auroc: 0.6538   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 7s - loss: 0.5679 - auroc: 0.6678 - val_loss: 0.5787 - val_auroc: 0.6553   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 5s - loss: 0.5675 - auroc: 0.6695 - val_loss: 0.5812 - val_auroc: 0.6539   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 7s - loss: 0.5667 - auroc: 0.6698 - val_loss: 0.5795 - val_auroc: 0.6569   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 5s - loss: 0.5666 - auroc: 0.6709 - val_loss: 0.5799 - val_auroc: 0.6553   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 5s - loss: 0.5664 - auroc: 0.6720 - val_loss: 0.5779 - val_auroc: 0.6588   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6588286757469177                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 7s - loss: 0.6478 - auroc: 0.6410 - val_loss: 0.5921 - val_auroc: 0.6438   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 5s - loss: 0.5722 - auroc: 0.6606 - val_loss: 0.5838 - val_auroc: 0.6453   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 6s - loss: 0.5700 - auroc: 0.6624 - val_loss: 0.5822 - val_auroc: 0.6510   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 10s - loss: 0.5685 - auroc: 0.6653 - val_loss: 0.5827 - val_auroc: 0.6513  \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 10s - loss: 0.5678 - auroc: 0.6671 - val_loss: 0.5786 - val_auroc: 0.6558  \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 6s - loss: 0.5678 - auroc: 0.6701 - val_loss: 0.5789 - val_auroc: 0.6578   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 9s - loss: 0.5670 - auroc: 0.6714 - val_loss: 0.5851 - val_auroc: 0.6553   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 6s - loss: 0.5663 - auroc: 0.6748 - val_loss: 0.5834 - val_auroc: 0.6507   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 6s - loss: 0.5651 - auroc: 0.6747 - val_loss: 0.5806 - val_auroc: 0.6586   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 9s - loss: 0.5660 - auroc: 0.6761 - val_loss: 0.5859 - val_auroc: 0.6589   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6588666439056396                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 5s - loss: 0.8720 - auroc: 0.5592 - val_loss: 0.7961 - val_auroc: 0.6053   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 4s - loss: 0.7372 - auroc: 0.6338 - val_loss: 0.7123 - val_auroc: 0.6387   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 5s - loss: 0.6762 - auroc: 0.6536 - val_loss: 0.6696 - val_auroc: 0.6484   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 7s - loss: 0.6432 - auroc: 0.6615 - val_loss: 0.6457 - val_auroc: 0.6511   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 0.6224 - auroc: 0.6666 - val_loss: 0.6292 - val_auroc: 0.6574   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 4s - loss: 0.6089 - auroc: 0.6706 - val_loss: 0.6186 - val_auroc: 0.6574   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 4s - loss: 0.5992 - auroc: 0.6744 - val_loss: 0.6109 - val_auroc: 0.6597   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 4s - loss: 0.5920 - auroc: 0.6771 - val_loss: 0.6052 - val_auroc: 0.6614   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 4s - loss: 0.5862 - auroc: 0.6808 - val_loss: 0.6007 - val_auroc: 0.6659   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 0.5815 - auroc: 0.6842 - val_loss: 0.5974 - val_auroc: 0.6649   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.665899395942688                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                   \n",
      " - 4s - loss: 2.8536 - auroc: 0.5814 - val_loss: 0.8886 - val_auroc: 0.6135  \n",
      "\n",
      "Epoch 2/10                                                                   \n",
      " - 3s - loss: 0.7295 - auroc: 0.6284 - val_loss: 0.6607 - val_auroc: 0.6171  \n",
      "\n",
      "Epoch 3/10                                                                   \n",
      " - 4s - loss: 0.6241 - auroc: 0.6324 - val_loss: 0.6186 - val_auroc: 0.6245  \n",
      "\n",
      "Epoch 4/10                                                                   \n",
      " - 4s - loss: 0.6000 - auroc: 0.6345 - val_loss: 0.6106 - val_auroc: 0.6196  \n",
      "\n",
      "Epoch 5/10                                                                   \n",
      " - 3s - loss: 0.5915 - auroc: 0.6366 - val_loss: 0.6011 - val_auroc: 0.6195  \n",
      "\n",
      "Epoch 6/10                                                                   \n",
      " - 4s - loss: 0.5891 - auroc: 0.6332 - val_loss: 0.6004 - val_auroc: 0.6246  \n",
      "\n",
      "Epoch 7/10                                                                   \n",
      " - 4s - loss: 0.5909 - auroc: 0.6287 - val_loss: 0.5979 - val_auroc: 0.6266  \n",
      "\n",
      "Epoch 8/10                                                                   \n",
      " - 4s - loss: 0.5907 - auroc: 0.6264 - val_loss: 0.6057 - val_auroc: 0.6160  \n",
      "\n",
      "Epoch 9/10                                                                   \n",
      " - 4s - loss: 0.5911 - auroc: 0.6290 - val_loss: 0.6082 - val_auroc: 0.6126  \n",
      "\n",
      "Epoch 10/10                                                                  \n",
      " - 6s - loss: 0.5955 - auroc: 0.6281 - val_loss: 0.6239 - val_auroc: 0.6213  \n",
      "\n",
      "Best validation auc of epoch:                                                \n",
      "0.6265516877174377                                                           \n",
      "Train on 81000 samples, validate on 9000 samples                             \n",
      "Epoch 1/10                                                                   \n",
      " - 4s - loss: 0.6783 - auroc: 0.6364 - val_loss: 0.5968 - val_auroc: 0.6420  \n",
      "\n",
      "Epoch 2/10                                                                   \n",
      " - 3s - loss: 0.5742 - auroc: 0.6595 - val_loss: 0.5830 - val_auroc: 0.6477  \n",
      "\n",
      "Epoch 3/10                                                                   \n",
      " - 3s - loss: 0.5703 - auroc: 0.6630 - val_loss: 0.5869 - val_auroc: 0.6425  \n",
      "\n",
      "Epoch 4/10                                                                   \n",
      " - 4s - loss: 0.5703 - auroc: 0.6640 - val_loss: 0.5834 - val_auroc: 0.6496  \n",
      "\n",
      "Epoch 5/10                                                                   \n",
      " - 4s - loss: 0.5688 - auroc: 0.6659 - val_loss: 0.5812 - val_auroc: 0.6496  \n",
      "\n",
      "Epoch 6/10                                                                   \n",
      " - 6s - loss: 0.5683 - auroc: 0.6664 - val_loss: 0.5819 - val_auroc: 0.6514  \n",
      "\n",
      "Epoch 7/10                                                                   \n",
      " - 4s - loss: 0.5668 - auroc: 0.6686 - val_loss: 0.5805 - val_auroc: 0.6543  \n",
      "\n",
      "Epoch 8/10                                                                   \n",
      " - 4s - loss: 0.5677 - auroc: 0.6707 - val_loss: 0.5813 - val_auroc: 0.6559  \n",
      "\n",
      "Epoch 9/10                                                                   \n",
      " - 4s - loss: 0.5661 - auroc: 0.6726 - val_loss: 0.5782 - val_auroc: 0.6602  \n",
      "\n",
      "Epoch 10/10                                                                  \n",
      " - 4s - loss: 0.5672 - auroc: 0.6736 - val_loss: 0.5820 - val_auroc: 0.6561  \n",
      "\n",
      "Best validation auc of epoch:                                                \n",
      "0.6601634621620178                                                           \n",
      "Train on 81000 samples, validate on 9000 samples                             \n",
      "Epoch 1/10                                                                   \n",
      " - 8s - loss: 0.5951 - auroc: 0.6404 - val_loss: 0.5853 - val_auroc: 0.6468  \n",
      "\n",
      "Epoch 2/10                                                                   \n",
      " - 7s - loss: 0.5754 - auroc: 0.6539 - val_loss: 0.5856 - val_auroc: 0.6486  \n",
      "\n",
      "Epoch 3/10                                                                   \n",
      " - 11s - loss: 0.5722 - auroc: 0.6577 - val_loss: 0.5828 - val_auroc: 0.6474 \n",
      "\n",
      "Epoch 4/10                                                                   \n",
      " - 8s - loss: 0.5707 - auroc: 0.6593 - val_loss: 0.5816 - val_auroc: 0.6473  \n",
      "\n",
      "Epoch 5/10                                                                   \n",
      " - 6s - loss: 0.5706 - auroc: 0.6609 - val_loss: 0.5862 - val_auroc: 0.6428  \n",
      "\n",
      "Epoch 6/10                                                                   \n",
      " - 5s - loss: 0.5696 - auroc: 0.6605 - val_loss: 0.5833 - val_auroc: 0.6499  \n",
      "\n",
      "Epoch 7/10                                                                   \n",
      " - 5s - loss: 0.5700 - auroc: 0.6616 - val_loss: 0.5834 - val_auroc: 0.6471  \n",
      "\n",
      "Epoch 8/10                                                                   \n",
      " - 8s - loss: 0.5694 - auroc: 0.6609 - val_loss: 0.5816 - val_auroc: 0.6472  \n",
      "\n",
      "Epoch 9/10                                                                   \n",
      " - 5s - loss: 0.5680 - auroc: 0.6623 - val_loss: 0.5820 - val_auroc: 0.6469  \n",
      "\n",
      "Epoch 10/10                                                                  \n",
      " - 5s - loss: 0.5696 - auroc: 0.6622 - val_loss: 0.5802 - val_auroc: 0.6556  \n",
      "\n",
      "Best validation auc of epoch:                                                \n",
      "0.6555919647216797                                                           \n",
      "Train on 81000 samples, validate on 9000 samples                             \n",
      "Epoch 1/10                                                                   \n",
      " - 5s - loss: 0.9212 - auroc: 0.5238 - val_loss: 0.8598 - val_auroc: 0.5609  \n",
      "\n",
      "Epoch 2/10                                                                   \n",
      " - 4s - loss: 0.8023 - auroc: 0.5972 - val_loss: 0.7768 - val_auroc: 0.6076  \n",
      "\n",
      "Epoch 3/10                                                                   \n",
      " - 6s - loss: 0.7346 - auroc: 0.6348 - val_loss: 0.7251 - val_auroc: 0.6253  \n",
      "\n",
      "Epoch 4/10                                                                   \n",
      " - 6s - loss: 0.6921 - auroc: 0.6486 - val_loss: 0.6912 - val_auroc: 0.6347  \n",
      "\n",
      "Epoch 5/10                                                                   \n",
      " - 6s - loss: 0.6633 - auroc: 0.6573 - val_loss: 0.6674 - val_auroc: 0.6415  \n",
      "\n",
      "Epoch 6/10                                                                   \n",
      " - 6s - loss: 0.6428 - auroc: 0.6634 - val_loss: 0.6501 - val_auroc: 0.6455  \n",
      "\n",
      "Epoch 7/10                                                                   \n",
      " - 5s - loss: 0.6278 - auroc: 0.6673 - val_loss: 0.6385 - val_auroc: 0.6449  \n",
      "\n",
      "Epoch 8/10                                                                   \n",
      " - 4s - loss: 0.6165 - auroc: 0.6694 - val_loss: 0.6273 - val_auroc: 0.6514  \n",
      "\n",
      "Epoch 9/10                                                                   \n",
      " - 3s - loss: 0.6077 - auroc: 0.6726 - val_loss: 0.6195 - val_auroc: 0.6535  \n",
      "\n",
      "Epoch 10/10                                                                  \n",
      " - 4s - loss: 0.6007 - auroc: 0.6742 - val_loss: 0.6143 - val_auroc: 0.6532  \n",
      "\n",
      "Best validation auc of epoch:                                                \n",
      "0.6534916758537292                                                           \n",
      "Train on 81000 samples, validate on 9000 samples                             \n",
      "Epoch 1/10                                                                   \n",
      " - 6s - loss: 0.9067 - auroc: 0.6068 - val_loss: 0.7308 - val_auroc: 0.6287  \n",
      "\n",
      "Epoch 2/10                                                                   \n",
      " - 3s - loss: 0.6531 - auroc: 0.6561 - val_loss: 0.6297 - val_auroc: 0.6437  \n",
      "\n",
      "Epoch 3/10                                                                   \n",
      " - 3s - loss: 0.5985 - auroc: 0.6652 - val_loss: 0.6005 - val_auroc: 0.6530  \n",
      "\n",
      "Epoch 4/10                                                                   \n",
      " - 4s - loss: 0.5813 - auroc: 0.6694 - val_loss: 0.5923 - val_auroc: 0.6490  \n",
      "\n",
      "Epoch 5/10                                                                   \n",
      " - 3s - loss: 0.5736 - auroc: 0.6720 - val_loss: 0.5859 - val_auroc: 0.6556  \n",
      "\n",
      "Epoch 6/10                                                                   \n",
      " - 4s - loss: 0.5694 - auroc: 0.6764 - val_loss: 0.5843 - val_auroc: 0.6540  \n",
      "\n",
      "Epoch 7/10                                                                   \n",
      " - 5s - loss: 0.5666 - auroc: 0.6794 - val_loss: 0.5840 - val_auroc: 0.6596  \n",
      "\n",
      "Epoch 8/10                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 5s - loss: 0.5647 - auroc: 0.6825 - val_loss: 0.5816 - val_auroc: 0.6601  \n",
      "\n",
      "Epoch 9/10                                                                   \n",
      " - 7s - loss: 0.5635 - auroc: 0.6846 - val_loss: 0.5808 - val_auroc: 0.6604  \n",
      "\n",
      "Epoch 10/10                                                                  \n",
      " - 5s - loss: 0.5621 - auroc: 0.6866 - val_loss: 0.5796 - val_auroc: 0.6629  \n",
      "\n",
      "Best validation auc of epoch:                                                \n",
      "0.6629023551940918                                                           \n",
      "Train on 81000 samples, validate on 9000 samples                             \n",
      "Epoch 1/10                                                                   \n",
      " - 11s - loss: 0.5983 - auroc: 0.6336 - val_loss: 0.5902 - val_auroc: 0.6381 \n",
      "\n",
      "Epoch 2/10                                                                   \n",
      " - 6s - loss: 0.5804 - auroc: 0.6475 - val_loss: 0.6062 - val_auroc: 0.6342  \n",
      "\n",
      "Epoch 3/10                                                                   \n",
      " - 6s - loss: 0.5772 - auroc: 0.6516 - val_loss: 0.5885 - val_auroc: 0.6411  \n",
      "\n",
      "Epoch 4/10                                                                   \n",
      " - 9s - loss: 0.5781 - auroc: 0.6521 - val_loss: 0.5952 - val_auroc: 0.6396  \n",
      "\n",
      "Epoch 5/10                                                                   \n",
      " - 6s - loss: 0.5759 - auroc: 0.6528 - val_loss: 0.5823 - val_auroc: 0.6541  \n",
      "\n",
      "Epoch 6/10                                                                   \n",
      " - 8s - loss: 0.5755 - auroc: 0.6525 - val_loss: 0.5872 - val_auroc: 0.6416  \n",
      "\n",
      "Epoch 7/10                                                                   \n",
      " - 6s - loss: 0.5764 - auroc: 0.6533 - val_loss: 0.5849 - val_auroc: 0.6452  \n",
      "\n",
      "Epoch 8/10                                                                   \n",
      " - 6s - loss: 0.5754 - auroc: 0.6548 - val_loss: 0.5840 - val_auroc: 0.6428  \n",
      "\n",
      "Epoch 9/10                                                                   \n",
      " - 6s - loss: 0.5733 - auroc: 0.6543 - val_loss: 0.5846 - val_auroc: 0.6413  \n",
      "\n",
      "Epoch 10/10                                                                  \n",
      " - 11s - loss: 0.5781 - auroc: 0.6530 - val_loss: 0.5891 - val_auroc: 0.6416 \n",
      "\n",
      "Best validation auc of epoch:                                                \n",
      "0.6540573835372925                                                           \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 5s - loss: 1.1186 - auroc: 0.5363 - val_loss: 1.0201 - val_auroc: 0.5802   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 3s - loss: 0.9303 - auroc: 0.6210 - val_loss: 0.8835 - val_auroc: 0.6161   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 4s - loss: 0.8198 - auroc: 0.6453 - val_loss: 0.7976 - val_auroc: 0.6284   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 4s - loss: 0.7496 - auroc: 0.6565 - val_loss: 0.7403 - val_auroc: 0.6366   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 3s - loss: 0.7016 - auroc: 0.6642 - val_loss: 0.7009 - val_auroc: 0.6413   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 3s - loss: 0.6689 - auroc: 0.6704 - val_loss: 0.6735 - val_auroc: 0.6454   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 4s - loss: 0.6454 - auroc: 0.6743 - val_loss: 0.6540 - val_auroc: 0.6509   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 3s - loss: 0.6283 - auroc: 0.6786 - val_loss: 0.6392 - val_auroc: 0.6540   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 3s - loss: 0.6156 - auroc: 0.6818 - val_loss: 0.6305 - val_auroc: 0.6551   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 0.6055 - auroc: 0.6865 - val_loss: 0.6218 - val_auroc: 0.6587   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6586533784866333                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 5s - loss: 0.6742 - auroc: 0.6066 - val_loss: 0.6169 - val_auroc: 0.6373   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 4s - loss: 0.5895 - auroc: 0.6574 - val_loss: 0.5935 - val_auroc: 0.6491   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 3s - loss: 0.5758 - auroc: 0.6645 - val_loss: 0.5863 - val_auroc: 0.6495   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 3s - loss: 0.5705 - auroc: 0.6674 - val_loss: 0.5819 - val_auroc: 0.6536   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 3s - loss: 0.5677 - auroc: 0.6710 - val_loss: 0.5800 - val_auroc: 0.6580   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 3s - loss: 0.5664 - auroc: 0.6729 - val_loss: 0.5800 - val_auroc: 0.6587   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 3s - loss: 0.5655 - auroc: 0.6754 - val_loss: 0.5795 - val_auroc: 0.6577   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 3s - loss: 0.5646 - auroc: 0.6765 - val_loss: 0.5789 - val_auroc: 0.6592   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 3s - loss: 0.5635 - auroc: 0.6790 - val_loss: 0.5796 - val_auroc: 0.6604   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 3s - loss: 0.5632 - auroc: 0.6804 - val_loss: 0.5786 - val_auroc: 0.6588   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6604442596435547                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 5s - loss: 0.7300 - auroc: 0.6214 - val_loss: 0.6055 - val_auroc: 0.6332   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 4s - loss: 0.5862 - auroc: 0.6381 - val_loss: 0.5925 - val_auroc: 0.6369   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 4s - loss: 0.5856 - auroc: 0.6425 - val_loss: 0.5970 - val_auroc: 0.6347   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 4s - loss: 0.5860 - auroc: 0.6438 - val_loss: 0.5968 - val_auroc: 0.6274   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 0.5858 - auroc: 0.6434 - val_loss: 0.5956 - val_auroc: 0.6291   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 4s - loss: 0.5937 - auroc: 0.6443 - val_loss: 0.5961 - val_auroc: 0.6303   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 4s - loss: 0.5815 - auroc: 0.6451 - val_loss: 0.5955 - val_auroc: 0.6344   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 4s - loss: 0.5874 - auroc: 0.6467 - val_loss: 0.6014 - val_auroc: 0.6378   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 4s - loss: 0.5811 - auroc: 0.6491 - val_loss: 0.5886 - val_auroc: 0.6354   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 0.5836 - auroc: 0.6490 - val_loss: 0.5991 - val_auroc: 0.6276   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6378364562988281                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 6s - loss: 0.7715 - auroc: 0.5679 - val_loss: 0.6587 - val_auroc: 0.5419   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 6s - loss: 0.6270 - auroc: 0.5640 - val_loss: 0.6554 - val_auroc: 0.5523   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 4s - loss: 0.6300 - auroc: 0.5661 - val_loss: 0.6348 - val_auroc: 0.5718   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 4s - loss: 0.6315 - auroc: 0.5627 - val_loss: 0.6550 - val_auroc: 0.5491   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 0.6332 - auroc: 0.5648 - val_loss: 0.6344 - val_auroc: 0.5596   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 4s - loss: 0.6242 - auroc: 0.5652 - val_loss: 0.6311 - val_auroc: 0.5635   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 8s - loss: 0.6273 - auroc: 0.5660 - val_loss: 0.6386 - val_auroc: 0.5514   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 4s - loss: 0.6309 - auroc: 0.5692 - val_loss: 0.6380 - val_auroc: 0.5567   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 4s - loss: 0.6304 - auroc: 0.5659 - val_loss: 0.6363 - val_auroc: 0.5669   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 0.6344 - auroc: 0.5656 - val_loss: 0.6346 - val_auroc: 0.5625   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.5717861652374268                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 8s - loss: 0.7640 - auroc: 0.5761 - val_loss: 0.6274 - val_auroc: 0.5844   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 4s - loss: 0.6219 - auroc: 0.5744 - val_loss: 0.6304 - val_auroc: 0.5581   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 4s - loss: 0.6306 - auroc: 0.5665 - val_loss: 0.6290 - val_auroc: 0.5555   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 5s - loss: 0.6199 - auroc: 0.5653 - val_loss: 0.6210 - val_auroc: 0.5715   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 0.6260 - auroc: 0.5670 - val_loss: 0.6276 - val_auroc: 0.5541   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 4s - loss: 0.6236 - auroc: 0.5667 - val_loss: 0.6389 - val_auroc: 0.5562   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 7s - loss: 0.6257 - auroc: 0.5727 - val_loss: 0.6253 - val_auroc: 0.5632   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 4s - loss: 0.6204 - auroc: 0.5714 - val_loss: 0.6346 - val_auroc: 0.5469   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 4s - loss: 0.6228 - auroc: 0.5683 - val_loss: 0.6304 - val_auroc: 0.5698   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 0.6245 - auroc: 0.5656 - val_loss: 0.6364 - val_auroc: 0.5744   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.5844090580940247                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 9s - loss: 0.5962 - auroc: 0.6305 - val_loss: 0.5919 - val_auroc: 0.6377   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 7s - loss: 0.5810 - auroc: 0.6450 - val_loss: 0.5946 - val_auroc: 0.6336   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 4s - loss: 0.5783 - auroc: 0.6489 - val_loss: 0.5869 - val_auroc: 0.6448   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 4s - loss: 0.5819 - auroc: 0.6473 - val_loss: 0.5851 - val_auroc: 0.6468   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 0.5766 - auroc: 0.6497 - val_loss: 0.5920 - val_auroc: 0.6424   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 5s - loss: 0.5777 - auroc: 0.6504 - val_loss: 0.5863 - val_auroc: 0.6418   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 4s - loss: 0.5768 - auroc: 0.6509 - val_loss: 0.5865 - val_auroc: 0.6380   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 4s - loss: 0.5757 - auroc: 0.6489 - val_loss: 0.5929 - val_auroc: 0.6454   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 5s - loss: 0.5772 - auroc: 0.6513 - val_loss: 0.5887 - val_auroc: 0.6454   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 0.5753 - auroc: 0.6504 - val_loss: 0.5861 - val_auroc: 0.6431   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6467645168304443                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 10s - loss: 0.6000 - auroc: 0.6342 - val_loss: 0.5991 - val_auroc: 0.6372  \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 5s - loss: 0.5822 - auroc: 0.6487 - val_loss: 0.5904 - val_auroc: 0.6379   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 6s - loss: 0.5809 - auroc: 0.6506 - val_loss: 0.6012 - val_auroc: 0.6314   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 5s - loss: 0.5786 - auroc: 0.6514 - val_loss: 0.5923 - val_auroc: 0.6410   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 7s - loss: 0.5787 - auroc: 0.6528 - val_loss: 0.5892 - val_auroc: 0.6428   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 9s - loss: 0.5756 - auroc: 0.6523 - val_loss: 0.5929 - val_auroc: 0.6417   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 5s - loss: 0.5778 - auroc: 0.6528 - val_loss: 0.5884 - val_auroc: 0.6387   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 6s - loss: 0.5784 - auroc: 0.6529 - val_loss: 0.5878 - val_auroc: 0.6433   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 7s - loss: 0.5749 - auroc: 0.6529 - val_loss: 0.5880 - val_auroc: 0.6442   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 5s - loss: 0.5777 - auroc: 0.6531 - val_loss: 0.5901 - val_auroc: 0.6491   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6490689516067505                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 7s - loss: 0.6479 - auroc: 0.6129 - val_loss: 0.6035 - val_auroc: 0.6457   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 5s - loss: 0.5807 - auroc: 0.6614 - val_loss: 0.5886 - val_auroc: 0.6526   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 6s - loss: 0.5724 - auroc: 0.6655 - val_loss: 0.5848 - val_auroc: 0.6525   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 4s - loss: 0.5689 - auroc: 0.6691 - val_loss: 0.5806 - val_auroc: 0.6594   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 0.5673 - auroc: 0.6715 - val_loss: 0.5789 - val_auroc: 0.6616   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 6s - loss: 0.5659 - auroc: 0.6732 - val_loss: 0.5806 - val_auroc: 0.6585   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 4s - loss: 0.5647 - auroc: 0.6752 - val_loss: 0.5792 - val_auroc: 0.6615   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10                                                                    \n",
      " - 7s - loss: 0.5638 - auroc: 0.6775 - val_loss: 0.5788 - val_auroc: 0.6637   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 4s - loss: 0.5634 - auroc: 0.6791 - val_loss: 0.5793 - val_auroc: 0.6607   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 0.5627 - auroc: 0.6807 - val_loss: 0.5770 - val_auroc: 0.6665   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6665163636207581                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 6s - loss: 0.7248 - auroc: 0.5776 - val_loss: 0.6447 - val_auroc: 0.6191    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 3s - loss: 0.6084 - auroc: 0.6497 - val_loss: 0.6065 - val_auroc: 0.6450    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 3s - loss: 0.5857 - auroc: 0.6610 - val_loss: 0.5927 - val_auroc: 0.6505    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 3s - loss: 0.5765 - auroc: 0.6663 - val_loss: 0.5866 - val_auroc: 0.6530    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 3s - loss: 0.5719 - auroc: 0.6691 - val_loss: 0.5840 - val_auroc: 0.6553    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 5s - loss: 0.5702 - auroc: 0.6715 - val_loss: 0.5825 - val_auroc: 0.6584    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 3s - loss: 0.5671 - auroc: 0.6752 - val_loss: 0.5804 - val_auroc: 0.6603    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 3s - loss: 0.5661 - auroc: 0.6769 - val_loss: 0.5796 - val_auroc: 0.6613    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 3s - loss: 0.5649 - auroc: 0.6790 - val_loss: 0.5811 - val_auroc: 0.6566    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 4s - loss: 0.5641 - auroc: 0.6800 - val_loss: 0.5804 - val_auroc: 0.6643    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.6642635464668274                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 10s - loss: 0.6942 - auroc: 0.5511 - val_loss: 0.6398 - val_auroc: 0.5367   \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 7s - loss: 0.6402 - auroc: 0.5548 - val_loss: 0.6619 - val_auroc: 0.5283    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 7s - loss: 0.6416 - auroc: 0.5623 - val_loss: 0.6486 - val_auroc: 0.5479    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 8s - loss: 0.6490 - auroc: 0.5591 - val_loss: 0.6398 - val_auroc: 0.5377    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 10s - loss: 0.6380 - auroc: 0.5580 - val_loss: 0.6481 - val_auroc: 0.5453   \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 7s - loss: 0.6399 - auroc: 0.5528 - val_loss: 0.6470 - val_auroc: 0.5521    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 7s - loss: 0.6444 - auroc: 0.5604 - val_loss: 0.6560 - val_auroc: 0.5528    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 7s - loss: 1.5576 - auroc: 0.5559 - val_loss: 199.0709 - val_auroc: 0.5112  \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 7s - loss: 36.3208 - auroc: 0.5599 - val_loss: 3.4557 - val_auroc: 0.5628   \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 7s - loss: 2.2211 - auroc: 0.5756 - val_loss: 1.5388 - val_auroc: 0.5663    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5662819743156433                                                             \n",
      "100%|██████████| 20/20 [18:43<00:00, 64.44s/it, best loss: -0.6665163636207581]\n",
      "Evalutation of best performing model:\n",
      "10000/10000 [==============================] - 1s 98us/step\n",
      "[0.575965975522995, 0.677500307559967]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'batch_size': 1, 'layers': 0, 'lr': 0.000850395343847371}\n"
     ]
    }
   ],
   "source": [
    "# new parameters to compare based on last results - see below\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=20,\n",
    "                                      trials=Trials(), \n",
    "                                      notebook_name='04_NeuMF_hyperparameter_tuning')\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "\n",
    "print(best_model.evaluate(X_test, Y_test, batch_size = 100))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load( open(\"data/models/tuning/test_5.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'batch_size': 512,\n",
       "  'layers': (128, 64, 32, 16),\n",
       "  'lr': 0.0026315396548810465,\n",
       "  'val_loss': 0.5796978988117641,\n",
       "  'best_val_auc': 0.6554561853408813},\n",
       " {'batch_size': 128,\n",
       "  'layers': (256, 128, 64, 32),\n",
       "  'lr': 0.0018119748031739601,\n",
       "  'val_loss': 0.5778736586040921,\n",
       "  'best_val_auc': 0.6588286757469177},\n",
       " {'batch_size': 128,\n",
       "  'layers': (512, 256, 128, 64),\n",
       "  'lr': 0.0013114585903650563,\n",
       "  'val_loss': 0.5858697779708438,\n",
       "  'best_val_auc': 0.6588666439056396},\n",
       " {'batch_size': 256,\n",
       "  'layers': (256, 128, 64, 32),\n",
       "  'lr': 0.00014812687643836193,\n",
       "  'val_loss': 0.5974413517581092,\n",
       "  'best_val_auc': 0.665899395942688},\n",
       " {'batch_size': 512,\n",
       "  'layers': (512, 256, 128, 64),\n",
       "  'lr': 0.05497918212450093,\n",
       "  'val_loss': 0.6238912242253621,\n",
       "  'best_val_auc': 0.6265516877174377},\n",
       " {'batch_size': 512,\n",
       "  'layers': (512, 256, 128, 64),\n",
       "  'lr': 0.0024235402954732887,\n",
       "  'val_loss': 0.5819869956970215,\n",
       "  'best_val_auc': 0.6601634621620178},\n",
       " {'batch_size': 128,\n",
       "  'layers': (128, 64, 32, 16),\n",
       "  'lr': 0.0031916702833145886,\n",
       "  'val_loss': 0.5801516071955363,\n",
       "  'best_val_auc': 0.6555919647216797},\n",
       " {'batch_size': 512,\n",
       "  'layers': (256, 128, 64, 32),\n",
       "  'lr': 0.00014136900871344774,\n",
       "  'val_loss': 0.6143099046283298,\n",
       "  'best_val_auc': 0.6534916758537292},\n",
       " {'batch_size': 512,\n",
       "  'layers': (512, 256, 128, 64),\n",
       "  'lr': 0.0005991721932783176,\n",
       "  'val_loss': 0.5796446196768019,\n",
       "  'best_val_auc': 0.6629023551940918},\n",
       " {'batch_size': 128,\n",
       "  'layers': (256, 128, 64, 32),\n",
       "  'lr': 0.006495227608973739,\n",
       "  'val_loss': 0.5890928036371866,\n",
       "  'best_val_auc': 0.6540573835372925},\n",
       " {'batch_size': 512,\n",
       "  'layers': (512, 256, 128, 64),\n",
       "  'lr': 0.00014836560602871627,\n",
       "  'val_loss': 0.6217755266295539,\n",
       "  'best_val_auc': 0.6586533784866333},\n",
       " {'batch_size': 512,\n",
       "  'layers': (128, 64, 32, 16),\n",
       "  'lr': 0.0011550669047601578,\n",
       "  'val_loss': 0.5785592096116807,\n",
       "  'best_val_auc': 0.6604442596435547},\n",
       " {'batch_size': 512,\n",
       "  'layers': (512, 256, 128, 64),\n",
       "  'lr': 0.021091153752075875,\n",
       "  'val_loss': 0.5990876722335815,\n",
       "  'best_val_auc': 0.6378364562988281},\n",
       " {'batch_size': 256,\n",
       "  'layers': (128, 64, 32, 16),\n",
       "  'lr': 0.11082627180656118,\n",
       "  'val_loss': 0.634609603828854,\n",
       "  'best_val_auc': 0.5717861652374268},\n",
       " {'batch_size': 256,\n",
       "  'layers': (128, 64, 32, 16),\n",
       "  'lr': 0.09935456362521095,\n",
       "  'val_loss': 0.6363638095325894,\n",
       "  'best_val_auc': 0.5844090580940247},\n",
       " {'batch_size': 256,\n",
       "  'layers': (128, 64, 32, 16),\n",
       "  'lr': 0.011462201565793235,\n",
       "  'val_loss': 0.586138290113873,\n",
       "  'best_val_auc': 0.6467645168304443},\n",
       " {'batch_size': 256,\n",
       "  'layers': (256, 128, 64, 32),\n",
       "  'lr': 0.010219392435286582,\n",
       "  'val_loss': 0.5901111791133881,\n",
       "  'best_val_auc': 0.6490689516067505},\n",
       " {'batch_size': 256,\n",
       "  'layers': (128, 64, 32, 16),\n",
       "  'lr': 0.000850395343847371,\n",
       "  'val_loss': 0.5769849214818743,\n",
       "  'best_val_auc': 0.6665163636207581},\n",
       " {'batch_size': 512,\n",
       "  'layers': (128, 64, 32, 16),\n",
       "  'lr': 0.000841243685133845,\n",
       "  'val_loss': 0.580361954636044,\n",
       "  'best_val_auc': 0.6642635464668274},\n",
       " {'batch_size': 128,\n",
       "  'layers': (128, 64, 32, 16),\n",
       "  'lr': 0.09315546518337009,\n",
       "  'val_loss': 1.538845819261339,\n",
       "  'best_val_auc': 0.5662819743156433}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of parameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc_1 = []\n",
    "learning_rate = []\n",
    "batch_size = []\n",
    "\n",
    "\n",
    "for i in range(0, len(results)):\n",
    "    best_auc_1.append(results[i]['best_val_auc'])\n",
    "    learning_rate.append(results[i]['lr'])\n",
    "    batch_size.append(results[i]['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('experiment')\n",
    "ax1.set_ylabel('auc', color=color)\n",
    "ax1.plot( best_auc_1, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('compare', color=color)  # we already handled the x-label with ax1\n",
    "#ax2.plot(learning_rate, color=color)\n",
    "ax2.plot(batch_size, color = 'tab:green')\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from numpy.random import seed\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow import set_random_seed\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pickle\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.sequence import pad_sequences\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import initializers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Embedding, Input, Dense, merge, Flatten, concatenate, multiply, dot, Reshape, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras.callbacks\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from time import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pdb\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from scipy import sparse\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sqlalchemy as db\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sqlalchemy import create_engine\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import psycopg2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from matplotlib import pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import roc_auc_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from matplotlib import pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'reg_mlp': hp.loguniform('reg_mlp',  -5, -2),\n",
      "        'reg_mlp_1': hp.loguniform('reg_mlp_1',  -5, -2),\n",
      "        'reg_mlp_2': hp.loguniform('reg_mlp_2',  -5, -2),\n",
      "        'lr': hp.loguniform('lr', -7, -2),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: data_bundle = pd.read_pickle(\"training_data/sample_100000.pkl\")\n",
      "   3: max_length = 200\n",
      "   4: #data_bundle = data_bundle.dropna().reset_index()\n",
      "   5: # context\n",
      "   6: month = np.nan_to_num(data_bundle.month_enc.values.reshape(-1,1))\n",
      "   7: days_online = np.nan_to_num(data_bundle.days_online_log_std.values.reshape(-1,1))#, dtype = \"float32\")\n",
      "   8: # item\n",
      "   9: item_anbieter = np.nan_to_num(data_bundle.anbieterid_enc.values.reshape(-1,1)) # nur weil es noch Nan gab - fix ich noch\n",
      "  10: item_mkt= np.nan_to_num(data_bundle.anbietermarktplatz_enc.values.reshape(-1,1))\n",
      "  11: item_wg = np.nan_to_num(data_bundle.warengruppe_enc.values.reshape(-1,1))\n",
      "  12: item_preis = np.nan_to_num(data_bundle.preis_log_std.values.reshape(-1,1))\n",
      "  13: item_ve = np.nan_to_num(data_bundle.minve_log_std.values.reshape(-1,1))\n",
      "  14: list_text = []\n",
      "  15: list_text_user = []\n",
      "  16: for i in range(len(data_bundle)):\n",
      "  17:     list_text.append(data_bundle.text_vec[i])\n",
      "  18:     list_text_user.append(data_bundle.text_vec_user[i])\n",
      "  19: item_text = np.array(list_text, ndmin = 2)\n",
      "  20: \n",
      "  21: # user\n",
      "  22: user_text = np.array(list_text_user, ndmin = 2)\n",
      "  23: user_mkt = np.nan_to_num(data_bundle.usermkt_enc.values.reshape(-1,1))\n",
      "  24: \n",
      "  25: user_anbieter = pad_sequences(data_bundle.anbieterid_enc_user, maxlen = max_length, padding = \"pre\")\n",
      "  26: user_anbietermkt = pad_sequences(data_bundle.anbietermarktplatz_enc_user, maxlen = max_length, padding = \"pre\")\n",
      "  27: user_wg = pad_sequences(data_bundle.warengruppe_enc_user, maxlen = max_length, padding = \"pre\")\n",
      "  28: user_preis = np.nan_to_num(data_bundle.preis_log_std_user.values.reshape(-1,1))\n",
      "  29: user_ve = np.nan_to_num(data_bundle.minve_log_std_user.values.reshape(-1,1))\n",
      "  30: \n",
      "  31: # features \n",
      "  32: x_train = [month[:90000], days_online[:90000], item_anbieter[:90000], item_mkt[:90000], item_wg[:90000], item_preis[:90000], item_ve[:90000], item_text[:90000], user_mkt[:90000], user_anbietermkt[:90000], user_wg[:90000], user_anbieter[:90000], user_preis[:90000], user_ve[:90000], user_text[:90000]]\n",
      "  33: x_test = [month[90000:], days_online[90000:], item_anbieter[90000:], item_mkt[90000:], item_wg[90000:], item_preis[90000:], item_ve[90000:], item_text[90000:], user_mkt[90000:], user_anbietermkt[90000:], user_wg[90000:], user_anbieter[90000:], user_preis[90000:], user_ve[90000:], user_text[90000:]]\n",
      "  34: # target\n",
      "  35: y_train = data_bundle.pick.values.reshape(-1,1)[:90000]\n",
      "  36: y_test = data_bundle.pick.values.reshape(-1,1)[90000:]\n",
      "  37: \n",
      "  38: \n",
      "  39: \n",
      "  40: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "    1: def keras_fmin_fnct(space):\n",
      "    2: \n",
      "    3:     layers = [256, 128,64,32] \n",
      "    4:     reg_mlp = space['reg_mlp'] \n",
      "    5:     reg_lay = space['reg_mlp_1'] \n",
      "    6:     reg_layers = [reg_mlp, reg_lay, reg_lay, reg_lay]\n",
      "    7:     reg_mf = space['reg_mlp_2'] \n",
      "    8:     \n",
      "    9:     def mask_aware_mean(x):\n",
      "   10:     # recreate the masks - all zero rows have been masked\n",
      "   11:         mask = K.not_equal(K.sum(K.abs(x), axis=2, keepdims=True), 0)\n",
      "   12:         # number of that rows are not all zeros\n",
      "   13:         n = K.sum(K.cast(mask, 'float32'), axis=1, keepdims=False)\n",
      "   14:         # compute mask-aware mean of x\n",
      "   15:         x_mean = K.sum(x, axis=1, keepdims=False) / n\n",
      "   16:         return x_mean \n",
      "   17: \n",
      "   18:     lrelu = lambda x: tf.keras.activations.relu(x, alpha=0.1)\n",
      "   19:     \n",
      "   20:     def get_model(layers, reg_layers, reg_mf):\n",
      "   21:         assert len(layers) == len(reg_layers)\n",
      "   22:         num_layer = len(layers) #Number of layers in the MLP\n",
      "   23:         ### Input variables\n",
      "   24:         max_length = 200\n",
      "   25:         months = 13\n",
      "   26:         supplier = 563\n",
      "   27:         wgs = 230\n",
      "   28:         mkt = 9\n",
      "   29: \n",
      "   30:         months_emb = round(months ** 0.25)\n",
      "   31:         supplier_emb = round(supplier ** 0.25)\n",
      "   32:         wgs_emb = round(wgs ** 0.25)\n",
      "   33:         mkt_emb = round(mkt ** 0.25)\n",
      "   34:         \n",
      "   35:         # Context\n",
      "   36:         month = Input(shape = (1,), dtype = \"float32\", name = \"month\")\n",
      "   37:         day_online = Input(shape = (1,), dtype = \"float32\", name = \"days_online\")\n",
      "   38:         # Item\n",
      "   39:         item_anbieter = Input(shape = (1,), dtype = \"float32\", name = \"item_anbieter\")\n",
      "   40:         item_mkt = Input(shape = (1,), dtype = \"float32\", name = \"item_mkt\")\n",
      "   41:         item_wg = Input(shape = (1,), dtype = \"float32\", name = \"item_wg\")\n",
      "   42:         item_preis = Input(shape = (1,), dtype = \"float32\", name = \"item_preis\")\n",
      "   43:         item_ve = Input(shape = (1,), dtype = \"float32\", name = \"item_ve\")\n",
      "   44:         item_text = Input(shape = (150,), dtype = \"float32\", name = \"item_text\")\n",
      "   45: \n",
      "   46:         # User\n",
      "   47:         user_mkt = Input(shape = (1,), dtype = \"float32\", name = \"user_mkt\")\n",
      "   48:         user_anbieter = Input(shape = (max_length,), dtype = \"float32\", name = \"user_anbieter\")\n",
      "   49:         user_anbietermkt = Input(shape = (max_length,), dtype = \"float32\", name = \"user_anbietermkt\")\n",
      "   50:         user_wg = Input(shape = (max_length,), dtype = \"float32\", name = \"user_wg\")\n",
      "   51:         user_preis = Input(shape = (1,), dtype = \"float32\", name = \"user_preis\")\n",
      "   52:         user_ve = Input(shape = (1,), dtype = \"float32\", name = \"user_ve\")\n",
      "   53:         user_text = Input(shape = (150,), dtype = \"float32\", name = \"user_text\")\n",
      "   54: \n",
      "   55: \n",
      "   56: \n",
      "   57:         ### Embedding layer\n",
      "   58:         # MF\n",
      "   59:         # Context\n",
      "   60:         MF_Embedding_Context_month = Embedding(input_dim=months, output_dim=months_emb, name='mf_embedding_month',\n",
      "   61:                                       embeddings_initializer=initializers.random_normal(),\n",
      "   62:                                       embeddings_regularizer=l2(reg_mf), input_length=1)\n",
      "   63: \n",
      "   64:         # Item\n",
      "   65: \n",
      "   66:         MF_Embedding_Item_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mf_embedding_item_anbieter',\n",
      "   67:                                       embeddings_initializer=initializers.random_normal(),\n",
      "   68:                                       embeddings_regularizer=l2(reg_mf),input_length=1)\n",
      "   69:         MF_Embedding_Item_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mf_embedding_item_mkt',\n",
      "   70:                                       embeddings_initializer=initializers.random_normal(),\n",
      "   71:                                       embeddings_regularizer=l2(reg_mf), input_length=1)\n",
      "   72: \n",
      "   73:         MF_Embedding_Item_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mf_embedding_item_wg',\n",
      "   74:                                       embeddings_initializer=initializers.random_normal(),\n",
      "   75:                                       embeddings_regularizer=l2(reg_mf), input_length=1)\n",
      "   76: \n",
      "   77: \n",
      "   78:         # User\n",
      "   79: \n",
      "   80:         MF_Embedding_User_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mf_embedding_user_mkt',\n",
      "   81:                                           embeddings_initializer=initializers.random_normal(),\n",
      "   82:                                           embeddings_regularizer=l2(reg_mf),\n",
      "   83:                                           #mask_zero = True,\n",
      "   84:                                           input_length=1)\n",
      "   85:         MF_Embedding_User_anbietermkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mf_embedding_user_anbietermkt',\n",
      "   86:                                                   embeddings_initializer=initializers.random_normal(),\n",
      "   87:                                                   embeddings_regularizer=l2(reg_mf),\n",
      "   88:                                                   mask_zero = True,\n",
      "   89:                                                   input_length=max_length)\n",
      "   90:         MF_Embedding_User_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mf_embedding_user_wg',\n",
      "   91:                                          embeddings_initializer=initializers.random_normal(),\n",
      "   92:                                          embeddings_regularizer=l2(reg_mf),\n",
      "   93:                                          mask_zero = True,\n",
      "   94:                                          input_length=max_length)\n",
      "   95:         MF_Embedding_User_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mf_embedding_user_anbieter',\n",
      "   96:                                                embeddings_initializer=initializers.random_normal(),\n",
      "   97:                                                embeddings_regularizer=l2(reg_mf),\n",
      "   98:                                                mask_zero = True,\n",
      "   99:                                                input_length=max_length)\n",
      "  100: \n",
      "  101: \n",
      "  102:         # MLP\n",
      "  103:         #Context\n",
      "  104: \n",
      "  105:         MLP_Embedding_Context_month = Embedding(input_dim=months, output_dim=months_emb, name='mlp_embedding_month',\n",
      "  106:                                       embeddings_initializer=initializers.random_normal(),\n",
      "  107:                                       embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
      "  108: \n",
      "  109: \n",
      "  110:         # Item\n",
      "  111: \n",
      "  112:         MLP_Embedding_Item_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mlp_embedding_item_anbieter',\n",
      "  113:                                       embeddings_initializer=initializers.random_normal(),\n",
      "  114:                                       embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
      "  115:         MLP_Embedding_Item_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mlp_embedding_item_mkt',\n",
      "  116:                                       embeddings_initializer=initializers.random_normal(),\n",
      "  117:                                       embeddings_regularizer=l2(reg_layers[0]),  \n",
      "  118:                                            input_length=1)\n",
      "  119: \n",
      "  120:         MLP_Embedding_Item_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mlp_embedding_item_wg',\n",
      "  121:                                       embeddings_initializer=initializers.random_normal(),\n",
      "  122:                                       embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
      "  123: \n",
      "  124: \n",
      "  125:         # User\n",
      "  126: \n",
      "  127:         MLP_Embedding_User_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mlp_embedding_user_mkt',\n",
      "  128:                                           embeddings_initializer=initializers.random_normal(),\n",
      "  129:                                           embeddings_regularizer=l2(reg_layers[0]),\n",
      "  130:                                           #mask_zero = True,\n",
      "  131:                                           input_length=1)\n",
      "  132:         MLP_Embedding_User_anbietermkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mlp_embedding_user_anbietermkt',\n",
      "  133:                                                   embeddings_initializer=initializers.random_normal(),\n",
      "  134:                                                   embeddings_regularizer=l2(reg_layers[0]),\n",
      "  135:                                                   mask_zero = True,\n",
      "  136:                                                   input_length=max_length)\n",
      "  137:         MLP_Embedding_User_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mlp_embedding_user_wg',\n",
      "  138:                                          embeddings_initializer=initializers.random_normal(),\n",
      "  139:                                          embeddings_regularizer=l2(reg_layers[0]),\n",
      "  140:                                          mask_zero = True,\n",
      "  141:                                          input_length=max_length)\n",
      "  142:         MLP_Embedding_User_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mlp_embedding_user_anbieter',\n",
      "  143:                                                embeddings_initializer=initializers.random_normal(),\n",
      "  144:                                                embeddings_regularizer=l2(reg_layers[0]),\n",
      "  145:                                                #mask_zero = True,\n",
      "  146:                                                input_length=max_length)\n",
      "  147: \n",
      "  148:         ## all user ones zb. and then concenate / multiply in MF \n",
      "  149:         # MF part\n",
      "  150:         ## context \n",
      "  151:         emb_item_month = Flatten(name = \"mf_flat_month\")(MF_Embedding_Context_month(month))\n",
      "  152: \n",
      "  153:         ## user\n",
      "  154:         emb_user_mkt = Flatten(name =\"mf_flat_user_mkt\")(MF_Embedding_User_mkt(user_mkt))\n",
      "  155:         #     emb_user_anbietermkt = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mf_avg_user_anbietermkt\"))(MF_Embedding_User_anbietermkt(user_anbietermkt))\n",
      "  156:         emb_user_wg = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mf_avg_user_wg\"))(MF_Embedding_User_wg(user_wg))\n",
      "  157:         emb_user_anbieter = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mf_avg_user_anbieter\"))(MF_Embedding_User_anbieter(user_anbieter))\n",
      "  158: \n",
      "  159:         ## item\n",
      "  160:         emb_item_anbieter = Flatten(name = \"mf_flat_item_anbieter\")(MF_Embedding_Item_anbieter(item_anbieter))\n",
      "  161:         emb_item_mkt = Flatten(name = \"mf_flat_item_mkt\")(MF_Embedding_Item_mkt(item_mkt))\n",
      "  162:         emb_item_wg = Flatten(name = \"mf_flat_item_wg\")(MF_Embedding_Item_wg(item_wg))\n",
      "  163: \n",
      "  164:         # MF connect\n",
      "  165:         mf_vector = multiply([concatenate([emb_user_mkt, emb_user_wg, emb_user_anbieter, user_preis, user_ve, user_text,\n",
      "  166:                                            day_online, emb_item_month], name = \"mf_user\"), \n",
      "  167:                               concatenate([emb_item_mkt, emb_item_wg, emb_item_anbieter, item_preis, item_ve, item_text,\n",
      "  168:                                            day_online, emb_item_month], name = \"mf_item\")], \n",
      "  169:                              name = \"mf_multiply\")\n",
      "  170: \n",
      "  171: \n",
      "  172:         # MLP part\n",
      "  173:         ## context \n",
      "  174:         emb_item_month2 = Flatten(name = \"mlp_flat_month\")(MLP_Embedding_Context_month(month))\n",
      "  175:         ## user\n",
      "  176:         emb_user_mkt2 = Flatten(name =\"mlp_flat_user_mkt\")(MLP_Embedding_User_mkt(user_mkt))\n",
      "  177:         emb_user_anbietermkt2 = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mlp_avg_user_anbietermkt\"))(MLP_Embedding_User_anbietermkt(user_anbietermkt))\n",
      "  178:         emb_user_wg2 = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mlp_avg_user_wg\"))(MLP_Embedding_User_wg(user_wg))\n",
      "  179:         emb_user_anbieter2 = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mlp_avg_user_anbieten\"))(MLP_Embedding_User_anbieter(user_anbieter))\n",
      "  180: \n",
      "  181:         ## item\n",
      "  182:         emb_item_anbieter2 = Flatten(name = \"mlp_flat_item_anbieter\")(MLP_Embedding_Item_anbieter(item_anbieter))\n",
      "  183:         emb_item_mkt2 = Flatten(name = \"mlp_flat_item_mkt\")(MLP_Embedding_Item_mkt(item_mkt))\n",
      "  184:         emb_item_wg2 = Flatten(name = \"mlp_flat_item_wg\")(MLP_Embedding_Item_wg(item_wg))\n",
      "  185: \n",
      "  186:         mlp_vector = concatenate([emb_item_month2, day_online,\n",
      "  187:                                   emb_user_mkt2, emb_user_anbietermkt2, emb_user_wg2, emb_user_anbieter2, user_preis, user_ve, user_text,\n",
      "  188:                                   emb_item_anbieter2, emb_item_mkt2, emb_item_wg2, item_preis, item_ve, item_text], name = \"mlp_conc\")\n",
      "  189: \n",
      "  190:         for idx in range(1, num_layer):\n",
      "  191:             layer = Dense(layers[idx], kernel_regularizer=l2(reg_layers[idx]), activation=lrelu, name=\"layer%d\" % idx)\n",
      "  192:             mlp_vector = layer(mlp_vector)\n",
      "  193: \n",
      "  194:         # Concatenate MF and MLP parts\n",
      "  195:         predict_vector = concatenate([mf_vector, mlp_vector])\n",
      "  196: \n",
      "  197:         # Final prediction layer\n",
      "  198:         prediction = Dense(1, activation='sigmoid', kernel_initializer=initializers.lecun_normal(),\n",
      "  199:                            name=\"prediction\")(predict_vector)\n",
      "  200: \n",
      "  201:         model_ = Model(inputs=[month, day_online,\n",
      "  202:                                item_anbieter, item_mkt, item_wg, item_preis, item_ve, user_text,\n",
      "  203:                                user_mkt, user_anbietermkt, user_wg, user_anbieter, user_preis, user_ve, item_text],\n",
      "  204:                        outputs=prediction)\n",
      "  205: \n",
      "  206:         return model_\n",
      "  207: \n",
      "  208:     \n",
      "  209:     # load functino to evaluate performance      \n",
      "  210:     def auroc(y_true, y_pred):\n",
      "  211:         return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)\n",
      "  212:                   \n",
      "  213: \n",
      "  214: \n",
      "  215:     # create Model \n",
      "  216:     \n",
      "  217:     if 'results' not in globals():\n",
      "  218:         global results\n",
      "  219:         results = []\n",
      "  220:         \n",
      "  221:     model = get_model(layers, reg_layers, reg_mf)\n",
      "  222:     model.compile(optimizer=Adam(lr=space['lr']), loss='binary_crossentropy', metrics = [auroc])\n",
      "  223:     result = model.fit(x_train, y_train, batch_size = 256, epochs = 10, verbose = 2, validation_split=0.1)\n",
      "  224:     validation_auc =np.amax(result.history['val_auroc'])\n",
      "  225:     \n",
      "  226:     valLoss = result.history['val_loss'][-1]\n",
      "  227:     parameters = space\n",
      "  228:     parameters[\"val_loss\"] = valLoss\n",
      "  229:     parameters[\"best_val_auc\"] = validation_auc\n",
      "  230:     results.append(parameters)\n",
      "  231:     with open(\"models/tuning/test_4.pkl\", 'wb') as fp:\n",
      "  232:         pickle.dump(results, fp)\n",
      "  233:     print('Best validation auc of epoch:', validation_auc)\n",
      "  234:     return {'loss': -validation_auc, 'status': STATUS_OK, 'model': model} # minimizes based on validation_auc\n",
      "  235: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 81000 samples, validate on 9000 samples    \n",
      "Epoch 1/10                                          \n",
      " - 4s - loss: 0.8225 - auroc: 0.5735 - val_loss: 0.6005 - val_auroc: 0.5636\n",
      "\n",
      "Epoch 2/10                                          \n",
      " - 3s - loss: 0.5895 - auroc: 0.5812 - val_loss: 0.6024 - val_auroc: 0.5634\n",
      "\n",
      "Epoch 3/10                                          \n",
      " - 3s - loss: 0.5917 - auroc: 0.5794 - val_loss: 0.6049 - val_auroc: 0.5685\n",
      "\n",
      "Epoch 4/10                                          \n",
      " - 3s - loss: 0.5905 - auroc: 0.5807 - val_loss: 0.6048 - val_auroc: 0.5636\n",
      "\n",
      "Epoch 5/10                                          \n",
      " - 3s - loss: 0.5910 - auroc: 0.5770 - val_loss: 0.6028 - val_auroc: 0.5684\n",
      "\n",
      "Epoch 6/10                                          \n",
      " - 3s - loss: 0.5908 - auroc: 0.5789 - val_loss: 0.6055 - val_auroc: 0.5627\n",
      "\n",
      "Epoch 7/10                                          \n",
      " - 3s - loss: 0.5908 - auroc: 0.5784 - val_loss: 0.6029 - val_auroc: 0.5733\n",
      "\n",
      "Epoch 8/10                                          \n",
      " - 3s - loss: 0.5903 - auroc: 0.5780 - val_loss: 0.6044 - val_auroc: 0.5691\n",
      "\n",
      "Epoch 9/10                                          \n",
      " - 3s - loss: 0.5907 - auroc: 0.5792 - val_loss: 0.6035 - val_auroc: 0.5699\n",
      "\n",
      "Epoch 10/10                                         \n",
      " - 3s - loss: 0.5906 - auroc: 0.5811 - val_loss: 0.6009 - val_auroc: 0.5689\n",
      "\n",
      "Best validation auc of epoch:                       \n",
      "0.5733485221862793                                  \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 4s - loss: 1.4132 - auroc: 0.5502 - val_loss: 0.6194 - val_auroc: 0.5580   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 3s - loss: 0.6214 - auroc: 0.5510 - val_loss: 0.6366 - val_auroc: 0.5333   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 3s - loss: 0.6253 - auroc: 0.5496 - val_loss: 0.6511 - val_auroc: 0.5274   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 3s - loss: 0.6351 - auroc: 0.5523 - val_loss: 0.6881 - val_auroc: 0.5389   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 3s - loss: 0.7246 - auroc: 0.5496 - val_loss: 0.6826 - val_auroc: 0.5426   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 3s - loss: 0.6815 - auroc: 0.5516 - val_loss: 0.6912 - val_auroc: 0.5576   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 3s - loss: 0.7003 - auroc: 0.5522 - val_loss: 0.7087 - val_auroc: 0.5619   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 3s - loss: 0.6941 - auroc: 0.5531 - val_loss: 0.6915 - val_auroc: 0.5551   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 3s - loss: 3300.0265 - auroc: 0.5383 - val_loss: 3928.3242 - val_auroc: 0.5237\n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 3s - loss: 837.1577 - auroc: 0.5561 - val_loss: 294.5887 - val_auroc: 0.6003\n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6002712249755859                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 4s - loss: 0.7484 - auroc: 0.5825 - val_loss: 0.5985 - val_auroc: 0.5729   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 3s - loss: 0.5860 - auroc: 0.5887 - val_loss: 0.6014 - val_auroc: 0.5729   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 3s - loss: 0.5863 - auroc: 0.5879 - val_loss: 0.5996 - val_auroc: 0.5714   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 3s - loss: 0.5860 - auroc: 0.5891 - val_loss: 0.5982 - val_auroc: 0.5774   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 3s - loss: 0.5860 - auroc: 0.5893 - val_loss: 0.5994 - val_auroc: 0.5765   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 3s - loss: 0.5859 - auroc: 0.5897 - val_loss: 0.5990 - val_auroc: 0.5718   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 3s - loss: 0.5863 - auroc: 0.5892 - val_loss: 0.6015 - val_auroc: 0.5686   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 3s - loss: 0.5859 - auroc: 0.5904 - val_loss: 0.5997 - val_auroc: 0.5690   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 4s - loss: 0.5860 - auroc: 0.5896 - val_loss: 0.5978 - val_auroc: 0.5739   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 0.5862 - auroc: 0.5905 - val_loss: 0.5972 - val_auroc: 0.5758   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.5774306058883667                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 4s - loss: 0.7136 - auroc: 0.5661 - val_loss: 0.6020 - val_auroc: 0.5604   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 4s - loss: 0.5968 - auroc: 0.5687 - val_loss: 0.6173 - val_auroc: 0.5585   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 4s - loss: 0.5987 - auroc: 0.5672 - val_loss: 0.6037 - val_auroc: 0.5677   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 4s - loss: 0.5975 - auroc: 0.5698 - val_loss: 0.6071 - val_auroc: 0.5650   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 0.5990 - auroc: 0.5681 - val_loss: 0.6122 - val_auroc: 0.5438   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 4s - loss: 0.5990 - auroc: 0.5675 - val_loss: 0.6080 - val_auroc: 0.5642   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 4s - loss: 0.5991 - auroc: 0.5669 - val_loss: 0.6127 - val_auroc: 0.5539   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 4s - loss: 0.5989 - auroc: 0.5674 - val_loss: 0.6092 - val_auroc: 0.5651   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 4s - loss: 0.5996 - auroc: 0.5678 - val_loss: 0.6189 - val_auroc: 0.5499   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 0.5984 - auroc: 0.5699 - val_loss: 0.6117 - val_auroc: 0.5576   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.567661464214325                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 5s - loss: 0.7743 - auroc: 0.5744 - val_loss: 0.6010 - val_auroc: 0.5684   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 4s - loss: 0.5911 - auroc: 0.5801 - val_loss: 0.6018 - val_auroc: 0.5650   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 4s - loss: 0.5908 - auroc: 0.5796 - val_loss: 0.6039 - val_auroc: 0.5707   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 4s - loss: 0.5895 - auroc: 0.5817 - val_loss: 0.6061 - val_auroc: 0.5661   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 0.5896 - auroc: 0.5833 - val_loss: 0.6014 - val_auroc: 0.5670   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 4s - loss: 0.5901 - auroc: 0.5804 - val_loss: 0.6027 - val_auroc: 0.5698   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 4s - loss: 0.5904 - auroc: 0.5786 - val_loss: 0.6006 - val_auroc: 0.5700   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 4s - loss: 0.5908 - auroc: 0.5794 - val_loss: 0.6038 - val_auroc: 0.5603   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 4s - loss: 0.5900 - auroc: 0.5796 - val_loss: 0.6013 - val_auroc: 0.5646   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 0.5906 - auroc: 0.5793 - val_loss: 0.6028 - val_auroc: 0.5680   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.5707494616508484                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 5s - loss: 1.0163 - auroc: 0.5525 - val_loss: 0.6529 - val_auroc: 0.5449   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 4s - loss: 0.6197 - auroc: 0.5493 - val_loss: 0.6631 - val_auroc: 0.5433   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 4s - loss: 0.6280 - auroc: 0.5503 - val_loss: 0.6509 - val_auroc: 0.5375   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 4s - loss: 792.5053 - auroc: 0.5434 - val_loss: 1141.2079 - val_auroc: 0.5304\n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 221.1417 - auroc: 0.5841 - val_loss: 68.5047 - val_auroc: 0.5903\n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 4s - loss: 45.2437 - auroc: 0.6018 - val_loss: 30.1268 - val_auroc: 0.5942 \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 4s - loss: 22.4064 - auroc: 0.5932 - val_loss: 16.4247 - val_auroc: 0.6026 \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 4s - loss: 12.6788 - auroc: 0.5829 - val_loss: 9.6084 - val_auroc: 0.5847  \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 4s - loss: 7.5281 - auroc: 0.5783 - val_loss: 5.8014 - val_auroc: 0.5675   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 4.5721 - auroc: 0.5670 - val_loss: 3.5495 - val_auroc: 0.5671   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6026124358177185                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 5s - loss: 0.6468 - auroc: 0.5794 - val_loss: 0.6076 - val_auroc: 0.5757   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 4s - loss: 0.5897 - auroc: 0.5805 - val_loss: 0.6001 - val_auroc: 0.5726   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 4s - loss: 0.5903 - auroc: 0.5822 - val_loss: 0.6050 - val_auroc: 0.5693   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 4s - loss: 0.5879 - auroc: 0.5833 - val_loss: 0.6006 - val_auroc: 0.5747   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 0.5888 - auroc: 0.5831 - val_loss: 0.6004 - val_auroc: 0.5691   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 4s - loss: 0.5888 - auroc: 0.5822 - val_loss: 0.6026 - val_auroc: 0.5662   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 4s - loss: 0.5890 - auroc: 0.5837 - val_loss: 0.6014 - val_auroc: 0.5743   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 4s - loss: 0.5883 - auroc: 0.5835 - val_loss: 0.6010 - val_auroc: 0.5639   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 4s - loss: 0.5882 - auroc: 0.5831 - val_loss: 0.5985 - val_auroc: 0.5730   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 5s - loss: 0.5882 - auroc: 0.5827 - val_loss: 0.5997 - val_auroc: 0.5677   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.5757067799568176                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 5s - loss: 0.6374 - auroc: 0.5695 - val_loss: 0.6209 - val_auroc: 0.5535   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 4s - loss: 0.5971 - auroc: 0.5691 - val_loss: 0.6114 - val_auroc: 0.5563   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 6s - loss: 0.5958 - auroc: 0.5714 - val_loss: 0.6042 - val_auroc: 0.5723   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 4s - loss: 0.5963 - auroc: 0.5720 - val_loss: 0.6151 - val_auroc: 0.5620   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 0.5961 - auroc: 0.5698 - val_loss: 0.6140 - val_auroc: 0.5635   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 4s - loss: 0.5956 - auroc: 0.5711 - val_loss: 0.6070 - val_auroc: 0.5651   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 4s - loss: 0.5941 - auroc: 0.5723 - val_loss: 0.6050 - val_auroc: 0.5649   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 4s - loss: 0.5954 - auroc: 0.5703 - val_loss: 0.6052 - val_auroc: 0.5557   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 4s - loss: 0.5962 - auroc: 0.5726 - val_loss: 0.6142 - val_auroc: 0.5558   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 0.6014 - auroc: 0.5741 - val_loss: 0.6040 - val_auroc: 0.5686   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.5722847580909729                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 5s - loss: 1.0400 - auroc: 0.5715 - val_loss: 0.6011 - val_auroc: 0.5620   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 4s - loss: 0.5910 - auroc: 0.5794 - val_loss: 0.6016 - val_auroc: 0.5690   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 4s - loss: 0.5917 - auroc: 0.5778 - val_loss: 0.6061 - val_auroc: 0.5630   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 4s - loss: 0.5914 - auroc: 0.5790 - val_loss: 0.6087 - val_auroc: 0.5601   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 0.5915 - auroc: 0.5789 - val_loss: 0.6036 - val_auroc: 0.5628   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 7s - loss: 0.5909 - auroc: 0.5798 - val_loss: 0.6031 - val_auroc: 0.5696   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 6s - loss: 0.5918 - auroc: 0.5774 - val_loss: 0.6037 - val_auroc: 0.5707   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10                                                                    \n",
      " - 5s - loss: 0.5921 - auroc: 0.5782 - val_loss: 0.6030 - val_auroc: 0.5622   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 4s - loss: 0.5912 - auroc: 0.5775 - val_loss: 0.6078 - val_auroc: 0.5621   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 0.5927 - auroc: 0.5773 - val_loss: 0.6058 - val_auroc: 0.5626   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.5707420706748962                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 6s - loss: 0.6892 - auroc: 0.5836 - val_loss: 0.5985 - val_auroc: 0.5751   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 4s - loss: 0.5862 - auroc: 0.5887 - val_loss: 0.5971 - val_auroc: 0.5780   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 6s - loss: 0.5862 - auroc: 0.5890 - val_loss: 0.5985 - val_auroc: 0.5706   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 4s - loss: 0.5867 - auroc: 0.5875 - val_loss: 0.5984 - val_auroc: 0.5749   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 0.5862 - auroc: 0.5888 - val_loss: 0.5989 - val_auroc: 0.5727   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 6s - loss: 0.5865 - auroc: 0.5879 - val_loss: 0.5977 - val_auroc: 0.5722   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 4s - loss: 0.5863 - auroc: 0.5887 - val_loss: 0.5983 - val_auroc: 0.5727   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 7s - loss: 0.5863 - auroc: 0.5894 - val_loss: 0.5992 - val_auroc: 0.5734   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 4s - loss: 0.5864 - auroc: 0.5898 - val_loss: 0.5992 - val_auroc: 0.5726   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 5s - loss: 0.5861 - auroc: 0.5889 - val_loss: 0.6000 - val_auroc: 0.5676   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.5780426263809204                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 7s - loss: 0.9292 - auroc: 0.5770 - val_loss: 0.5990 - val_auroc: 0.5670    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 4s - loss: 0.5849 - auroc: 0.5898 - val_loss: 0.5978 - val_auroc: 0.5753    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 8s - loss: 0.5842 - auroc: 0.5944 - val_loss: 0.5987 - val_auroc: 0.5734    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 4s - loss: 0.5842 - auroc: 0.5934 - val_loss: 0.5985 - val_auroc: 0.5752    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 5s - loss: 0.5840 - auroc: 0.5938 - val_loss: 0.5974 - val_auroc: 0.5751    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 6s - loss: 0.5840 - auroc: 0.5944 - val_loss: 0.5980 - val_auroc: 0.5758    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 7s - loss: 0.5840 - auroc: 0.5933 - val_loss: 0.5974 - val_auroc: 0.5737    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 4s - loss: 0.5840 - auroc: 0.5951 - val_loss: 0.5969 - val_auroc: 0.5784    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 4s - loss: 0.5841 - auroc: 0.5938 - val_loss: 0.5972 - val_auroc: 0.5763    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 6s - loss: 0.5840 - auroc: 0.5939 - val_loss: 0.5977 - val_auroc: 0.5747    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5783781409263611                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 6s - loss: 2.2680 - auroc: 0.5507 - val_loss: 0.5980 - val_auroc: 0.5705    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 7s - loss: 0.5852 - auroc: 0.5895 - val_loss: 0.5968 - val_auroc: 0.5777    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 6s - loss: 0.5843 - auroc: 0.5934 - val_loss: 0.5970 - val_auroc: 0.5739    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 6s - loss: 0.5838 - auroc: 0.5950 - val_loss: 0.5969 - val_auroc: 0.5772    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 4s - loss: 0.5838 - auroc: 0.5948 - val_loss: 0.5969 - val_auroc: 0.5766    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 4s - loss: 0.5838 - auroc: 0.5958 - val_loss: 0.5977 - val_auroc: 0.5801    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 6s - loss: 0.5839 - auroc: 0.5945 - val_loss: 0.5974 - val_auroc: 0.5759    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 5s - loss: 0.5838 - auroc: 0.5939 - val_loss: 0.5967 - val_auroc: 0.5769    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 4s - loss: 0.5837 - auroc: 0.5952 - val_loss: 0.5970 - val_auroc: 0.5790    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 4s - loss: 0.5838 - auroc: 0.5946 - val_loss: 0.5974 - val_auroc: 0.5761    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5800676941871643                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 6s - loss: 1.2609 - auroc: 0.5490 - val_loss: 0.6438 - val_auroc: 0.5422    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 4s - loss: 0.6319 - auroc: 0.5466 - val_loss: 0.6362 - val_auroc: 0.5566    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 4s - loss: 0.6342 - auroc: 0.5499 - val_loss: 0.6661 - val_auroc: 0.5339    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 4s - loss: 0.6355 - auroc: 0.5487 - val_loss: 0.6668 - val_auroc: 0.5506    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 4s - loss: 0.6700 - auroc: 0.5472 - val_loss: 0.6492 - val_auroc: 0.5251    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 7s - loss: 0.6618 - auroc: 0.5494 - val_loss: 0.6814 - val_auroc: 0.5411    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 4s - loss: 0.6711 - auroc: 0.5493 - val_loss: 0.6903 - val_auroc: 0.5468    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 5s - loss: 0.6716 - auroc: 0.5469 - val_loss: 0.6718 - val_auroc: 0.5499    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 6s - loss: 0.6819 - auroc: 0.5498 - val_loss: 0.6667 - val_auroc: 0.5443    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 4s - loss: 0.6740 - auroc: 0.5495 - val_loss: 0.6946 - val_auroc: 0.5503    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5565707683563232                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 6s - loss: 0.7337 - auroc: 0.5714 - val_loss: 0.6057 - val_auroc: 0.5649    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 4s - loss: 0.5924 - auroc: 0.5784 - val_loss: 0.5996 - val_auroc: 0.5747    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 6s - loss: 0.5921 - auroc: 0.5779 - val_loss: 0.6100 - val_auroc: 0.5674    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 4s - loss: 0.5913 - auroc: 0.5772 - val_loss: 0.6023 - val_auroc: 0.5668    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 4s - loss: 0.5927 - auroc: 0.5772 - val_loss: 0.6041 - val_auroc: 0.5670    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 4s - loss: 0.5927 - auroc: 0.5747 - val_loss: 0.6079 - val_auroc: 0.5598    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 5s - loss: 0.5916 - auroc: 0.5770 - val_loss: 0.6057 - val_auroc: 0.5604    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 5s - loss: 0.5916 - auroc: 0.5777 - val_loss: 0.6006 - val_auroc: 0.5713    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 5s - loss: 0.5913 - auroc: 0.5784 - val_loss: 0.6056 - val_auroc: 0.5660    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 4s - loss: 0.5921 - auroc: 0.5785 - val_loss: 0.6021 - val_auroc: 0.5630    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5746629238128662                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 7s - loss: 0.6979 - auroc: 0.5817 - val_loss: 0.5999 - val_auroc: 0.5723    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 5s - loss: 0.5866 - auroc: 0.5864 - val_loss: 0.5977 - val_auroc: 0.5766    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 5s - loss: 0.5874 - auroc: 0.5863 - val_loss: 0.6039 - val_auroc: 0.5793    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 7s - loss: 0.5869 - auroc: 0.5881 - val_loss: 0.5980 - val_auroc: 0.5742    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 5s - loss: 0.5870 - auroc: 0.5864 - val_loss: 0.5987 - val_auroc: 0.5715    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 5s - loss: 0.5874 - auroc: 0.5866 - val_loss: 0.6000 - val_auroc: 0.5686    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 7s - loss: 0.5872 - auroc: 0.5845 - val_loss: 0.5995 - val_auroc: 0.5744    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 5s - loss: 0.5873 - auroc: 0.5865 - val_loss: 0.6023 - val_auroc: 0.5712    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 5s - loss: 0.5872 - auroc: 0.5877 - val_loss: 0.5992 - val_auroc: 0.5723    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 5s - loss: 0.5871 - auroc: 0.5848 - val_loss: 0.5998 - val_auroc: 0.5709    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5792803764343262                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 7s - loss: 0.7157 - auroc: 0.5852 - val_loss: 0.5971 - val_auroc: 0.5798    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 5s - loss: 0.5864 - auroc: 0.5886 - val_loss: 0.6022 - val_auroc: 0.5722    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 6s - loss: 0.5861 - auroc: 0.5885 - val_loss: 0.5980 - val_auroc: 0.5719    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 5s - loss: 0.5861 - auroc: 0.5882 - val_loss: 0.5979 - val_auroc: 0.5753    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 5s - loss: 0.5863 - auroc: 0.5900 - val_loss: 0.5996 - val_auroc: 0.5702    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 9s - loss: 0.5863 - auroc: 0.5881 - val_loss: 0.5995 - val_auroc: 0.5739    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 5s - loss: 0.5864 - auroc: 0.5876 - val_loss: 0.6005 - val_auroc: 0.5709    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 5s - loss: 0.5863 - auroc: 0.5883 - val_loss: 0.6000 - val_auroc: 0.5708    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 5s - loss: 0.5862 - auroc: 0.5887 - val_loss: 0.5983 - val_auroc: 0.5767    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 5s - loss: 0.5866 - auroc: 0.5875 - val_loss: 0.5999 - val_auroc: 0.5774    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5798372626304626                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 7s - loss: 1.2291 - auroc: 0.5711 - val_loss: 0.6022 - val_auroc: 0.5787    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 5s - loss: 0.5865 - auroc: 0.5882 - val_loss: 0.5996 - val_auroc: 0.5688    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 5s - loss: 0.5862 - auroc: 0.5886 - val_loss: 0.5986 - val_auroc: 0.5733    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 5s - loss: 0.5861 - auroc: 0.5887 - val_loss: 0.5991 - val_auroc: 0.5714    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 5s - loss: 0.5859 - auroc: 0.5891 - val_loss: 0.5990 - val_auroc: 0.5747    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 5s - loss: 0.5860 - auroc: 0.5880 - val_loss: 0.5984 - val_auroc: 0.5766    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 5s - loss: 0.5861 - auroc: 0.5889 - val_loss: 0.6002 - val_auroc: 0.5697    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 9s - loss: 0.5866 - auroc: 0.5869 - val_loss: 0.6005 - val_auroc: 0.5741    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 9s - loss: 0.5865 - auroc: 0.5884 - val_loss: 0.5993 - val_auroc: 0.5733    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 5s - loss: 0.5861 - auroc: 0.5885 - val_loss: 0.5989 - val_auroc: 0.5740    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.578723669052124                                                              \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 7s - loss: 0.8865 - auroc: 0.5648 - val_loss: 0.6065 - val_auroc: 0.5714    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 5s - loss: 0.5975 - auroc: 0.5685 - val_loss: 0.6122 - val_auroc: 0.5522    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 5s - loss: 0.5972 - auroc: 0.5696 - val_loss: 0.6068 - val_auroc: 0.5627    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 5s - loss: 0.5974 - auroc: 0.5692 - val_loss: 0.6102 - val_auroc: 0.5643    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 5s - loss: 0.5988 - auroc: 0.5684 - val_loss: 0.6134 - val_auroc: 0.5535    \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10                                                                     \n",
      " - 5s - loss: 0.6004 - auroc: 0.5677 - val_loss: 0.6156 - val_auroc: 0.5511    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 5s - loss: 0.6006 - auroc: 0.5690 - val_loss: 0.6266 - val_auroc: 0.5544    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 8s - loss: 0.6005 - auroc: 0.5664 - val_loss: 0.6090 - val_auroc: 0.5625    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 5s - loss: 0.6009 - auroc: 0.5672 - val_loss: 0.6102 - val_auroc: 0.5651    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 8s - loss: 0.6025 - auroc: 0.5699 - val_loss: 0.6113 - val_auroc: 0.5640    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5714282393455505                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 7s - loss: 0.7190 - auroc: 0.5885 - val_loss: 0.5970 - val_auroc: 0.5784    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 5s - loss: 0.5855 - auroc: 0.5898 - val_loss: 0.5978 - val_auroc: 0.5734    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 5s - loss: 0.5854 - auroc: 0.5907 - val_loss: 0.5981 - val_auroc: 0.5760    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 5s - loss: 0.5855 - auroc: 0.5909 - val_loss: 0.5981 - val_auroc: 0.5729    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 5s - loss: 0.5854 - auroc: 0.5892 - val_loss: 0.5979 - val_auroc: 0.5738    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 6s - loss: 0.5858 - auroc: 0.5900 - val_loss: 0.5981 - val_auroc: 0.5760    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 5s - loss: 0.5852 - auroc: 0.5908 - val_loss: 0.5972 - val_auroc: 0.5750    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 5s - loss: 0.5856 - auroc: 0.5895 - val_loss: 0.6003 - val_auroc: 0.5721    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 7s - loss: 0.5856 - auroc: 0.5909 - val_loss: 0.5974 - val_auroc: 0.5747    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 5s - loss: 0.5852 - auroc: 0.5904 - val_loss: 0.6002 - val_auroc: 0.5720    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.578445553779602                                                              \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 7s - loss: 2.0182 - auroc: 0.5393 - val_loss: 0.6053 - val_auroc: 0.5537    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 5s - loss: 0.5871 - auroc: 0.5838 - val_loss: 0.5977 - val_auroc: 0.5662    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 5s - loss: 0.5846 - auroc: 0.5904 - val_loss: 0.5985 - val_auroc: 0.5743    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 5s - loss: 0.5842 - auroc: 0.5931 - val_loss: 0.5976 - val_auroc: 0.5755    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 7s - loss: 0.5836 - auroc: 0.5952 - val_loss: 0.5977 - val_auroc: 0.5744    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 5s - loss: 0.5836 - auroc: 0.5962 - val_loss: 0.5971 - val_auroc: 0.5768    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 6s - loss: 0.5835 - auroc: 0.5962 - val_loss: 0.5970 - val_auroc: 0.5766    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 9s - loss: 0.5835 - auroc: 0.5945 - val_loss: 0.5971 - val_auroc: 0.5755    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 5s - loss: 0.5834 - auroc: 0.5954 - val_loss: 0.5968 - val_auroc: 0.5768    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 5s - loss: 0.5835 - auroc: 0.5955 - val_loss: 0.5973 - val_auroc: 0.5767    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.576779842376709                                                              \n",
      "100%|██████████| 20/20 [17:30<00:00, 63.42s/it, best loss: -0.6026124358177185]\n",
      "Evalutation of best performing model:\n",
      "10000/10000 [==============================] - 1s 90us/step\n",
      "[3.5555807638168333, 0.5653349161148071]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'lr': 0.1208033507910474, 'reg_mlp': 0.053136460509001955, 'reg_mlp_1': 0.03283367468901467, 'reg_mlp_2': 0.024138682045991487}\n"
     ]
    }
   ],
   "source": [
    "# new parameters to compare based on last results - see below\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=20,\n",
    "                                      trials=Trials(), \n",
    "                                      notebook_name='04_NeuMF_hyperparameter_tuning')\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "\n",
    "print(best_model.evaluate(X_test, Y_test, batch_size = 100))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load( open(\"data/models/tuning/test_4.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lr': 0.019339252240379672,\n",
       "  'reg_mlp': 0.06151400001134183,\n",
       "  'reg_mlp_1': 0.047614899427820455,\n",
       "  'reg_mlp_2': 0.025005706461411516,\n",
       "  'val_loss': 0.6008564678563012,\n",
       "  'best_val_auc': 0.5733485221862793},\n",
       " {'lr': 0.1206336016195465,\n",
       "  'reg_mlp': 0.08290997106348277,\n",
       "  'reg_mlp_1': 0.10419286156552511,\n",
       "  'reg_mlp_2': 0.02131011537608561,\n",
       "  'val_loss': 294.5886646050347,\n",
       "  'best_val_auc': 0.6002712249755859},\n",
       " {'lr': 0.006571377083578594,\n",
       "  'reg_mlp': 0.1258651170581643,\n",
       "  'reg_mlp_1': 0.016018477292065587,\n",
       "  'reg_mlp_2': 0.018553054188967046,\n",
       "  'val_loss': 0.5972261113060845,\n",
       "  'best_val_auc': 0.5774306058883667},\n",
       " {'lr': 0.041714077498507994,\n",
       "  'reg_mlp': 0.0232368991874887,\n",
       "  'reg_mlp_1': 0.028820641529007943,\n",
       "  'reg_mlp_2': 0.007286291371008999,\n",
       "  'val_loss': 0.6117258868217468,\n",
       "  'best_val_auc': 0.567661464214325},\n",
       " {'lr': 0.01721513468749108,\n",
       "  'reg_mlp': 0.020731624570864096,\n",
       "  'reg_mlp_1': 0.0388322068191507,\n",
       "  'reg_mlp_2': 0.09199165330861926,\n",
       "  'val_loss': 0.6028317552672492,\n",
       "  'best_val_auc': 0.5707494616508484},\n",
       " {'lr': 0.1208033507910474,\n",
       "  'reg_mlp': 0.053136460509001955,\n",
       "  'reg_mlp_1': 0.03283367468901467,\n",
       "  'reg_mlp_2': 0.024138682045991487,\n",
       "  'val_loss': 3.549483060412937,\n",
       "  'best_val_auc': 0.6026124358177185},\n",
       " {'lr': 0.01372146814602308,\n",
       "  'reg_mlp': 0.018325565141702648,\n",
       "  'reg_mlp_1': 0.009241249957278276,\n",
       "  'reg_mlp_2': 0.027161696139666364,\n",
       "  'val_loss': 0.5997253413730197,\n",
       "  'best_val_auc': 0.5757067799568176},\n",
       " {'lr': 0.03512482741806369,\n",
       "  'reg_mlp': 0.014653995468021604,\n",
       "  'reg_mlp_1': 0.00862061235804025,\n",
       "  'reg_mlp_2': 0.0071419244985798025,\n",
       "  'val_loss': 0.6039529580010308,\n",
       "  'best_val_auc': 0.5722847580909729},\n",
       " {'lr': 0.02083221944571217,\n",
       "  'reg_mlp': 0.025508689927087942,\n",
       "  'reg_mlp_1': 0.10284907801100969,\n",
       "  'reg_mlp_2': 0.013262156743556885,\n",
       "  'val_loss': 0.6058479657173157,\n",
       "  'best_val_auc': 0.5707420706748962},\n",
       " {'lr': 0.0072395949714510535,\n",
       "  'reg_mlp': 0.008886510996387209,\n",
       "  'reg_mlp_1': 0.012629337336460754,\n",
       "  'reg_mlp_2': 0.03683018458411511,\n",
       "  'val_loss': 0.5999857155481975,\n",
       "  'best_val_auc': 0.5780426263809204},\n",
       " {'lr': 0.002298857684659698,\n",
       "  'reg_mlp': 0.031175027941577043,\n",
       "  'reg_mlp_1': 0.01625383574136397,\n",
       "  'reg_mlp_2': 0.00729132176821172,\n",
       "  'val_loss': 0.5976791839599609,\n",
       "  'best_val_auc': 0.5783781409263611},\n",
       " {'lr': 0.0017692631288983081,\n",
       "  'reg_mlp': 0.11500797521817907,\n",
       "  'reg_mlp_1': 0.06397251317791636,\n",
       "  'reg_mlp_2': 0.017570365474915344,\n",
       "  'val_loss': 0.597449307150311,\n",
       "  'best_val_auc': 0.5800676941871643},\n",
       " {'lr': 0.13485694366137754,\n",
       "  'reg_mlp': 0.020831766706130572,\n",
       "  'reg_mlp_1': 0.05590955919404406,\n",
       "  'reg_mlp_2': 0.06101276919453788,\n",
       "  'val_loss': 0.6945847315788269,\n",
       "  'best_val_auc': 0.5565707683563232},\n",
       " {'lr': 0.022723406207557223,\n",
       "  'reg_mlp': 0.05673822004173885,\n",
       "  'reg_mlp_1': 0.02672052352359496,\n",
       "  'reg_mlp_2': 0.1242294654833493,\n",
       "  'val_loss': 0.602087547328737,\n",
       "  'best_val_auc': 0.5746629238128662},\n",
       " {'lr': 0.009427617754857443,\n",
       "  'reg_mlp': 0.0964167155613079,\n",
       "  'reg_mlp_1': 0.0082801640452505,\n",
       "  'reg_mlp_2': 0.11854597168292276,\n",
       "  'val_loss': 0.5997661562230852,\n",
       "  'best_val_auc': 0.5792803764343262},\n",
       " {'lr': 0.007524766482373546,\n",
       "  'reg_mlp': 0.10410933401898746,\n",
       "  'reg_mlp_1': 0.011775371446134834,\n",
       "  'reg_mlp_2': 0.04698088367489786,\n",
       "  'val_loss': 0.5998639261987474,\n",
       "  'best_val_auc': 0.5798372626304626},\n",
       " {'lr': 0.00728026214133041,\n",
       "  'reg_mlp': 0.006965902316422482,\n",
       "  'reg_mlp_1': 0.09783340836396745,\n",
       "  'reg_mlp_2': 0.044725984685282515,\n",
       "  'val_loss': 0.5989217115773096,\n",
       "  'best_val_auc': 0.578723669052124},\n",
       " {'lr': 0.040768793517214186,\n",
       "  'reg_mlp': 0.03553005898284985,\n",
       "  'reg_mlp_1': 0.07839304167457248,\n",
       "  'reg_mlp_2': 0.015409433778639882,\n",
       "  'val_loss': 0.6113466083738539,\n",
       "  'best_val_auc': 0.5714282393455505},\n",
       " {'lr': 0.005548975246543821,\n",
       "  'reg_mlp': 0.020082103665666602,\n",
       "  'reg_mlp_1': 0.013352818621404257,\n",
       "  'reg_mlp_2': 0.015338143601149462,\n",
       "  'val_loss': 0.6002138426833683,\n",
       "  'best_val_auc': 0.578445553779602},\n",
       " {'lr': 0.001127467953185886,\n",
       "  'reg_mlp': 0.034201967047812316,\n",
       "  'reg_mlp_1': 0.03017573564458185,\n",
       "  'reg_mlp_2': 0.11531759980167432,\n",
       "  'val_loss': 0.5973040359814962,\n",
       "  'best_val_auc': 0.576779842376709}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of parameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc_1 = []\n",
    "learning_rate = []\n",
    "reg_mlp = []\n",
    "reg_lay = []\n",
    "reg_mf = []\n",
    "\n",
    "\n",
    "for i in range(0, len(results)):\n",
    "    best_auc_1.append(results[i]['best_val_auc'])\n",
    "    learning_rate.append(results[i]['lr'])\n",
    "    reg_mlp.append(results[i]['reg_mlp'])\n",
    "    reg_lay.append(results[i]['reg_mlp_1'])\n",
    "    reg_mf.append(results[i]['reg_mlp_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.019339252240379672,\n",
       " 0.1206336016195465,\n",
       " 0.006571377083578594,\n",
       " 0.041714077498507994,\n",
       " 0.01721513468749108,\n",
       " 0.1208033507910474,\n",
       " 0.01372146814602308,\n",
       " 0.03512482741806369,\n",
       " 0.02083221944571217,\n",
       " 0.0072395949714510535,\n",
       " 0.002298857684659698,\n",
       " 0.0017692631288983081,\n",
       " 0.13485694366137754,\n",
       " 0.022723406207557223,\n",
       " 0.009427617754857443,\n",
       " 0.007524766482373546,\n",
       " 0.00728026214133041,\n",
       " 0.040768793517214186,\n",
       " 0.005548975246543821,\n",
       " 0.001127467953185886]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('experiment')\n",
    "ax1.set_ylabel('auc', color=color)\n",
    "ax1.plot( best_auc_1, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('compare', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(learning_rate, color=color)\n",
    "#ax2.plot(reg_mlp, color = 'tab:green')\n",
    "#ax2.plot(reg_lay, color = \"tab:gray\")\n",
    "#ax2.plot(reg_mf, color = 'tab:orange')\n",
    "#ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from numpy.random import seed\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow import set_random_seed\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pickle\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.sequence import pad_sequences\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import initializers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Embedding, Input, Dense, merge, Flatten, concatenate, multiply, dot, Reshape, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras.callbacks\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from time import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pdb\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from scipy import sparse\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sqlalchemy as db\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sqlalchemy import create_engine\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import psycopg2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import roc_auc_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import hyperopt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from matplotlib import pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'reg_mlp': hp.loguniform('reg_mlp',  -7, 0),\n",
      "        'reg_mlp_1': hp.loguniform('reg_mlp_1',  -7, 0),\n",
      "        'reg_mlp_2': hp.loguniform('reg_mlp_2',  -7, 0),\n",
      "        'lr': hp.loguniform('lr', -7, 0),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: data_bundle = pd.read_pickle(\"training_data/sample_100000.pkl\")\n",
      "   3: max_length = 200\n",
      "   4: #data_bundle = data_bundle.dropna().reset_index()\n",
      "   5: # context\n",
      "   6: month = np.nan_to_num(data_bundle.month_enc.values.reshape(-1,1))\n",
      "   7: days_online = np.nan_to_num(data_bundle.days_online_log_std.values.reshape(-1,1))#, dtype = \"float32\")\n",
      "   8: # item\n",
      "   9: item_anbieter = np.nan_to_num(data_bundle.anbieterid_enc.values.reshape(-1,1)) # nur weil es noch Nan gab - fix ich noch\n",
      "  10: item_mkt= np.nan_to_num(data_bundle.anbietermarktplatz_enc.values.reshape(-1,1))\n",
      "  11: item_wg = np.nan_to_num(data_bundle.warengruppe_enc.values.reshape(-1,1))\n",
      "  12: item_preis = np.nan_to_num(data_bundle.preis_log_std.values.reshape(-1,1))\n",
      "  13: item_ve = np.nan_to_num(data_bundle.minve_log_std.values.reshape(-1,1))\n",
      "  14: list_text = []\n",
      "  15: list_text_user = []\n",
      "  16: for i in range(len(data_bundle)):\n",
      "  17:     list_text.append(data_bundle.text_vec[i])\n",
      "  18:     list_text_user.append(data_bundle.text_vec_user[i])\n",
      "  19: item_text = np.array(list_text, ndmin = 2)\n",
      "  20: \n",
      "  21: # user\n",
      "  22: user_text = np.array(list_text_user, ndmin = 2)\n",
      "  23: user_mkt = np.nan_to_num(data_bundle.usermkt_enc.values.reshape(-1,1))\n",
      "  24: \n",
      "  25: user_anbieter = pad_sequences(data_bundle.anbieterid_enc_user, maxlen = max_length, padding = \"pre\")\n",
      "  26: user_anbietermkt = pad_sequences(data_bundle.anbietermarktplatz_enc_user, maxlen = max_length, padding = \"pre\")\n",
      "  27: user_wg = pad_sequences(data_bundle.warengruppe_enc_user, maxlen = max_length, padding = \"pre\")\n",
      "  28: user_preis = np.nan_to_num(data_bundle.preis_log_std_user.values.reshape(-1,1))\n",
      "  29: user_ve = np.nan_to_num(data_bundle.minve_log_std_user.values.reshape(-1,1))\n",
      "  30: \n",
      "  31: # features \n",
      "  32: x_train = [month[:90000], days_online[:90000], item_anbieter[:90000], item_mkt[:90000], item_wg[:90000], item_preis[:90000], item_ve[:90000], item_text[:90000], user_mkt[:90000], user_anbietermkt[:90000], user_wg[:90000], user_anbieter[:90000], user_preis[:90000], user_ve[:90000], user_text[:90000]]\n",
      "  33: x_test = [month[90000:], days_online[90000:], item_anbieter[90000:], item_mkt[90000:], item_wg[90000:], item_preis[90000:], item_ve[90000:], item_text[90000:], user_mkt[90000:], user_anbietermkt[90000:], user_wg[90000:], user_anbieter[90000:], user_preis[90000:], user_ve[90000:], user_text[90000:]]\n",
      "  34: # target\n",
      "  35: y_train = data_bundle.pick.values.reshape(-1,1)[:90000]\n",
      "  36: y_test = data_bundle.pick.values.reshape(-1,1)[90000:]\n",
      "  37: \n",
      "  38: \n",
      "  39: \n",
      "  40: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "    1: def keras_fmin_fnct(space):\n",
      "    2: \n",
      "    3:     layers = [256, 128,64,32] \n",
      "    4:     reg_mlp = space['reg_mlp'] \n",
      "    5:     reg_lay = space['reg_mlp_1'] \n",
      "    6:     reg_layers = [reg_mlp, reg_lay, reg_lay, reg_lay]\n",
      "    7:     reg_mf = space['reg_mlp_2'] \n",
      "    8:     \n",
      "    9:     def mask_aware_mean(x):\n",
      "   10:     # recreate the masks - all zero rows have been masked\n",
      "   11:         mask = K.not_equal(K.sum(K.abs(x), axis=2, keepdims=True), 0)\n",
      "   12:         # number of that rows are not all zeros\n",
      "   13:         n = K.sum(K.cast(mask, 'float32'), axis=1, keepdims=False)\n",
      "   14:         # compute mask-aware mean of x\n",
      "   15:         x_mean = K.sum(x, axis=1, keepdims=False) / n\n",
      "   16:         return x_mean \n",
      "   17: \n",
      "   18:     lrelu = lambda x: tf.keras.activations.relu(x, alpha=0.1)\n",
      "   19:     \n",
      "   20:     def get_model(layers, reg_layers, reg_mf):\n",
      "   21:         assert len(layers) == len(reg_layers)\n",
      "   22:         num_layer = len(layers) #Number of layers in the MLP\n",
      "   23:         ### Input variables\n",
      "   24:         max_length = 200\n",
      "   25:         months = 13\n",
      "   26:         supplier = 563\n",
      "   27:         wgs = 230\n",
      "   28:         mkt = 9\n",
      "   29: \n",
      "   30:         months_emb = round(months ** 0.25)\n",
      "   31:         supplier_emb = round(supplier ** 0.25)\n",
      "   32:         wgs_emb = round(wgs ** 0.25)\n",
      "   33:         mkt_emb = round(mkt ** 0.25)\n",
      "   34:         \n",
      "   35:         # Context\n",
      "   36:         month = Input(shape = (1,), dtype = \"float32\", name = \"month\")\n",
      "   37:         day_online = Input(shape = (1,), dtype = \"float32\", name = \"days_online\")\n",
      "   38:         # Item\n",
      "   39:         item_anbieter = Input(shape = (1,), dtype = \"float32\", name = \"item_anbieter\")\n",
      "   40:         item_mkt = Input(shape = (1,), dtype = \"float32\", name = \"item_mkt\")\n",
      "   41:         item_wg = Input(shape = (1,), dtype = \"float32\", name = \"item_wg\")\n",
      "   42:         item_preis = Input(shape = (1,), dtype = \"float32\", name = \"item_preis\")\n",
      "   43:         item_ve = Input(shape = (1,), dtype = \"float32\", name = \"item_ve\")\n",
      "   44:         item_text = Input(shape = (150,), dtype = \"float32\", name = \"item_text\")\n",
      "   45: \n",
      "   46:         # User\n",
      "   47:         user_mkt = Input(shape = (1,), dtype = \"float32\", name = \"user_mkt\")\n",
      "   48:         user_anbieter = Input(shape = (max_length,), dtype = \"float32\", name = \"user_anbieter\")\n",
      "   49:         user_anbietermkt = Input(shape = (max_length,), dtype = \"float32\", name = \"user_anbietermkt\")\n",
      "   50:         user_wg = Input(shape = (max_length,), dtype = \"float32\", name = \"user_wg\")\n",
      "   51:         user_preis = Input(shape = (1,), dtype = \"float32\", name = \"user_preis\")\n",
      "   52:         user_ve = Input(shape = (1,), dtype = \"float32\", name = \"user_ve\")\n",
      "   53:         user_text = Input(shape = (150,), dtype = \"float32\", name = \"user_text\")\n",
      "   54: \n",
      "   55: \n",
      "   56: \n",
      "   57:         ### Embedding layer\n",
      "   58:         # MF\n",
      "   59:         # Context\n",
      "   60:         MF_Embedding_Context_month = Embedding(input_dim=months, output_dim=months_emb, name='mf_embedding_month',\n",
      "   61:                                       embeddings_initializer=initializers.random_normal(),\n",
      "   62:                                       embeddings_regularizer=l2(reg_mf), input_length=1)\n",
      "   63: \n",
      "   64:         # Item\n",
      "   65: \n",
      "   66:         MF_Embedding_Item_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mf_embedding_item_anbieter',\n",
      "   67:                                       embeddings_initializer=initializers.random_normal(),\n",
      "   68:                                       embeddings_regularizer=l2(reg_mf),input_length=1)\n",
      "   69:         MF_Embedding_Item_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mf_embedding_item_mkt',\n",
      "   70:                                       embeddings_initializer=initializers.random_normal(),\n",
      "   71:                                       embeddings_regularizer=l2(reg_mf), input_length=1)\n",
      "   72: \n",
      "   73:         MF_Embedding_Item_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mf_embedding_item_wg',\n",
      "   74:                                       embeddings_initializer=initializers.random_normal(),\n",
      "   75:                                       embeddings_regularizer=l2(reg_mf), input_length=1)\n",
      "   76: \n",
      "   77: \n",
      "   78:         # User\n",
      "   79: \n",
      "   80:         MF_Embedding_User_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mf_embedding_user_mkt',\n",
      "   81:                                           embeddings_initializer=initializers.random_normal(),\n",
      "   82:                                           embeddings_regularizer=l2(reg_mf),\n",
      "   83:                                           #mask_zero = True,\n",
      "   84:                                           input_length=1)\n",
      "   85:         MF_Embedding_User_anbietermkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mf_embedding_user_anbietermkt',\n",
      "   86:                                                   embeddings_initializer=initializers.random_normal(),\n",
      "   87:                                                   embeddings_regularizer=l2(reg_mf),\n",
      "   88:                                                   mask_zero = True,\n",
      "   89:                                                   input_length=max_length)\n",
      "   90:         MF_Embedding_User_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mf_embedding_user_wg',\n",
      "   91:                                          embeddings_initializer=initializers.random_normal(),\n",
      "   92:                                          embeddings_regularizer=l2(reg_mf),\n",
      "   93:                                          mask_zero = True,\n",
      "   94:                                          input_length=max_length)\n",
      "   95:         MF_Embedding_User_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mf_embedding_user_anbieter',\n",
      "   96:                                                embeddings_initializer=initializers.random_normal(),\n",
      "   97:                                                embeddings_regularizer=l2(reg_mf),\n",
      "   98:                                                mask_zero = True,\n",
      "   99:                                                input_length=max_length)\n",
      "  100: \n",
      "  101: \n",
      "  102:         # MLP\n",
      "  103:         #Context\n",
      "  104: \n",
      "  105:         MLP_Embedding_Context_month = Embedding(input_dim=months, output_dim=months_emb, name='mlp_embedding_month',\n",
      "  106:                                       embeddings_initializer=initializers.random_normal(),\n",
      "  107:                                       embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
      "  108: \n",
      "  109: \n",
      "  110:         # Item\n",
      "  111: \n",
      "  112:         MLP_Embedding_Item_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mlp_embedding_item_anbieter',\n",
      "  113:                                       embeddings_initializer=initializers.random_normal(),\n",
      "  114:                                       embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
      "  115:         MLP_Embedding_Item_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mlp_embedding_item_mkt',\n",
      "  116:                                       embeddings_initializer=initializers.random_normal(),\n",
      "  117:                                       embeddings_regularizer=l2(reg_layers[0]),  \n",
      "  118:                                            input_length=1)\n",
      "  119: \n",
      "  120:         MLP_Embedding_Item_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mlp_embedding_item_wg',\n",
      "  121:                                       embeddings_initializer=initializers.random_normal(),\n",
      "  122:                                       embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
      "  123: \n",
      "  124: \n",
      "  125:         # User\n",
      "  126: \n",
      "  127:         MLP_Embedding_User_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mlp_embedding_user_mkt',\n",
      "  128:                                           embeddings_initializer=initializers.random_normal(),\n",
      "  129:                                           embeddings_regularizer=l2(reg_layers[0]),\n",
      "  130:                                           #mask_zero = True,\n",
      "  131:                                           input_length=1)\n",
      "  132:         MLP_Embedding_User_anbietermkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mlp_embedding_user_anbietermkt',\n",
      "  133:                                                   embeddings_initializer=initializers.random_normal(),\n",
      "  134:                                                   embeddings_regularizer=l2(reg_layers[0]),\n",
      "  135:                                                   mask_zero = True,\n",
      "  136:                                                   input_length=max_length)\n",
      "  137:         MLP_Embedding_User_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mlp_embedding_user_wg',\n",
      "  138:                                          embeddings_initializer=initializers.random_normal(),\n",
      "  139:                                          embeddings_regularizer=l2(reg_layers[0]),\n",
      "  140:                                          mask_zero = True,\n",
      "  141:                                          input_length=max_length)\n",
      "  142:         MLP_Embedding_User_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mlp_embedding_user_anbieter',\n",
      "  143:                                                embeddings_initializer=initializers.random_normal(),\n",
      "  144:                                                embeddings_regularizer=l2(reg_layers[0]),\n",
      "  145:                                                #mask_zero = True,\n",
      "  146:                                                input_length=max_length)\n",
      "  147: \n",
      "  148:         ## all user ones zb. and then concenate / multiply in MF \n",
      "  149:         # MF part\n",
      "  150:         ## context \n",
      "  151:         emb_item_month = Flatten(name = \"mf_flat_month\")(MF_Embedding_Context_month(month))\n",
      "  152: \n",
      "  153:         ## user\n",
      "  154:         emb_user_mkt = Flatten(name =\"mf_flat_user_mkt\")(MF_Embedding_User_mkt(user_mkt))\n",
      "  155:         #     emb_user_anbietermkt = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mf_avg_user_anbietermkt\"))(MF_Embedding_User_anbietermkt(user_anbietermkt))\n",
      "  156:         emb_user_wg = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mf_avg_user_wg\"))(MF_Embedding_User_wg(user_wg))\n",
      "  157:         emb_user_anbieter = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mf_avg_user_anbieter\"))(MF_Embedding_User_anbieter(user_anbieter))\n",
      "  158: \n",
      "  159:         ## item\n",
      "  160:         emb_item_anbieter = Flatten(name = \"mf_flat_item_anbieter\")(MF_Embedding_Item_anbieter(item_anbieter))\n",
      "  161:         emb_item_mkt = Flatten(name = \"mf_flat_item_mkt\")(MF_Embedding_Item_mkt(item_mkt))\n",
      "  162:         emb_item_wg = Flatten(name = \"mf_flat_item_wg\")(MF_Embedding_Item_wg(item_wg))\n",
      "  163: \n",
      "  164:         # MF connect\n",
      "  165:         mf_vector = multiply([concatenate([emb_user_mkt, emb_user_wg, emb_user_anbieter, user_preis, user_ve, user_text,\n",
      "  166:                                            day_online, emb_item_month], name = \"mf_user\"), \n",
      "  167:                               concatenate([emb_item_mkt, emb_item_wg, emb_item_anbieter, item_preis, item_ve, item_text,\n",
      "  168:                                            day_online, emb_item_month], name = \"mf_item\")], \n",
      "  169:                              name = \"mf_multiply\")\n",
      "  170: \n",
      "  171: \n",
      "  172:         # MLP part\n",
      "  173:         ## context \n",
      "  174:         emb_item_month2 = Flatten(name = \"mlp_flat_month\")(MLP_Embedding_Context_month(month))\n",
      "  175:         ## user\n",
      "  176:         emb_user_mkt2 = Flatten(name =\"mlp_flat_user_mkt\")(MLP_Embedding_User_mkt(user_mkt))\n",
      "  177:         emb_user_anbietermkt2 = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mlp_avg_user_anbietermkt\"))(MLP_Embedding_User_anbietermkt(user_anbietermkt))\n",
      "  178:         emb_user_wg2 = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mlp_avg_user_wg\"))(MLP_Embedding_User_wg(user_wg))\n",
      "  179:         emb_user_anbieter2 = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mlp_avg_user_anbieten\"))(MLP_Embedding_User_anbieter(user_anbieter))\n",
      "  180: \n",
      "  181:         ## item\n",
      "  182:         emb_item_anbieter2 = Flatten(name = \"mlp_flat_item_anbieter\")(MLP_Embedding_Item_anbieter(item_anbieter))\n",
      "  183:         emb_item_mkt2 = Flatten(name = \"mlp_flat_item_mkt\")(MLP_Embedding_Item_mkt(item_mkt))\n",
      "  184:         emb_item_wg2 = Flatten(name = \"mlp_flat_item_wg\")(MLP_Embedding_Item_wg(item_wg))\n",
      "  185: \n",
      "  186:         mlp_vector = concatenate([emb_item_month2, day_online,\n",
      "  187:                                   emb_user_mkt2, emb_user_anbietermkt2, emb_user_wg2, emb_user_anbieter2, user_preis, user_ve, user_text,\n",
      "  188:                                   emb_item_anbieter2, emb_item_mkt2, emb_item_wg2, item_preis, item_ve, item_text], name = \"mlp_conc\")\n",
      "  189: \n",
      "  190:         for idx in range(1, num_layer):\n",
      "  191:             layer = Dense(layers[idx], kernel_regularizer=l2(reg_layers[idx]), activation=lrelu, name=\"layer%d\" % idx)\n",
      "  192:             mlp_vector = layer(mlp_vector)\n",
      "  193: \n",
      "  194:         # Concatenate MF and MLP parts\n",
      "  195:         predict_vector = concatenate([mf_vector, mlp_vector])\n",
      "  196: \n",
      "  197:         # Final prediction layer\n",
      "  198:         prediction = Dense(1, activation='sigmoid', kernel_initializer=initializers.lecun_normal(),\n",
      "  199:                            name=\"prediction\")(predict_vector)\n",
      "  200: \n",
      "  201:         model_ = Model(inputs=[month, day_online,\n",
      "  202:                                item_anbieter, item_mkt, item_wg, item_preis, item_ve, user_text,\n",
      "  203:                                user_mkt, user_anbietermkt, user_wg, user_anbieter, user_preis, user_ve, item_text],\n",
      "  204:                        outputs=prediction)\n",
      "  205: \n",
      "  206:         return model_\n",
      "  207: \n",
      "  208:     \n",
      "  209:     # load functino to evaluate performance      \n",
      "  210:     def auroc(y_true, y_pred):\n",
      "  211:         return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)\n",
      "  212:                   \n",
      "  213: \n",
      "  214: \n",
      "  215:     # create Model \n",
      "  216:     \n",
      "  217:     if 'results' not in globals():\n",
      "  218:         global results\n",
      "  219:         results = []\n",
      "  220:         \n",
      "  221:     model = get_model(layers, reg_layers, reg_mf)\n",
      "  222:     model.compile(optimizer=Adam(lr=space['lr']), loss='binary_crossentropy', metrics = [auroc])\n",
      "  223:     result = model.fit(x_train, y_train, batch_size = 256, epochs = 10, verbose = 2, validation_split=0.1)\n",
      "  224:     validation_auc =np.amax(result.history['val_auroc'])\n",
      "  225:     \n",
      "  226:     valLoss = result.history['val_loss'][-1]\n",
      "  227:     parameters = space\n",
      "  228:     parameters[\"val_loss\"] = valLoss\n",
      "  229:     parameters[\"best_val_auc\"] = validation_auc\n",
      "  230:     results.append(parameters)\n",
      "  231:     with open(\"models/tuning/test_3.pkl\", 'wb') as fp:\n",
      "  232:         pickle.dump(results, fp)\n",
      "  233:     print('Best validation auc of epoch:', validation_auc)\n",
      "  234:     return {'loss': -validation_auc, 'status': STATUS_OK, 'model': model} # minimizes based on validation_auc\n",
      "  235: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81000 samples, validate on 9000 samples    \n",
      "Epoch 1/10                                          \n",
      " - 6s - loss: 1.0205 - auroc: 0.5587 - val_loss: 0.6187 - val_auroc: 0.5372\n",
      "\n",
      "Epoch 2/10                                          \n",
      " - 4s - loss: 0.6034 - auroc: 0.5626 - val_loss: 0.6161 - val_auroc: 0.5478\n",
      "\n",
      "Epoch 3/10                                          \n",
      " - 4s - loss: 0.6060 - auroc: 0.5620 - val_loss: 0.6282 - val_auroc: 0.5505\n",
      "\n",
      "Epoch 4/10                                          \n",
      " - 4s - loss: 0.6053 - auroc: 0.5624 - val_loss: 0.6258 - val_auroc: 0.5460\n",
      "\n",
      "Epoch 5/10                                          \n",
      " - 4s - loss: 0.6144 - auroc: 0.5585 - val_loss: 0.6211 - val_auroc: 0.5572\n",
      "\n",
      "Epoch 6/10                                          \n",
      " - 4s - loss: 0.6168 - auroc: 0.5585 - val_loss: 0.6480 - val_auroc: 0.5362\n",
      "\n",
      "Epoch 7/10                                          \n",
      " - 4s - loss: 0.6187 - auroc: 0.5602 - val_loss: 0.6554 - val_auroc: 0.5561\n",
      "\n",
      "Epoch 8/10                                          \n",
      " - 4s - loss: 0.6228 - auroc: 0.5605 - val_loss: 0.6290 - val_auroc: 0.5509\n",
      "\n",
      "Epoch 9/10                                          \n",
      " - 4s - loss: 0.6237 - auroc: 0.5610 - val_loss: 0.6361 - val_auroc: 0.5661\n",
      "\n",
      "Epoch 10/10                                         \n",
      " - 4s - loss: 0.6265 - auroc: 0.5632 - val_loss: 0.6493 - val_auroc: 0.5511\n",
      "\n",
      "Best validation auc of epoch:                       \n",
      "0.5661081075668335                                  \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 6s - loss: 8037.7636 - auroc: 0.5215 - val_loss: 26.1523 - val_auroc: 0.5565\n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 4s - loss: 6.4416 - auroc: 0.5327 - val_loss: 1.1110 - val_auroc: 0.5141   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 4s - loss: 0.9182 - auroc: 0.5273 - val_loss: 1.3032 - val_auroc: 0.5106   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 4s - loss: 0.8725 - auroc: 0.5264 - val_loss: 0.8664 - val_auroc: 0.5318   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 0.8633 - auroc: 0.5256 - val_loss: 0.7868 - val_auroc: 0.5185   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 4s - loss: 1.0554 - auroc: 0.5265 - val_loss: 1.6909 - val_auroc: 0.5103   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 4s - loss: 2.0846 - auroc: 0.5251 - val_loss: 1.4622 - val_auroc: 0.5134   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 4s - loss: 1.0979 - auroc: 0.5296 - val_loss: 1.1032 - val_auroc: 0.5361   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 4s - loss: 1.8704 - auroc: 0.5261 - val_loss: 1.9108 - val_auroc: 0.5445   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 1.4984 - auroc: 0.5232 - val_loss: 1.6296 - val_auroc: 0.5105   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.5565431118011475                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 6s - loss: 0.8249 - auroc: 0.5832 - val_loss: 0.6005 - val_auroc: 0.5652   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 4s - loss: 0.5890 - auroc: 0.5821 - val_loss: 0.6049 - val_auroc: 0.5691   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 4s - loss: 0.5902 - auroc: 0.5793 - val_loss: 0.6017 - val_auroc: 0.5689   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 4s - loss: 0.5896 - auroc: 0.5815 - val_loss: 0.6012 - val_auroc: 0.5770   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 0.5895 - auroc: 0.5828 - val_loss: 0.6013 - val_auroc: 0.5709   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 4s - loss: 0.5890 - auroc: 0.5828 - val_loss: 0.6013 - val_auroc: 0.5661   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 4s - loss: 0.5896 - auroc: 0.5831 - val_loss: 0.6100 - val_auroc: 0.5615   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 4s - loss: 0.5899 - auroc: 0.5847 - val_loss: 0.6067 - val_auroc: 0.5588   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 4s - loss: 0.5889 - auroc: 0.5822 - val_loss: 0.6004 - val_auroc: 0.5710   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 0.5892 - auroc: 0.5851 - val_loss: 0.5991 - val_auroc: 0.5740   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.576994776725769                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                   \n",
      " - 6s - loss: 9.2331 - auroc: 0.5497 - val_loss: 0.6498 - val_auroc: 0.5445  \n",
      "\n",
      "Epoch 2/10                                                                   \n",
      " - 4s - loss: 0.6461 - auroc: 0.5461 - val_loss: 0.6534 - val_auroc: 0.5569  \n",
      "\n",
      "Epoch 3/10                                                                   \n",
      " - 4s - loss: 0.6417 - auroc: 0.5465 - val_loss: 0.6608 - val_auroc: 0.5486  \n",
      "\n",
      "Epoch 4/10                                                                   \n",
      " - 4s - loss: 0.6394 - auroc: 0.5475 - val_loss: 0.6541 - val_auroc: 0.5513  \n",
      "\n",
      "Epoch 5/10                                                                   \n",
      " - 4s - loss: 0.6688 - auroc: 0.5450 - val_loss: 0.7027 - val_auroc: 0.5186  \n",
      "\n",
      "Epoch 6/10                                                                   \n",
      " - 4s - loss: 0.6573 - auroc: 0.5449 - val_loss: 0.6605 - val_auroc: 0.5382  \n",
      "\n",
      "Epoch 7/10                                                                   \n",
      " - 4s - loss: 0.6684 - auroc: 0.5455 - val_loss: 0.6925 - val_auroc: 0.5444  \n",
      "\n",
      "Epoch 8/10                                                                   \n",
      " - 4s - loss: 0.6565 - auroc: 0.5419 - val_loss: 0.6804 - val_auroc: 0.5352  \n",
      "\n",
      "Epoch 9/10                                                                   \n",
      " - 4s - loss: 0.6589 - auroc: 0.5405 - val_loss: 0.6545 - val_auroc: 0.5322  \n",
      "\n",
      "Epoch 10/10                                                                  \n",
      " - 4s - loss: 0.6502 - auroc: 0.5434 - val_loss: 0.6428 - val_auroc: 0.5533  \n",
      "\n",
      "Best validation auc of epoch:                                                \n",
      "0.5568708777427673                                                           \n",
      "Train on 81000 samples, validate on 9000 samples                             \n",
      "Epoch 1/10                                                                   \n",
      " - 6s - loss: 0.9338 - auroc: 0.5611 - val_loss: 0.6180 - val_auroc: 0.5515  \n",
      "\n",
      "Epoch 2/10                                                                   \n",
      " - 4s - loss: 0.6028 - auroc: 0.5631 - val_loss: 0.6178 - val_auroc: 0.5419  \n",
      "\n",
      "Epoch 3/10                                                                   \n",
      " - 4s - loss: 0.6048 - auroc: 0.5618 - val_loss: 0.6123 - val_auroc: 0.5669  \n",
      "\n",
      "Epoch 4/10                                                                   \n",
      " - 5s - loss: 0.6005 - auroc: 0.5652 - val_loss: 0.6190 - val_auroc: 0.5496  \n",
      "\n",
      "Epoch 5/10                                                                   \n",
      " - 4s - loss: 0.6097 - auroc: 0.5662 - val_loss: 0.6236 - val_auroc: 0.5431  \n",
      "\n",
      "Epoch 6/10                                                                   \n",
      " - 4s - loss: 0.6093 - auroc: 0.5621 - val_loss: 0.6278 - val_auroc: 0.5507  \n",
      "\n",
      "Epoch 7/10                                                                   \n",
      " - 4s - loss: 0.6121 - auroc: 0.5651 - val_loss: 0.6219 - val_auroc: 0.5644  \n",
      "\n",
      "Epoch 8/10                                                                   \n",
      " - 4s - loss: 0.6142 - auroc: 0.5639 - val_loss: 0.6299 - val_auroc: 0.5425  \n",
      "\n",
      "Epoch 9/10                                                                   \n",
      " - 4s - loss: 0.6150 - auroc: 0.5622 - val_loss: 0.6472 - val_auroc: 0.5478  \n",
      "\n",
      "Epoch 10/10                                                                  \n",
      " - 4s - loss: 0.6170 - auroc: 0.5651 - val_loss: 0.6312 - val_auroc: 0.5510  \n",
      "\n",
      "Best validation auc of epoch:                                                \n",
      "0.5669046640396118                                                           \n",
      "Train on 81000 samples, validate on 9000 samples                             \n",
      "Epoch 1/10                                                                   \n",
      " - 6s - loss: 6935.5868 - auroc: 0.5301 - val_loss: 409.9037 - val_auroc: 0.5535\n",
      "\n",
      "Epoch 2/10                                                                   \n",
      " - 4s - loss: 196.1898 - auroc: 0.5619 - val_loss: 76.8755 - val_auroc: 0.5698\n",
      "\n",
      "Epoch 3/10                                                                   \n",
      " - 4s - loss: 38.8750 - auroc: 0.5484 - val_loss: 16.5995 - val_auroc: 0.5115\n",
      "\n",
      "Epoch 4/10                                                                   \n",
      " - 4s - loss: 8.5961 - auroc: 0.5359 - val_loss: 3.9813 - val_auroc: 0.5275  \n",
      "\n",
      "Epoch 5/10                                                                   \n",
      " - 4s - loss: 2.4332 - auroc: 0.5329 - val_loss: 1.9795 - val_auroc: 0.5367  \n",
      "\n",
      "Epoch 6/10                                                                   \n",
      " - 4s - loss: 1.3006 - auroc: 0.5286 - val_loss: 1.1418 - val_auroc: 0.5397  \n",
      "\n",
      "Epoch 7/10                                                                   \n",
      " - 4s - loss: 1.1686 - auroc: 0.5257 - val_loss: 1.0987 - val_auroc: 0.5197  \n",
      "\n",
      "Epoch 8/10                                                                   \n",
      " - 4s - loss: 1.0947 - auroc: 0.5273 - val_loss: 1.1375 - val_auroc: 0.5386  \n",
      "\n",
      "Epoch 9/10                                                                   \n",
      " - 4s - loss: 1.1146 - auroc: 0.5291 - val_loss: 1.2503 - val_auroc: 0.5200  \n",
      "\n",
      "Epoch 10/10                                                                  \n",
      " - 4s - loss: 1.1685 - auroc: 0.5258 - val_loss: 1.1198 - val_auroc: 0.5468  \n",
      "\n",
      "Best validation auc of epoch:                                                \n",
      "0.569757878780365                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                             \n",
      "Epoch 1/10                                                                   \n",
      " - 6s - loss: 0.6472 - auroc: 0.5913 - val_loss: 0.6064 - val_auroc: 0.6050  \n",
      "\n",
      "Epoch 2/10                                                                   \n",
      " - 4s - loss: 0.6105 - auroc: 0.5916 - val_loss: 0.6530 - val_auroc: 0.5857  \n",
      "\n",
      "Epoch 3/10                                                                   \n",
      " - 4s - loss: 0.6139 - auroc: 0.5919 - val_loss: 0.6082 - val_auroc: 0.5851  \n",
      "\n",
      "Epoch 4/10                                                                   \n",
      " - 4s - loss: 0.6077 - auroc: 0.6034 - val_loss: 0.6118 - val_auroc: 0.5963  \n",
      "\n",
      "Epoch 5/10                                                                   \n",
      " - 4s - loss: 0.6135 - auroc: 0.6017 - val_loss: 0.6318 - val_auroc: 0.5869  \n",
      "\n",
      "Epoch 6/10                                                                   \n",
      " - 4s - loss: 0.6128 - auroc: 0.6032 - val_loss: 0.6255 - val_auroc: 0.5823  \n",
      "\n",
      "Epoch 7/10                                                                   \n",
      " - 4s - loss: 0.6087 - auroc: 0.6041 - val_loss: 0.6165 - val_auroc: 0.5675  \n",
      "\n",
      "Epoch 8/10                                                                   \n",
      " - 4s - loss: 0.6115 - auroc: 0.6025 - val_loss: 0.6482 - val_auroc: 0.5969  \n",
      "\n",
      "Epoch 9/10                                                                   \n",
      " - 4s - loss: 19.2506 - auroc: 0.5753 - val_loss: 14.7363 - val_auroc: 0.5315\n",
      "\n",
      "Epoch 10/10                                                                  \n",
      " - 4s - loss: 5.9261 - auroc: 0.5728 - val_loss: 2.9916 - val_auroc: 0.5738  \n",
      "\n",
      "Best validation auc of epoch:                                                \n",
      "0.6049711108207703                                                           \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 6s - loss: 2.6275 - auroc: 0.5691 - val_loss: 0.7199 - val_auroc: 0.5734   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 4s - loss: 0.6514 - auroc: 0.5759 - val_loss: 0.6849 - val_auroc: 0.5477   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 4s - loss: 0.6411 - auroc: 0.5716 - val_loss: 0.6201 - val_auroc: 0.5826   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 4s - loss: 0.6374 - auroc: 0.5713 - val_loss: 0.7177 - val_auroc: 0.5603   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 0.6348 - auroc: 0.5593 - val_loss: 0.6711 - val_auroc: 0.5430   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 4s - loss: 0.6365 - auroc: 0.5560 - val_loss: 0.6904 - val_auroc: 0.5439   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 5s - loss: 0.6412 - auroc: 0.5461 - val_loss: 0.6386 - val_auroc: 0.5411   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 4s - loss: 0.6397 - auroc: 0.5444 - val_loss: 0.6552 - val_auroc: 0.5259   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 4s - loss: 0.6355 - auroc: 0.5461 - val_loss: 0.6255 - val_auroc: 0.5585   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 0.6349 - auroc: 0.5439 - val_loss: 0.6379 - val_auroc: 0.5478   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.5826254487037659                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 7s - loss: 2.8383 - auroc: 0.5598 - val_loss: 0.6239 - val_auroc: 0.5451   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 4s - loss: 0.6114 - auroc: 0.5577 - val_loss: 0.6168 - val_auroc: 0.5560   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 5s - loss: 0.6075 - auroc: 0.5592 - val_loss: 0.6337 - val_auroc: 0.5476   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 4s - loss: 0.6244 - auroc: 0.5594 - val_loss: 0.7008 - val_auroc: 0.5502   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 5s - loss: 0.6485 - auroc: 0.5641 - val_loss: 0.6971 - val_auroc: 0.5473   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 5s - loss: 0.6756 - auroc: 0.5646 - val_loss: 0.6946 - val_auroc: 0.5620   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 5s - loss: 0.6930 - auroc: 0.5627 - val_loss: 0.6983 - val_auroc: 0.5518   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 5s - loss: 0.7079 - auroc: 0.5634 - val_loss: 0.7395 - val_auroc: 0.5488   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 5s - loss: 0.7280 - auroc: 0.5693 - val_loss: 0.7582 - val_auroc: 0.5650   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 0.7275 - auroc: 0.5671 - val_loss: 0.7144 - val_auroc: 0.5615   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.5649540424346924                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 7s - loss: 0.6358 - auroc: 0.6082 - val_loss: 0.6030 - val_auroc: 0.6121   \n",
      "\n",
      "Epoch 2/10                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 0.5960 - auroc: 0.6234 - val_loss: 0.6432 - val_auroc: 0.6139   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 4s - loss: 0.5939 - auroc: 0.6295 - val_loss: 0.6023 - val_auroc: 0.6196   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 5s - loss: 0.5964 - auroc: 0.6312 - val_loss: 0.6131 - val_auroc: 0.6209   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 5s - loss: 0.5940 - auroc: 0.6308 - val_loss: 0.5958 - val_auroc: 0.6410   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 5s - loss: 0.5956 - auroc: 0.6302 - val_loss: 0.5981 - val_auroc: 0.6218   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 5s - loss: 0.6294 - auroc: 0.6222 - val_loss: 0.6021 - val_auroc: 0.6246   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 5s - loss: 0.5885 - auroc: 0.6334 - val_loss: 0.5970 - val_auroc: 0.6239   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 5s - loss: 0.5855 - auroc: 0.6329 - val_loss: 0.5965 - val_auroc: 0.6194   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 5s - loss: 0.5913 - auroc: 0.6315 - val_loss: 0.6062 - val_auroc: 0.6200   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6409952044487                                                               \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                  \n",
      " - 7s - loss: 0.7072 - auroc: 0.6211 - val_loss: 0.5988 - val_auroc: 0.6198 \n",
      "\n",
      "Epoch 2/10                                                                  \n",
      " - 4s - loss: 0.5835 - auroc: 0.6412 - val_loss: 0.5936 - val_auroc: 0.6360 \n",
      "\n",
      "Epoch 3/10                                                                  \n",
      " - 4s - loss: 0.5808 - auroc: 0.6473 - val_loss: 0.5957 - val_auroc: 0.6396 \n",
      "\n",
      "Epoch 4/10                                                                  \n",
      " - 4s - loss: 0.5801 - auroc: 0.6494 - val_loss: 0.5917 - val_auroc: 0.6344 \n",
      "\n",
      "Epoch 5/10                                                                  \n",
      " - 4s - loss: 0.5809 - auroc: 0.6498 - val_loss: 0.5925 - val_auroc: 0.6363 \n",
      "\n",
      "Epoch 6/10                                                                  \n",
      " - 5s - loss: 0.5782 - auroc: 0.6517 - val_loss: 0.5906 - val_auroc: 0.6352 \n",
      "\n",
      "Epoch 7/10                                                                  \n",
      " - 4s - loss: 0.5774 - auroc: 0.6521 - val_loss: 0.5961 - val_auroc: 0.6379 \n",
      "\n",
      "Epoch 8/10                                                                  \n",
      " - 4s - loss: 0.5771 - auroc: 0.6531 - val_loss: 0.5871 - val_auroc: 0.6458 \n",
      "\n",
      "Epoch 9/10                                                                  \n",
      " - 4s - loss: 0.5798 - auroc: 0.6526 - val_loss: 0.5935 - val_auroc: 0.6377 \n",
      "\n",
      "Epoch 10/10                                                                 \n",
      " - 5s - loss: 0.5767 - auroc: 0.6540 - val_loss: 0.5909 - val_auroc: 0.6457 \n",
      "\n",
      "Best validation auc of epoch:                                               \n",
      "0.6458449959754944                                                          \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 7s - loss: 4.2631 - auroc: 0.5571 - val_loss: 0.5978 - val_auroc: 0.5732    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 5s - loss: 0.5849 - auroc: 0.5912 - val_loss: 0.5968 - val_auroc: 0.5787    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 4s - loss: 0.5844 - auroc: 0.5935 - val_loss: 0.5971 - val_auroc: 0.5750    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 5s - loss: 0.5840 - auroc: 0.5945 - val_loss: 0.5970 - val_auroc: 0.5768    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 5s - loss: 0.5840 - auroc: 0.5942 - val_loss: 0.5971 - val_auroc: 0.5765    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 5s - loss: 0.5840 - auroc: 0.5952 - val_loss: 0.5979 - val_auroc: 0.5797    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 5s - loss: 0.5842 - auroc: 0.5935 - val_loss: 0.5976 - val_auroc: 0.5753    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 4s - loss: 0.5841 - auroc: 0.5932 - val_loss: 0.5967 - val_auroc: 0.5771    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 4s - loss: 0.5840 - auroc: 0.5946 - val_loss: 0.5970 - val_auroc: 0.5790    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 4s - loss: 0.5841 - auroc: 0.5938 - val_loss: 0.5976 - val_auroc: 0.5758    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5796536207199097                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 7s - loss: 9179.1645 - auroc: 0.5299 - val_loss: 186.1256 - val_auroc: 0.5208\n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 4s - loss: 69.3531 - auroc: 0.5284 - val_loss: 22.3682 - val_auroc: 0.5545  \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 5s - loss: 13.2787 - auroc: 0.5247 - val_loss: 8.0153 - val_auroc: 0.5085   \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 5s - loss: 5.9589 - auroc: 0.5219 - val_loss: 4.4184 - val_auroc: 0.5121    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 5s - loss: 3.5548 - auroc: 0.5266 - val_loss: 2.8061 - val_auroc: 0.5314    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 5s - loss: 2.6095 - auroc: 0.5264 - val_loss: 2.5382 - val_auroc: 0.5340    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 5s - loss: 2.7338 - auroc: 0.5238 - val_loss: 2.3055 - val_auroc: 0.5478    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 5s - loss: 2.2721 - auroc: 0.5186 - val_loss: 2.1432 - val_auroc: 0.5233    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 5s - loss: 2.1006 - auroc: 0.5232 - val_loss: 2.3111 - val_auroc: 0.5303    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 5s - loss: 2.0206 - auroc: 0.5248 - val_loss: 1.7460 - val_auroc: 0.5393    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5544852018356323                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 7s - loss: 1.1781 - auroc: 0.5532 - val_loss: 0.6238 - val_auroc: 0.5366    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 4s - loss: 0.6103 - auroc: 0.5578 - val_loss: 0.6285 - val_auroc: 0.5439    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 4s - loss: 0.6117 - auroc: 0.5570 - val_loss: 0.6327 - val_auroc: 0.5609    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 5s - loss: 0.6147 - auroc: 0.5555 - val_loss: 0.6229 - val_auroc: 0.5543    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 5s - loss: 0.6554 - auroc: 0.5558 - val_loss: 0.6389 - val_auroc: 0.5458    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 5s - loss: 0.6358 - auroc: 0.5547 - val_loss: 0.6561 - val_auroc: 0.5500    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 4s - loss: 0.6584 - auroc: 0.5564 - val_loss: 1.0066 - val_auroc: 0.5376    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 4s - loss: 1475.1043 - auroc: 0.5303 - val_loss: 197.8170 - val_auroc: 0.5252\n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 4s - loss: 110.8777 - auroc: 0.5619 - val_loss: 61.0797 - val_auroc: 0.5519 \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 5s - loss: 40.8905 - auroc: 0.5705 - val_loss: 26.6206 - val_auroc: 0.5519  \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5609162449836731                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 7s - loss: 0.9000 - auroc: 0.6141 - val_loss: 0.6103 - val_auroc: 0.6116    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 4s - loss: 0.5963 - auroc: 0.6266 - val_loss: 0.5984 - val_auroc: 0.6247    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 5s - loss: 0.5985 - auroc: 0.6275 - val_loss: 0.6084 - val_auroc: 0.6270    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 5s - loss: 0.5943 - auroc: 0.6300 - val_loss: 0.6182 - val_auroc: 0.6158    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 5s - loss: 0.5977 - auroc: 0.6289 - val_loss: 0.6110 - val_auroc: 0.6262    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 5s - loss: 0.5944 - auroc: 0.6299 - val_loss: 0.6110 - val_auroc: 0.6258    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 5s - loss: 0.5989 - auroc: 0.6305 - val_loss: 0.6097 - val_auroc: 0.6190    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 5s - loss: 0.5928 - auroc: 0.6316 - val_loss: 0.6116 - val_auroc: 0.6152    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 5s - loss: 0.5992 - auroc: 0.6300 - val_loss: 0.6024 - val_auroc: 0.6229    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 5s - loss: 0.5907 - auroc: 0.6283 - val_loss: 0.6226 - val_auroc: 0.6240    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.6270108819007874                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 8s - loss: 0.7634 - auroc: 0.6068 - val_loss: 0.6379 - val_auroc: 0.6148    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 5s - loss: 0.6023 - auroc: 0.6267 - val_loss: 0.6085 - val_auroc: 0.6161    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 5s - loss: 0.6006 - auroc: 0.6283 - val_loss: 0.6046 - val_auroc: 0.6273    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 5s - loss: 0.5946 - auroc: 0.6295 - val_loss: 0.6006 - val_auroc: 0.6356    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 5s - loss: 0.5937 - auroc: 0.6312 - val_loss: 0.6038 - val_auroc: 0.6261    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 5s - loss: 0.5950 - auroc: 0.6331 - val_loss: 0.6065 - val_auroc: 0.6268    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 5s - loss: 0.5912 - auroc: 0.6321 - val_loss: 0.6064 - val_auroc: 0.6254    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 5s - loss: 0.5908 - auroc: 0.6293 - val_loss: 0.7026 - val_auroc: 0.5993    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 5s - loss: 0.5997 - auroc: 0.6331 - val_loss: 0.5985 - val_auroc: 0.6225    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 5s - loss: 0.5895 - auroc: 0.6319 - val_loss: 0.6045 - val_auroc: 0.6291    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.6355680227279663                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 8s - loss: 2.6756 - auroc: 0.5733 - val_loss: 0.6020 - val_auroc: 0.5721    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 5s - loss: 0.5899 - auroc: 0.5811 - val_loss: 0.6039 - val_auroc: 0.5638    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 5s - loss: 0.5903 - auroc: 0.5817 - val_loss: 0.6029 - val_auroc: 0.5651    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 5s - loss: 0.5896 - auroc: 0.5814 - val_loss: 0.6043 - val_auroc: 0.5635    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 5s - loss: 0.5900 - auroc: 0.5830 - val_loss: 0.6031 - val_auroc: 0.5723    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 5s - loss: 0.5900 - auroc: 0.5806 - val_loss: 0.6047 - val_auroc: 0.5702    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 5s - loss: 0.5908 - auroc: 0.5821 - val_loss: 0.6059 - val_auroc: 0.5630    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 5s - loss: 0.5915 - auroc: 0.5793 - val_loss: 0.6049 - val_auroc: 0.5712    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 5s - loss: 0.5909 - auroc: 0.5813 - val_loss: 0.6044 - val_auroc: 0.5690    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 5s - loss: 0.5912 - auroc: 0.5818 - val_loss: 0.6028 - val_auroc: 0.5721    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5722936391830444                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 8s - loss: 4.6135 - auroc: 0.5447 - val_loss: 0.6868 - val_auroc: 0.5434    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 5s - loss: 0.6528 - auroc: 0.5432 - val_loss: 0.6850 - val_auroc: 0.5409    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 5s - loss: 0.6473 - auroc: 0.5450 - val_loss: 0.6493 - val_auroc: 0.5519    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 5s - loss: 0.7068 - auroc: 0.5433 - val_loss: 1.0867 - val_auroc: 0.5339    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 5s - loss: 0.9045 - auroc: 0.5438 - val_loss: 1.1270 - val_auroc: 0.5301    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 5s - loss: 10894.1419 - auroc: 0.5406 - val_loss: 351.3992 - val_auroc: 0.5556\n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 5s - loss: 143.2920 - auroc: 0.5529 - val_loss: 56.9526 - val_auroc: 0.5364 \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 5s - loss: 36.5935 - auroc: 0.5466 - val_loss: 24.0162 - val_auroc: 0.5516  \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 5s - loss: 18.5026 - auroc: 0.5440 - val_loss: 14.5255 - val_auroc: 0.5341  \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 5s - loss: 12.2635 - auroc: 0.5439 - val_loss: 10.4601 - val_auroc: 0.5496  \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5556451678276062                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 8s - loss: 0.6283 - auroc: 0.6169 - val_loss: 0.6008 - val_auroc: 0.6090    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 5s - loss: 0.5927 - auroc: 0.6292 - val_loss: 0.6248 - val_auroc: 0.6156    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 5s - loss: 0.5915 - auroc: 0.6355 - val_loss: 0.5961 - val_auroc: 0.6240    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 5s - loss: 0.5904 - auroc: 0.6347 - val_loss: 0.6096 - val_auroc: 0.6277    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 5s - loss: 0.5899 - auroc: 0.6374 - val_loss: 0.6005 - val_auroc: 0.6273    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 5s - loss: 0.5881 - auroc: 0.6383 - val_loss: 0.5960 - val_auroc: 0.6262    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 5s - loss: 0.5878 - auroc: 0.6389 - val_loss: 0.5974 - val_auroc: 0.6356    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 5s - loss: 0.5887 - auroc: 0.6392 - val_loss: 0.6384 - val_auroc: 0.6222    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 5s - loss: 0.5928 - auroc: 0.6390 - val_loss: 0.6249 - val_auroc: 0.6300    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 5s - loss: 0.5849 - auroc: 0.6393 - val_loss: 0.5980 - val_auroc: 0.6307    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.6355649828910828                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 8s - loss: 2.7137 - auroc: 0.5418 - val_loss: 0.6025 - val_auroc: 0.5560    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 5s - loss: 0.5864 - auroc: 0.5851 - val_loss: 0.5975 - val_auroc: 0.5675    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 5s - loss: 0.5845 - auroc: 0.5910 - val_loss: 0.5984 - val_auroc: 0.5752    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 5s - loss: 0.5841 - auroc: 0.5935 - val_loss: 0.5977 - val_auroc: 0.5757    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 5s - loss: 0.5836 - auroc: 0.5954 - val_loss: 0.5977 - val_auroc: 0.5745    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 5s - loss: 0.5836 - auroc: 0.5962 - val_loss: 0.5971 - val_auroc: 0.5769    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 5s - loss: 0.5835 - auroc: 0.5962 - val_loss: 0.5970 - val_auroc: 0.5769    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 5s - loss: 0.5836 - auroc: 0.5944 - val_loss: 0.5972 - val_auroc: 0.5755    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 5s - loss: 0.5835 - auroc: 0.5953 - val_loss: 0.5969 - val_auroc: 0.5769    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 5s - loss: 0.5836 - auroc: 0.5953 - val_loss: 0.5973 - val_auroc: 0.5770    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5770218968391418                                                             \n",
      "100%|██████████| 20/20 [18:58<00:00, 60.93s/it, best loss: -0.6458449959754944]\n"
     ]
    }
   ],
   "source": [
    "# new parameters to compare based on last results - see below\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=20,\n",
    "                                      trials=Trials(), \n",
    "                                      notebook_name='04_NeuMF_hyperparameter_tuning')\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test, batch_size = 100))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load( open(\"data/models/tuning/test_3.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lr': 0.0656205850979466,\n",
       "  'reg_mlp': 0.15884742355288448,\n",
       "  'reg_mlp_1': 0.08738581654544396,\n",
       "  'reg_mlp_2': 0.01944459413647665,\n",
       "  'val_loss': 0.6492889176474678,\n",
       "  'best_val_auc': 0.5661081075668335},\n",
       " {'lr': 0.8512951542004875,\n",
       "  'reg_mlp': 0.3187547734733631,\n",
       "  'reg_mlp_1': 0.5432455392347993,\n",
       "  'reg_mlp_2': 0.013388783470501216,\n",
       "  'val_loss': 1.6295705931981406,\n",
       "  'best_val_auc': 0.5565431118011475},\n",
       " {'lr': 0.014479164219165274,\n",
       "  'reg_mlp': 0.8442806030437364,\n",
       "  'reg_mlp_1': 0.006878447898180849,\n",
       "  'reg_mlp_2': 0.009690441095631913,\n",
       "  'val_loss': 0.5990753332508935,\n",
       "  'best_val_auc': 0.576994776725769},\n",
       " {'lr': 0.1924953533171111,\n",
       "  'reg_mlp': 0.016385385158633334,\n",
       "  'reg_mlp_1': 0.027082125783875713,\n",
       "  'reg_mlp_2': 0.0010945177997624242,\n",
       "  'val_loss': 0.6428116053740184,\n",
       "  'best_val_auc': 0.5568708777427673},\n",
       " {'lr': 0.055756967340773386,\n",
       "  'reg_mlp': 0.012556020958169713,\n",
       "  'reg_mlp_1': 0.054302824180740165,\n",
       "  'reg_mlp_2': 0.4062442609912626,\n",
       "  'val_loss': 0.6312063196500143,\n",
       "  'best_val_auc': 0.5669046640396118},\n",
       " {'lr': 0.8529726820453147,\n",
       "  'reg_mlp': 0.11288174137132602,\n",
       "  'reg_mlp_1': 0.03671016989518598,\n",
       "  'reg_mlp_2': 0.017907675201271095,\n",
       "  'val_loss': 1.1198383311165703,\n",
       "  'best_val_auc': 0.569757878780365},\n",
       " {'lr': 0.040586851619818204,\n",
       "  'reg_mlp': 0.009415458233955955,\n",
       "  'reg_mlp_1': 0.001905808482823134,\n",
       "  'reg_mlp_2': 0.023583430772701388,\n",
       "  'val_loss': 2.9915618114471436,\n",
       "  'best_val_auc': 0.6049711108207703},\n",
       " {'lr': 0.15131585216838056,\n",
       "  'reg_mlp': 0.005588196971515807,\n",
       "  'reg_mlp_1': 0.0016204284310370083,\n",
       "  'reg_mlp_2': 0.0010445835360338804,\n",
       "  'val_loss': 0.6378734383583069,\n",
       "  'best_val_auc': 0.5826254487037659},\n",
       " {'lr': 0.07282060919387907,\n",
       "  'reg_mlp': 0.02036948004417064,\n",
       "  'reg_mlp_1': 0.5270379102351119,\n",
       "  'reg_mlp_2': 0.004427316949052807,\n",
       "  'val_loss': 0.7144221088621352,\n",
       "  'best_val_auc': 0.5649540424346924},\n",
       " {'lr': 0.016581527171696586,\n",
       "  'reg_mlp': 0.0017394578453197689,\n",
       "  'reg_mlp_1': 0.003949986070060234,\n",
       "  'reg_mlp_2': 0.04799360117803017,\n",
       "  'val_loss': 0.6062132487297058,\n",
       "  'best_val_auc': 0.6409952044487},\n",
       " {'lr': 0.0033276793375232316,\n",
       "  'reg_mlp': 0.03252796561081108,\n",
       "  'reg_mlp_1': 0.0071165789349766076,\n",
       "  'reg_mlp_2': 0.001096281786098028,\n",
       "  'val_loss': 0.5909177550209893,\n",
       "  'best_val_auc': 0.6458449959754944},\n",
       " {'lr': 0.0023063963767203945,\n",
       "  'reg_mlp': 0.6840263256094772,\n",
       "  'reg_mlp_1': 0.17405728750283084,\n",
       "  'reg_mlp_2': 0.008534854157290992,\n",
       "  'val_loss': 0.5975601616435581,\n",
       "  'best_val_auc': 0.5796536207199097},\n",
       " {'lr': 0.9950552312477451,\n",
       "  'reg_mlp': 0.01269799514011078,\n",
       "  'reg_mlp_1': 0.12710864076759287,\n",
       "  'reg_mlp_2': 0.15584371826526033,\n",
       "  'val_loss': 1.7460454489390056,\n",
       "  'best_val_auc': 0.5544852018356323},\n",
       " {'lr': 0.08224080529084624,\n",
       "  'reg_mlp': 0.13154799479053292,\n",
       "  'reg_mlp_1': 0.022699301611854183,\n",
       "  'reg_mlp_2': 0.8189015396140169,\n",
       "  'val_loss': 26.620556560940212,\n",
       "  'best_val_auc': 0.5609162449836731},\n",
       " {'lr': 0.023998677035500213,\n",
       "  'reg_mlp': 0.4533111062726758,\n",
       "  'reg_mlp_1': 0.0014750218191975652,\n",
       "  'reg_mlp_2': 0.7341364443114583,\n",
       "  'val_loss': 0.622554095029831,\n",
       "  'best_val_auc': 0.6270108819007874},\n",
       " {'lr': 0.017503092462877488,\n",
       "  'reg_mlp': 0.5422299161423769,\n",
       "  'reg_mlp_1': 0.0033546587756850335,\n",
       "  'reg_mlp_2': 0.08469485038689696,\n",
       "  'val_loss': 0.6044818605846829,\n",
       "  'best_val_auc': 0.6355680227279663},\n",
       " {'lr': 0.016712074905551506,\n",
       "  'reg_mlp': 0.000985495855587526,\n",
       "  'reg_mlp_1': 0.46900522178000076,\n",
       "  'reg_mlp_2': 0.07551166400132008,\n",
       "  'val_loss': 0.6028139213191138,\n",
       "  'best_val_auc': 0.5722936391830444},\n",
       " {'lr': 0.18641615780088924,\n",
       "  'reg_mlp': 0.04413313194440189,\n",
       "  'reg_mlp_1': 0.2796974144944289,\n",
       "  'reg_mlp_2': 0.006283618901957644,\n",
       "  'val_loss': 10.460111760457357,\n",
       "  'best_val_auc': 0.5556451678276062},\n",
       " {'lr': 0.011426742791061331,\n",
       "  'reg_mlp': 0.011657239690575378,\n",
       "  'reg_mlp_1': 0.004498259077586219,\n",
       "  'reg_mlp_2': 0.006215996782325133,\n",
       "  'val_loss': 0.5979868194792006,\n",
       "  'best_val_auc': 0.6355649828910828},\n",
       " {'lr': 0.0012273556845022553,\n",
       "  'reg_mlp': 0.040379423302944396,\n",
       "  'reg_mlp_1': 0.030146896079320498,\n",
       "  'reg_mlp_2': 0.6883309581619026,\n",
       "  'val_loss': 0.5973422662417094,\n",
       "  'best_val_auc': 0.5770218968391418}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of parameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualization of parameter combinationsbest_auc_1 = []\n",
    "learning_rate = []\n",
    "reg_mlp = []\n",
    "reg_lay = []\n",
    "reg_mf = []\n",
    "\n",
    "\n",
    "for i in range(0, len(results)):\n",
    "    best_auc_1.append(results[i]['best_val_auc'])\n",
    "    learning_rate.append(results[i]['lr'])\n",
    "    reg_mlp.append(results[i]['reg_mlp'])\n",
    "    reg_lay.append(results[i]['reg_mlp_1'])\n",
    "    reg_mf.append(results[i]['reg_mlp_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('experiment')\n",
    "ax1.set_ylabel('auc', color=color)\n",
    "ax1.plot( best_auc_1, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('compare', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(learning_rate, color=color)\n",
    "ax2.plot(reg_mlp, color = 'tab:green')\n",
    "ax2.plot(reg_lay, color = \"tab:gray\")\n",
    "ax2.plot(reg_mf, color = 'tab:orange')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.016581527171696586,\n",
       " 'reg_mlp': 0.0017394578453197689,\n",
       " 'reg_mlp_1': 0.003949986070060234,\n",
       " 'reg_mlp_2': 0.04799360117803017,\n",
       " 'val_loss': 0.6062132487297058,\n",
       " 'best_val_auc': 0.6409952044487}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from numpy.random import seed\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow import set_random_seed\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pickle\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.sequence import pad_sequences\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import initializers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Embedding, Input, Dense, merge, Flatten, concatenate, multiply, dot, Reshape, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras.callbacks\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from time import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pdb\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from scipy import sparse\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sqlalchemy as db\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sqlalchemy import create_engine\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import psycopg2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import roc_auc_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from matplotlib import pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'layers': hp.choice('layers', [[128,64,32,16] , [256, 128,64,32] , [512, 256, 128,64]]),\n",
      "        'layers_1': hp.uniform('layers_1', 0, 0.1),\n",
      "        'layers_2': hp.uniform('layers_2', 0, 0.1),\n",
      "        'layers_3': hp.uniform('layers_3', 0, 0.1),\n",
      "        'lr': hp.choice('lr', [0.001, 0.01, 0.0001]),\n",
      "        'batch_size': hp.choice('batch_size', [128, 256, 512]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: data_bundle = pd.read_pickle(\"training_data/sample_100000.pkl\")\n",
      "   3: max_length = 200\n",
      "   4: #data_bundle = data_bundle.dropna().reset_index()\n",
      "   5: # context\n",
      "   6: month = np.nan_to_num(data_bundle.month_enc.values.reshape(-1,1))\n",
      "   7: days_online = np.nan_to_num(data_bundle.days_online_log_std.values.reshape(-1,1))#, dtype = \"float32\")\n",
      "   8: # item\n",
      "   9: item_anbieter = np.nan_to_num(data_bundle.anbieterid_enc.values.reshape(-1,1)) # nur weil es noch Nan gab - fix ich noch\n",
      "  10: item_mkt= np.nan_to_num(data_bundle.anbietermarktplatz_enc.values.reshape(-1,1))\n",
      "  11: item_wg = np.nan_to_num(data_bundle.warengruppe_enc.values.reshape(-1,1))\n",
      "  12: item_preis = np.nan_to_num(data_bundle.preis_log_std.values.reshape(-1,1))\n",
      "  13: item_ve = np.nan_to_num(data_bundle.minve_log_std.values.reshape(-1,1))\n",
      "  14: list_text = []\n",
      "  15: list_text_user = []\n",
      "  16: for i in range(len(data_bundle)):\n",
      "  17:     list_text.append(data_bundle.text_vec[i])\n",
      "  18:     list_text_user.append(data_bundle.text_vec_user[i])\n",
      "  19: item_text = np.array(list_text, ndmin = 2)\n",
      "  20: \n",
      "  21: # user\n",
      "  22: user_text = np.array(list_text_user, ndmin = 2)\n",
      "  23: user_mkt = np.nan_to_num(data_bundle.usermkt_enc.values.reshape(-1,1))\n",
      "  24: \n",
      "  25: user_anbieter = pad_sequences(data_bundle.anbieterid_enc_user, maxlen = max_length, padding = \"pre\")\n",
      "  26: user_anbietermkt = pad_sequences(data_bundle.anbietermarktplatz_enc_user, maxlen = max_length, padding = \"pre\")\n",
      "  27: user_wg = pad_sequences(data_bundle.warengruppe_enc_user, maxlen = max_length, padding = \"pre\")\n",
      "  28: user_preis = np.nan_to_num(data_bundle.preis_log_std_user.values.reshape(-1,1))\n",
      "  29: user_ve = np.nan_to_num(data_bundle.minve_log_std_user.values.reshape(-1,1))\n",
      "  30: \n",
      "  31: # features \n",
      "  32: x_train = [month[:90000], days_online[:90000], item_anbieter[:90000], item_mkt[:90000], item_wg[:90000], item_preis[:90000], item_ve[:90000], item_text[:90000], user_mkt[:90000], user_anbietermkt[:90000], user_wg[:90000], user_anbieter[:90000], user_preis[:90000], user_ve[:90000], user_text[:90000]]\n",
      "  33: x_test = [month[90000:], days_online[90000:], item_anbieter[90000:], item_mkt[90000:], item_wg[90000:], item_preis[90000:], item_ve[90000:], item_text[90000:], user_mkt[90000:], user_anbietermkt[90000:], user_wg[90000:], user_anbieter[90000:], user_preis[90000:], user_ve[90000:], user_text[90000:]]\n",
      "  34: # target\n",
      "  35: y_train = data_bundle.pick.values.reshape(-1,1)[:90000]\n",
      "  36: y_test = data_bundle.pick.values.reshape(-1,1)[90000:]\n",
      "  37: \n",
      "  38: \n",
      "  39: \n",
      "  40: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "    1: def keras_fmin_fnct(space):\n",
      "    2: \n",
      "    3:     layers = space['layers']\n",
      "    4:     reg_mlp = 0.01 #space['layers_1']\n",
      "    5:     reg_lay = 0.01 #space['layers_2']\n",
      "    6:     reg_layers = [reg_mlp, reg_lay, reg_lay, reg_lay]\n",
      "    7:     reg_mf = 0.001 #space['layers_3']\n",
      "    8:     \n",
      "    9:     def mask_aware_mean(x):\n",
      "   10:     # recreate the masks - all zero rows have been masked\n",
      "   11:         mask = K.not_equal(K.sum(K.abs(x), axis=2, keepdims=True), 0)\n",
      "   12:         # number of that rows are not all zeros\n",
      "   13:         n = K.sum(K.cast(mask, 'float32'), axis=1, keepdims=False)\n",
      "   14:         # compute mask-aware mean of x\n",
      "   15:         x_mean = K.sum(x, axis=1, keepdims=False) / n\n",
      "   16:         return x_mean \n",
      "   17: \n",
      "   18:     lrelu = lambda x: tf.keras.activations.relu(x, alpha=0.1)\n",
      "   19:     \n",
      "   20:     def get_model(layers, reg_layers, reg_mf):\n",
      "   21:         assert len(layers) == len(reg_layers)\n",
      "   22:         num_layer = len(layers) #Number of layers in the MLP\n",
      "   23:         ### Input variables\n",
      "   24:         max_length = 200\n",
      "   25:         months = 13\n",
      "   26:         supplier = 563\n",
      "   27:         wgs = 230\n",
      "   28:         mkt = 9\n",
      "   29: \n",
      "   30:         months_emb = round(months ** 0.25)\n",
      "   31:         supplier_emb = round(supplier ** 0.25)\n",
      "   32:         wgs_emb = round(wgs ** 0.25)\n",
      "   33:         mkt_emb = round(mkt ** 0.25)\n",
      "   34:         \n",
      "   35:         # Context\n",
      "   36:         month = Input(shape = (1,), dtype = \"float32\", name = \"month\")\n",
      "   37:         day_online = Input(shape = (1,), dtype = \"float32\", name = \"days_online\")\n",
      "   38:         # Item\n",
      "   39:         item_anbieter = Input(shape = (1,), dtype = \"float32\", name = \"item_anbieter\")\n",
      "   40:         item_mkt = Input(shape = (1,), dtype = \"float32\", name = \"item_mkt\")\n",
      "   41:         item_wg = Input(shape = (1,), dtype = \"float32\", name = \"item_wg\")\n",
      "   42:         item_preis = Input(shape = (1,), dtype = \"float32\", name = \"item_preis\")\n",
      "   43:         item_ve = Input(shape = (1,), dtype = \"float32\", name = \"item_ve\")\n",
      "   44:         item_text = Input(shape = (150,), dtype = \"float32\", name = \"item_text\")\n",
      "   45: \n",
      "   46:         # User\n",
      "   47:         user_mkt = Input(shape = (1,), dtype = \"float32\", name = \"user_mkt\")\n",
      "   48:         user_anbieter = Input(shape = (max_length,), dtype = \"float32\", name = \"user_anbieter\")\n",
      "   49:         user_anbietermkt = Input(shape = (max_length,), dtype = \"float32\", name = \"user_anbietermkt\")\n",
      "   50:         user_wg = Input(shape = (max_length,), dtype = \"float32\", name = \"user_wg\")\n",
      "   51:         user_preis = Input(shape = (1,), dtype = \"float32\", name = \"user_preis\")\n",
      "   52:         user_ve = Input(shape = (1,), dtype = \"float32\", name = \"user_ve\")\n",
      "   53:         user_text = Input(shape = (150,), dtype = \"float32\", name = \"user_text\")\n",
      "   54: \n",
      "   55: \n",
      "   56: \n",
      "   57:         ### Embedding layer\n",
      "   58:         # MF\n",
      "   59:         # Context\n",
      "   60:         MF_Embedding_Context_month = Embedding(input_dim=months, output_dim=months_emb, name='mf_embedding_month',\n",
      "   61:                                       embeddings_initializer=initializers.random_normal(),\n",
      "   62:                                       embeddings_regularizer=l2(reg_mf), input_length=1)\n",
      "   63: \n",
      "   64:         # Item\n",
      "   65: \n",
      "   66:         MF_Embedding_Item_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mf_embedding_item_anbieter',\n",
      "   67:                                       embeddings_initializer=initializers.random_normal(),\n",
      "   68:                                       embeddings_regularizer=l2(reg_mf),input_length=1)\n",
      "   69:         MF_Embedding_Item_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mf_embedding_item_mkt',\n",
      "   70:                                       embeddings_initializer=initializers.random_normal(),\n",
      "   71:                                       embeddings_regularizer=l2(reg_mf), input_length=1)\n",
      "   72: \n",
      "   73:         MF_Embedding_Item_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mf_embedding_item_wg',\n",
      "   74:                                       embeddings_initializer=initializers.random_normal(),\n",
      "   75:                                       embeddings_regularizer=l2(reg_mf), input_length=1)\n",
      "   76: \n",
      "   77: \n",
      "   78:         # User\n",
      "   79: \n",
      "   80:         MF_Embedding_User_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mf_embedding_user_mkt',\n",
      "   81:                                           embeddings_initializer=initializers.random_normal(),\n",
      "   82:                                           embeddings_regularizer=l2(reg_mf),\n",
      "   83:                                           #mask_zero = True,\n",
      "   84:                                           input_length=1)\n",
      "   85:         MF_Embedding_User_anbietermkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mf_embedding_user_anbietermkt',\n",
      "   86:                                                   embeddings_initializer=initializers.random_normal(),\n",
      "   87:                                                   embeddings_regularizer=l2(reg_mf),\n",
      "   88:                                                   mask_zero = True,\n",
      "   89:                                                   input_length=max_length)\n",
      "   90:         MF_Embedding_User_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mf_embedding_user_wg',\n",
      "   91:                                          embeddings_initializer=initializers.random_normal(),\n",
      "   92:                                          embeddings_regularizer=l2(reg_mf),\n",
      "   93:                                          mask_zero = True,\n",
      "   94:                                          input_length=max_length)\n",
      "   95:         MF_Embedding_User_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mf_embedding_user_anbieter',\n",
      "   96:                                                embeddings_initializer=initializers.random_normal(),\n",
      "   97:                                                embeddings_regularizer=l2(reg_mf),\n",
      "   98:                                                mask_zero = True,\n",
      "   99:                                                input_length=max_length)\n",
      "  100: \n",
      "  101: \n",
      "  102:         # MLP\n",
      "  103:         #Context\n",
      "  104: \n",
      "  105:         MLP_Embedding_Context_month = Embedding(input_dim=months, output_dim=months_emb, name='mlp_embedding_month',\n",
      "  106:                                       embeddings_initializer=initializers.random_normal(),\n",
      "  107:                                       embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
      "  108: \n",
      "  109: \n",
      "  110:         # Item\n",
      "  111: \n",
      "  112:         MLP_Embedding_Item_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mlp_embedding_item_anbieter',\n",
      "  113:                                       embeddings_initializer=initializers.random_normal(),\n",
      "  114:                                       embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
      "  115:         MLP_Embedding_Item_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mlp_embedding_item_mkt',\n",
      "  116:                                       embeddings_initializer=initializers.random_normal(),\n",
      "  117:                                       embeddings_regularizer=l2(reg_layers[0]),  \n",
      "  118:                                            input_length=1)\n",
      "  119: \n",
      "  120:         MLP_Embedding_Item_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mlp_embedding_item_wg',\n",
      "  121:                                       embeddings_initializer=initializers.random_normal(),\n",
      "  122:                                       embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
      "  123: \n",
      "  124: \n",
      "  125:         # User\n",
      "  126: \n",
      "  127:         MLP_Embedding_User_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mlp_embedding_user_mkt',\n",
      "  128:                                           embeddings_initializer=initializers.random_normal(),\n",
      "  129:                                           embeddings_regularizer=l2(reg_layers[0]),\n",
      "  130:                                           #mask_zero = True,\n",
      "  131:                                           input_length=1)\n",
      "  132:         MLP_Embedding_User_anbietermkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mlp_embedding_user_anbietermkt',\n",
      "  133:                                                   embeddings_initializer=initializers.random_normal(),\n",
      "  134:                                                   embeddings_regularizer=l2(reg_layers[0]),\n",
      "  135:                                                   mask_zero = True,\n",
      "  136:                                                   input_length=max_length)\n",
      "  137:         MLP_Embedding_User_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mlp_embedding_user_wg',\n",
      "  138:                                          embeddings_initializer=initializers.random_normal(),\n",
      "  139:                                          embeddings_regularizer=l2(reg_layers[0]),\n",
      "  140:                                          mask_zero = True,\n",
      "  141:                                          input_length=max_length)\n",
      "  142:         MLP_Embedding_User_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mlp_embedding_user_anbieter',\n",
      "  143:                                                embeddings_initializer=initializers.random_normal(),\n",
      "  144:                                                embeddings_regularizer=l2(reg_layers[0]),\n",
      "  145:                                                #mask_zero = True,\n",
      "  146:                                                input_length=max_length)\n",
      "  147: \n",
      "  148:         ## all user ones zb. and then concenate / multiply in MF \n",
      "  149:         # MF part\n",
      "  150:         ## context \n",
      "  151:         emb_item_month = Flatten(name = \"mf_flat_month\")(MF_Embedding_Context_month(month))\n",
      "  152: \n",
      "  153:         ## user\n",
      "  154:         emb_user_mkt = Flatten(name =\"mf_flat_user_mkt\")(MF_Embedding_User_mkt(user_mkt))\n",
      "  155:         #     emb_user_anbietermkt = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mf_avg_user_anbietermkt\"))(MF_Embedding_User_anbietermkt(user_anbietermkt))\n",
      "  156:         emb_user_wg = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mf_avg_user_wg\"))(MF_Embedding_User_wg(user_wg))\n",
      "  157:         emb_user_anbieter = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mf_avg_user_anbieter\"))(MF_Embedding_User_anbieter(user_anbieter))\n",
      "  158: \n",
      "  159:         ## item\n",
      "  160:         emb_item_anbieter = Flatten(name = \"mf_flat_item_anbieter\")(MF_Embedding_Item_anbieter(item_anbieter))\n",
      "  161:         emb_item_mkt = Flatten(name = \"mf_flat_item_mkt\")(MF_Embedding_Item_mkt(item_mkt))\n",
      "  162:         emb_item_wg = Flatten(name = \"mf_flat_item_wg\")(MF_Embedding_Item_wg(item_wg))\n",
      "  163: \n",
      "  164:         # MF connect\n",
      "  165:         mf_vector = multiply([concatenate([emb_user_mkt, emb_user_wg, emb_user_anbieter, user_preis, user_ve, user_text,\n",
      "  166:                                            day_online, emb_item_month], name = \"mf_user\"), \n",
      "  167:                               concatenate([emb_item_mkt, emb_item_wg, emb_item_anbieter, item_preis, item_ve, item_text,\n",
      "  168:                                            day_online, emb_item_month], name = \"mf_item\")], \n",
      "  169:                              name = \"mf_multiply\")\n",
      "  170: \n",
      "  171: \n",
      "  172:         # MLP part\n",
      "  173:         ## context \n",
      "  174:         emb_item_month2 = Flatten(name = \"mlp_flat_month\")(MLP_Embedding_Context_month(month))\n",
      "  175:         ## user\n",
      "  176:         emb_user_mkt2 = Flatten(name =\"mlp_flat_user_mkt\")(MLP_Embedding_User_mkt(user_mkt))\n",
      "  177:         emb_user_anbietermkt2 = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mlp_avg_user_anbietermkt\"))(MLP_Embedding_User_anbietermkt(user_anbietermkt))\n",
      "  178:         emb_user_wg2 = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mlp_avg_user_wg\"))(MLP_Embedding_User_wg(user_wg))\n",
      "  179:         emb_user_anbieter2 = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mlp_avg_user_anbieten\"))(MLP_Embedding_User_anbieter(user_anbieter))\n",
      "  180: \n",
      "  181:         ## item\n",
      "  182:         emb_item_anbieter2 = Flatten(name = \"mlp_flat_item_anbieter\")(MLP_Embedding_Item_anbieter(item_anbieter))\n",
      "  183:         emb_item_mkt2 = Flatten(name = \"mlp_flat_item_mkt\")(MLP_Embedding_Item_mkt(item_mkt))\n",
      "  184:         emb_item_wg2 = Flatten(name = \"mlp_flat_item_wg\")(MLP_Embedding_Item_wg(item_wg))\n",
      "  185: \n",
      "  186:         mlp_vector = concatenate([emb_item_month2, day_online,\n",
      "  187:                                   emb_user_mkt2, emb_user_anbietermkt2, emb_user_wg2, emb_user_anbieter2, user_preis, user_ve, user_text,\n",
      "  188:                                   emb_item_anbieter2, emb_item_mkt2, emb_item_wg2, item_preis, item_ve, item_text], name = \"mlp_conc\")\n",
      "  189: \n",
      "  190:         for idx in range(1, num_layer):\n",
      "  191:             layer = Dense(layers[idx], kernel_regularizer=l2(reg_layers[idx]), activation=lrelu, name=\"layer%d\" % idx)\n",
      "  192:             mlp_vector = layer(mlp_vector)\n",
      "  193: \n",
      "  194:         # Concatenate MF and MLP parts\n",
      "  195:         predict_vector = concatenate([mf_vector, mlp_vector])\n",
      "  196: \n",
      "  197:         # Final prediction layer\n",
      "  198:         prediction = Dense(1, activation='sigmoid', kernel_initializer=initializers.lecun_normal(),\n",
      "  199:                            name=\"prediction\")(predict_vector)\n",
      "  200: \n",
      "  201:         model_ = Model(inputs=[month, day_online,\n",
      "  202:                                item_anbieter, item_mkt, item_wg, item_preis, item_ve, user_text,\n",
      "  203:                                user_mkt, user_anbietermkt, user_wg, user_anbieter, user_preis, user_ve, item_text],\n",
      "  204:                        outputs=prediction)\n",
      "  205: \n",
      "  206:         return model_\n",
      "  207: \n",
      "  208:     \n",
      "  209:     # load functino to evaluate performance      \n",
      "  210:     def auroc(y_true, y_pred):\n",
      "  211:         return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)\n",
      "  212:                   \n",
      "  213: \n",
      "  214: \n",
      "  215:     # create Model \n",
      "  216:     \n",
      "  217:     if 'results' not in globals():\n",
      "  218:         global results\n",
      "  219:         results = []\n",
      "  220:         \n",
      "  221:     model = get_model(layers, reg_layers, reg_mf)\n",
      "  222:     model.compile(optimizer=Adam(lr=space['lr']), loss='binary_crossentropy', metrics = [auroc])\n",
      "  223:     result = model.fit(x_train, y_train, batch_size = space['batch_size'], epochs = 10, verbose = 2, validation_split=0.1)\n",
      "  224:     validation_auc =np.amax(result.history['val_auroc'])\n",
      "  225:     \n",
      "  226:     valLoss = result.history['val_loss'][-1]\n",
      "  227:     parameters = space\n",
      "  228:     parameters[\"val_loss\"] = valLoss\n",
      "  229:     parameters[\"best_val_auc\"] = validation_auc\n",
      "  230:     results.append(parameters)\n",
      "  231:     with open(\"models/tuning/test_1.pkl\", 'wb') as fp:\n",
      "  232:         pickle.dump(results, fp)\n",
      "  233:     print('Best validation auc of epoch:', validation_auc)\n",
      "  234:     return {'loss': -validation_auc, 'status': STATUS_OK, 'model': model} # minimizes based on validation_auc\n",
      "  235: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 81000 samples, validate on 9000 samples    \n",
      "Epoch 1/10                                          \n",
      " - 3s - loss: 3.5194 - auroc: 0.4960 - val_loss: 3.0111 - val_auroc: 0.5085\n",
      "\n",
      "Epoch 2/10                                          \n",
      " - 3s - loss: 2.6287 - auroc: 0.5081 - val_loss: 2.2946 - val_auroc: 0.5338\n",
      "\n",
      "Epoch 3/10                                          \n",
      " - 3s - loss: 2.0255 - auroc: 0.5391 - val_loss: 1.7950 - val_auroc: 0.5595\n",
      "\n",
      "Epoch 4/10                                          \n",
      " - 3s - loss: 1.6014 - auroc: 0.5690 - val_loss: 1.4429 - val_auroc: 0.5800\n",
      "\n",
      "Epoch 5/10                                          \n",
      " - 3s - loss: 1.3028 - auroc: 0.5927 - val_loss: 1.1955 - val_auroc: 0.5956\n",
      "\n",
      "Epoch 6/10                                          \n",
      " - 3s - loss: 1.0930 - auroc: 0.6091 - val_loss: 1.0220 - val_auroc: 0.6062\n",
      "\n",
      "Epoch 7/10                                          \n",
      " - 3s - loss: 0.9459 - auroc: 0.6197 - val_loss: 0.9005 - val_auroc: 0.6109\n",
      "\n",
      "Epoch 8/10                                          \n",
      " - 3s - loss: 0.8427 - auroc: 0.6264 - val_loss: 0.8145 - val_auroc: 0.6163\n",
      "\n",
      "Epoch 9/10                                          \n",
      " - 3s - loss: 0.7701 - auroc: 0.6320 - val_loss: 0.7539 - val_auroc: 0.6205\n",
      "\n",
      "Epoch 10/10                                         \n",
      " - 3s - loss: 0.7188 - auroc: 0.6365 - val_loss: 0.7113 - val_auroc: 0.6223\n",
      "\n",
      "Best validation auc of epoch:                       \n",
      "0.6222817897796631                                  \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 4s - loss: 1.8596 - auroc: 0.5990 - val_loss: 0.6498 - val_auroc: 0.6219   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 3s - loss: 0.5991 - auroc: 0.6396 - val_loss: 0.5987 - val_auroc: 0.6272   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 3s - loss: 0.5851 - auroc: 0.6442 - val_loss: 0.5972 - val_auroc: 0.6290   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 3s - loss: 0.5835 - auroc: 0.6458 - val_loss: 0.5940 - val_auroc: 0.6335   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 3s - loss: 0.5823 - auroc: 0.6488 - val_loss: 0.5947 - val_auroc: 0.6287   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 3s - loss: 0.5804 - auroc: 0.6508 - val_loss: 0.5930 - val_auroc: 0.6321   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 3s - loss: 0.5793 - auroc: 0.6524 - val_loss: 0.5929 - val_auroc: 0.6386   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 3s - loss: 0.5786 - auroc: 0.6528 - val_loss: 0.5901 - val_auroc: 0.6391   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 3s - loss: 0.5778 - auroc: 0.6547 - val_loss: 0.5903 - val_auroc: 0.6368   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 3s - loss: 0.5768 - auroc: 0.6547 - val_loss: 0.5897 - val_auroc: 0.6362   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6390849351882935                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 6s - loss: 0.6354 - auroc: 0.5803 - val_loss: 0.6040 - val_auroc: 0.5609   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 5s - loss: 0.5902 - auroc: 0.5821 - val_loss: 0.6005 - val_auroc: 0.5696   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 5s - loss: 0.5906 - auroc: 0.5814 - val_loss: 0.6027 - val_auroc: 0.5679   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 5s - loss: 0.5898 - auroc: 0.5816 - val_loss: 0.6009 - val_auroc: 0.5711   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 5s - loss: 0.5896 - auroc: 0.5831 - val_loss: 0.6005 - val_auroc: 0.5702   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 5s - loss: 0.5897 - auroc: 0.6004 - val_loss: 0.5996 - val_auroc: 0.5930   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 5s - loss: 0.5870 - auroc: 0.6184 - val_loss: 0.6046 - val_auroc: 0.6030   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 5s - loss: 0.5858 - auroc: 0.6279 - val_loss: 0.6010 - val_auroc: 0.6027   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 5s - loss: 0.5849 - auroc: 0.6297 - val_loss: 0.5950 - val_auroc: 0.6196   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 5s - loss: 0.5836 - auroc: 0.6328 - val_loss: 0.6060 - val_auroc: 0.6160   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6196146607398987                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 6s - loss: 0.6360 - auroc: 0.5779 - val_loss: 0.5984 - val_auroc: 0.5677   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 5s - loss: 0.5890 - auroc: 0.5818 - val_loss: 0.6022 - val_auroc: 0.5658   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 5s - loss: 0.5905 - auroc: 0.5831 - val_loss: 0.6019 - val_auroc: 0.5793   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 5s - loss: 0.5903 - auroc: 0.5947 - val_loss: 0.6003 - val_auroc: 0.5793   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 5s - loss: 0.5892 - auroc: 0.5951 - val_loss: 0.6031 - val_auroc: 0.5778   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 5s - loss: 0.5891 - auroc: 0.5974 - val_loss: 0.6027 - val_auroc: 0.5818   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 5s - loss: 0.5884 - auroc: 0.5989 - val_loss: 0.6019 - val_auroc: 0.5759   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 5s - loss: 0.5882 - auroc: 0.5976 - val_loss: 0.5995 - val_auroc: 0.5888   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 5s - loss: 0.5882 - auroc: 0.5973 - val_loss: 0.6001 - val_auroc: 0.5887   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 6s - loss: 0.5877 - auroc: 0.5996 - val_loss: 0.6004 - val_auroc: 0.5853   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.5887677669525146                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 4s - loss: 1.8781 - auroc: 0.5970 - val_loss: 0.6555 - val_auroc: 0.6209   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 3s - loss: 0.6001 - auroc: 0.6408 - val_loss: 0.6009 - val_auroc: 0.6258   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 3s - loss: 0.5860 - auroc: 0.6427 - val_loss: 0.5976 - val_auroc: 0.6284   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 3s - loss: 0.5838 - auroc: 0.6452 - val_loss: 0.5948 - val_auroc: 0.6332   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 3s - loss: 0.5825 - auroc: 0.6484 - val_loss: 0.5947 - val_auroc: 0.6308   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 3s - loss: 0.5810 - auroc: 0.6494 - val_loss: 0.5941 - val_auroc: 0.6317   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 3s - loss: 0.5806 - auroc: 0.6501 - val_loss: 0.5930 - val_auroc: 0.6345   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 3s - loss: 0.5790 - auroc: 0.6524 - val_loss: 0.5912 - val_auroc: 0.6372   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 3s - loss: 0.5789 - auroc: 0.6544 - val_loss: 0.5906 - val_auroc: 0.6386   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 3s - loss: 0.5777 - auroc: 0.6547 - val_loss: 0.5902 - val_auroc: 0.6399   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6398872137069702                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 6s - loss: 2.4480 - auroc: 0.5333 - val_loss: 1.4638 - val_auroc: 0.5683   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 5s - loss: 1.0633 - auroc: 0.6005 - val_loss: 0.8266 - val_auroc: 0.6095   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 5s - loss: 0.7168 - auroc: 0.6297 - val_loss: 0.6660 - val_auroc: 0.6180   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 5s - loss: 0.6261 - auroc: 0.6380 - val_loss: 0.6200 - val_auroc: 0.6256   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 5s - loss: 0.5997 - auroc: 0.6432 - val_loss: 0.6068 - val_auroc: 0.6270   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 5s - loss: 0.5916 - auroc: 0.6447 - val_loss: 0.6027 - val_auroc: 0.6281   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 5s - loss: 0.5884 - auroc: 0.6474 - val_loss: 0.6002 - val_auroc: 0.6308   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 5s - loss: 0.5867 - auroc: 0.6491 - val_loss: 0.5985 - val_auroc: 0.6331   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 5s - loss: 0.5855 - auroc: 0.6498 - val_loss: 0.5971 - val_auroc: 0.6357   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 5s - loss: 0.5844 - auroc: 0.6504 - val_loss: 0.5966 - val_auroc: 0.6355   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6356604099273682                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 5s - loss: 1.0756 - auroc: 0.5980 - val_loss: 0.6072 - val_auroc: 0.6289   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 4s - loss: 0.5894 - auroc: 0.6384 - val_loss: 0.5980 - val_auroc: 0.6323   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 4s - loss: 0.5851 - auroc: 0.6417 - val_loss: 0.5963 - val_auroc: 0.6321   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 4s - loss: 0.5828 - auroc: 0.6453 - val_loss: 0.5942 - val_auroc: 0.6352   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 0.5822 - auroc: 0.6460 - val_loss: 0.5930 - val_auroc: 0.6357   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 4s - loss: 0.5809 - auroc: 0.6489 - val_loss: 0.5926 - val_auroc: 0.6372   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 4s - loss: 0.5796 - auroc: 0.6504 - val_loss: 0.5928 - val_auroc: 0.6372   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 4s - loss: 0.5786 - auroc: 0.6512 - val_loss: 0.5909 - val_auroc: 0.6410   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 4s - loss: 0.5777 - auroc: 0.6531 - val_loss: 0.5911 - val_auroc: 0.6395   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 0.5769 - auroc: 0.6534 - val_loss: 0.5910 - val_auroc: 0.6478   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6477921009063721                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 4s - loss: 3.4713 - auroc: 0.5009 - val_loss: 2.9845 - val_auroc: 0.5135   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 3s - loss: 2.6034 - auroc: 0.5161 - val_loss: 2.2719 - val_auroc: 0.5384   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 3s - loss: 2.0015 - auroc: 0.5473 - val_loss: 1.7723 - val_auroc: 0.5649   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 3s - loss: 1.5785 - auroc: 0.5770 - val_loss: 1.4211 - val_auroc: 0.5869   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 3s - loss: 1.2815 - auroc: 0.6015 - val_loss: 1.1754 - val_auroc: 0.6010   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 3s - loss: 1.0735 - auroc: 0.6174 - val_loss: 1.0031 - val_auroc: 0.6092   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 3s - loss: 0.9283 - auroc: 0.6274 - val_loss: 0.8831 - val_auroc: 0.6147   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 3s - loss: 0.8270 - auroc: 0.6322 - val_loss: 0.7988 - val_auroc: 0.6206   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 3s - loss: 0.7563 - auroc: 0.6372 - val_loss: 0.7404 - val_auroc: 0.6233   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 3s - loss: 0.7067 - auroc: 0.6393 - val_loss: 0.6994 - val_auroc: 0.6248   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6247851252555847                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 4s - loss: 1.5308 - auroc: 0.5805 - val_loss: 0.7072 - val_auroc: 0.6112   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 3s - loss: 0.6240 - auroc: 0.6356 - val_loss: 0.6066 - val_auroc: 0.6227   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 3s - loss: 0.5899 - auroc: 0.6412 - val_loss: 0.5990 - val_auroc: 0.6264   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 3s - loss: 0.5863 - auroc: 0.6432 - val_loss: 0.5983 - val_auroc: 0.6212   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 3s - loss: 0.5844 - auroc: 0.6436 - val_loss: 0.5967 - val_auroc: 0.6288   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 3s - loss: 0.5834 - auroc: 0.6460 - val_loss: 0.5958 - val_auroc: 0.6298   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 3s - loss: 0.5819 - auroc: 0.6489 - val_loss: 0.5948 - val_auroc: 0.6315   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10                                                                    \n",
      " - 3s - loss: 0.5809 - auroc: 0.6503 - val_loss: 0.5937 - val_auroc: 0.6321   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 3s - loss: 0.5801 - auroc: 0.6513 - val_loss: 0.5929 - val_auroc: 0.6341   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 3s - loss: 0.5792 - auroc: 0.6523 - val_loss: 0.5925 - val_auroc: 0.6369   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6368501782417297                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 4s - loss: 0.6688 - auroc: 0.6002 - val_loss: 0.6006 - val_auroc: 0.5889   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 3s - loss: 0.5868 - auroc: 0.5940 - val_loss: 0.5969 - val_auroc: 0.5727   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 3s - loss: 0.5859 - auroc: 0.5895 - val_loss: 0.5980 - val_auroc: 0.5672   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 3s - loss: 0.5864 - auroc: 0.5879 - val_loss: 0.5992 - val_auroc: 0.5680   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 3s - loss: 0.5862 - auroc: 0.5894 - val_loss: 0.5988 - val_auroc: 0.5677   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 3s - loss: 0.5860 - auroc: 0.5885 - val_loss: 0.5981 - val_auroc: 0.5667   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 3s - loss: 0.5860 - auroc: 0.5889 - val_loss: 0.6001 - val_auroc: 0.5705   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 3s - loss: 0.5858 - auroc: 0.5902 - val_loss: 0.5988 - val_auroc: 0.5703   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 3s - loss: 0.5867 - auroc: 0.5880 - val_loss: 0.5995 - val_auroc: 0.5713   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 3s - loss: 0.5860 - auroc: 0.5892 - val_loss: 0.6000 - val_auroc: 0.5658   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.5888638496398926                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 5s - loss: 0.7879 - auroc: 0.5893 - val_loss: 0.6018 - val_auroc: 0.5639    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 3s - loss: 0.5860 - auroc: 0.5889 - val_loss: 0.5995 - val_auroc: 0.5699    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 3s - loss: 0.5860 - auroc: 0.5909 - val_loss: 0.6026 - val_auroc: 0.5705    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 3s - loss: 0.5860 - auroc: 0.5892 - val_loss: 0.6053 - val_auroc: 0.5665    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 3s - loss: 0.5862 - auroc: 0.5882 - val_loss: 0.5996 - val_auroc: 0.5689    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 3s - loss: 0.5860 - auroc: 0.5899 - val_loss: 0.6029 - val_auroc: 0.5647    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 3s - loss: 0.5864 - auroc: 0.5876 - val_loss: 0.5985 - val_auroc: 0.5661    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 3s - loss: 0.5857 - auroc: 0.5903 - val_loss: 0.5992 - val_auroc: 0.5725    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 3s - loss: 0.5861 - auroc: 0.5876 - val_loss: 0.6004 - val_auroc: 0.5687    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 3s - loss: 0.5859 - auroc: 0.5899 - val_loss: 0.5988 - val_auroc: 0.5696    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.572520911693573                                                              \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 4s - loss: 0.7021 - auroc: 0.5913 - val_loss: 0.5982 - val_auroc: 0.5721    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 3s - loss: 0.5864 - auroc: 0.5883 - val_loss: 0.5976 - val_auroc: 0.5740    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 3s - loss: 0.5862 - auroc: 0.5888 - val_loss: 0.5989 - val_auroc: 0.5680    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 3s - loss: 0.5856 - auroc: 0.5900 - val_loss: 0.5976 - val_auroc: 0.5729    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 3s - loss: 0.5856 - auroc: 0.5897 - val_loss: 0.5979 - val_auroc: 0.5717    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 3s - loss: 0.5857 - auroc: 0.5905 - val_loss: 0.5986 - val_auroc: 0.5729    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 3s - loss: 0.5865 - auroc: 0.5885 - val_loss: 0.5988 - val_auroc: 0.5692    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 3s - loss: 0.5862 - auroc: 0.5877 - val_loss: 0.5967 - val_auroc: 0.5719    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 3s - loss: 0.5857 - auroc: 0.5902 - val_loss: 0.5991 - val_auroc: 0.5707    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 3s - loss: 0.5862 - auroc: 0.5886 - val_loss: 0.5989 - val_auroc: 0.5675    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5740381479263306                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 8s - loss: 0.6372 - auroc: 0.5785 - val_loss: 0.6021 - val_auroc: 0.5651    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 6s - loss: 0.5895 - auroc: 0.5820 - val_loss: 0.6024 - val_auroc: 0.5806    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 6s - loss: 0.5892 - auroc: 0.6110 - val_loss: 0.5982 - val_auroc: 0.6025    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 6s - loss: 0.5862 - auroc: 0.6194 - val_loss: 0.5964 - val_auroc: 0.6076    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 6s - loss: 0.5847 - auroc: 0.6225 - val_loss: 0.5998 - val_auroc: 0.5986    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 6s - loss: 0.5843 - auroc: 0.6268 - val_loss: 0.5964 - val_auroc: 0.6122    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 6s - loss: 0.5832 - auroc: 0.6291 - val_loss: 0.5981 - val_auroc: 0.6056    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 7s - loss: 0.5828 - auroc: 0.6285 - val_loss: 0.5934 - val_auroc: 0.6168    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 6s - loss: 0.5824 - auroc: 0.6292 - val_loss: 0.5931 - val_auroc: 0.6146    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 7s - loss: 0.5829 - auroc: 0.6314 - val_loss: 0.6003 - val_auroc: 0.6133    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.6167516708374023                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 7s - loss: 0.7642 - auroc: 0.5995 - val_loss: 0.6031 - val_auroc: 0.6172    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 6s - loss: 0.5885 - auroc: 0.6337 - val_loss: 0.5986 - val_auroc: 0.6190    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 6s - loss: 0.5851 - auroc: 0.6380 - val_loss: 0.6010 - val_auroc: 0.6258    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 6s - loss: 0.5831 - auroc: 0.6417 - val_loss: 0.5964 - val_auroc: 0.6258    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 6s - loss: 0.5812 - auroc: 0.6479 - val_loss: 0.5944 - val_auroc: 0.6384    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 6s - loss: 0.5797 - auroc: 0.6500 - val_loss: 0.5924 - val_auroc: 0.6360    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 6s - loss: 0.5787 - auroc: 0.6518 - val_loss: 0.5919 - val_auroc: 0.6355    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 6s - loss: 0.5776 - auroc: 0.6531 - val_loss: 0.5898 - val_auroc: 0.6398    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 6s - loss: 0.5771 - auroc: 0.6548 - val_loss: 0.5893 - val_auroc: 0.6419    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 6s - loss: 0.5764 - auroc: 0.6554 - val_loss: 0.5879 - val_auroc: 0.6424    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.6424117088317871                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 6s - loss: 0.7666 - auroc: 0.5829 - val_loss: 0.5993 - val_auroc: 0.5687    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 4s - loss: 0.5855 - auroc: 0.5898 - val_loss: 0.5976 - val_auroc: 0.5738    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 4s - loss: 0.5859 - auroc: 0.5902 - val_loss: 0.5979 - val_auroc: 0.5743    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 3s - loss: 0.5859 - auroc: 0.5896 - val_loss: 0.5981 - val_auroc: 0.5689    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 4s - loss: 0.5860 - auroc: 0.5884 - val_loss: 0.5978 - val_auroc: 0.5705    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 4s - loss: 0.5863 - auroc: 0.5894 - val_loss: 0.5985 - val_auroc: 0.5661    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 3s - loss: 0.5859 - auroc: 0.5876 - val_loss: 0.5979 - val_auroc: 0.5709    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 3s - loss: 0.5874 - auroc: 0.5885 - val_loss: 0.6028 - val_auroc: 0.5701    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 3s - loss: 0.5861 - auroc: 0.5904 - val_loss: 0.6012 - val_auroc: 0.5673    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 3s - loss: 0.5864 - auroc: 0.5873 - val_loss: 0.5986 - val_auroc: 0.5714    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5743148326873779                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 6s - loss: 0.6859 - auroc: 0.5782 - val_loss: 0.5981 - val_auroc: 0.5780    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 4s - loss: 0.5874 - auroc: 0.5863 - val_loss: 0.6053 - val_auroc: 0.5690    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 5s - loss: 0.5870 - auroc: 0.5865 - val_loss: 0.5995 - val_auroc: 0.5704    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 5s - loss: 0.5871 - auroc: 0.5863 - val_loss: 0.5987 - val_auroc: 0.5740    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 5s - loss: 0.5874 - auroc: 0.5882 - val_loss: 0.6006 - val_auroc: 0.5685    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 5s - loss: 0.5876 - auroc: 0.5857 - val_loss: 0.6001 - val_auroc: 0.5732    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 5s - loss: 0.5875 - auroc: 0.5851 - val_loss: 0.6018 - val_auroc: 0.5681    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 5s - loss: 0.5873 - auroc: 0.5856 - val_loss: 0.6013 - val_auroc: 0.5702    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 5s - loss: 0.5871 - auroc: 0.5867 - val_loss: 0.5984 - val_auroc: 0.5760    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 5s - loss: 0.5876 - auroc: 0.5850 - val_loss: 0.6004 - val_auroc: 0.5782    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5781593918800354                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 9s - loss: 0.6212 - auroc: 0.5804 - val_loss: 0.6010 - val_auroc: 0.5722    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 7s - loss: 0.5894 - auroc: 0.5842 - val_loss: 0.6025 - val_auroc: 0.5625    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 6s - loss: 0.5898 - auroc: 0.5830 - val_loss: 0.6032 - val_auroc: 0.5635    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 6s - loss: 0.5890 - auroc: 0.5889 - val_loss: 0.6017 - val_auroc: 0.5818    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 6s - loss: 0.5871 - auroc: 0.6151 - val_loss: 0.5984 - val_auroc: 0.6052    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 6s - loss: 0.5854 - auroc: 0.6223 - val_loss: 0.5997 - val_auroc: 0.6033    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 6s - loss: 0.5843 - auroc: 0.6264 - val_loss: 0.5989 - val_auroc: 0.6046    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 6s - loss: 0.5835 - auroc: 0.6278 - val_loss: 0.5982 - val_auroc: 0.6120    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 6s - loss: 0.5826 - auroc: 0.6300 - val_loss: 0.5961 - val_auroc: 0.6098    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 6s - loss: 0.5836 - auroc: 0.6312 - val_loss: 0.5939 - val_auroc: 0.6178    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.6178091764450073                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 9s - loss: 0.9131 - auroc: 0.6161 - val_loss: 0.5994 - val_auroc: 0.6204    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 7s - loss: 0.5855 - auroc: 0.6392 - val_loss: 0.5973 - val_auroc: 0.6238    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 7s - loss: 0.5826 - auroc: 0.6441 - val_loss: 0.5949 - val_auroc: 0.6288    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 7s - loss: 0.5802 - auroc: 0.6477 - val_loss: 0.5901 - val_auroc: 0.6364    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 7s - loss: 0.5788 - auroc: 0.6509 - val_loss: 0.5893 - val_auroc: 0.6398    \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10                                                                     \n",
      " - 7s - loss: 0.5793 - auroc: 0.6521 - val_loss: 0.5914 - val_auroc: 0.6319    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 7s - loss: 0.5766 - auroc: 0.6537 - val_loss: 0.5905 - val_auroc: 0.6406    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 7s - loss: 0.5764 - auroc: 0.6549 - val_loss: 0.5907 - val_auroc: 0.6425    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 7s - loss: 0.5767 - auroc: 0.6559 - val_loss: 0.5892 - val_auroc: 0.6374    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 7s - loss: 0.5755 - auroc: 0.6575 - val_loss: 0.5885 - val_auroc: 0.6441    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.6441203951835632                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 5s - loss: 1.5429 - auroc: 0.5940 - val_loss: 0.7086 - val_auroc: 0.6174    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 3s - loss: 0.6255 - auroc: 0.6356 - val_loss: 0.6071 - val_auroc: 0.6294    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 3s - loss: 0.5904 - auroc: 0.6405 - val_loss: 0.5992 - val_auroc: 0.6261    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 3s - loss: 0.5863 - auroc: 0.6429 - val_loss: 0.5971 - val_auroc: 0.6263    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 3s - loss: 0.5846 - auroc: 0.6448 - val_loss: 0.5973 - val_auroc: 0.6273    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 4s - loss: 0.5845 - auroc: 0.6442 - val_loss: 0.5961 - val_auroc: 0.6335    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 3s - loss: 0.5823 - auroc: 0.6475 - val_loss: 0.5939 - val_auroc: 0.6317    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 3s - loss: 0.5818 - auroc: 0.6486 - val_loss: 0.5941 - val_auroc: 0.6327    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 3s - loss: 0.5811 - auroc: 0.6503 - val_loss: 0.5941 - val_auroc: 0.6284    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 3s - loss: 0.5806 - auroc: 0.6496 - val_loss: 0.5945 - val_auroc: 0.6317    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.6335114240646362                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 9s - loss: 0.6207 - auroc: 0.5768 - val_loss: 0.5984 - val_auroc: 0.5742    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 6s - loss: 0.5894 - auroc: 0.5832 - val_loss: 0.6025 - val_auroc: 0.5617    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 6s - loss: 0.5893 - auroc: 0.5840 - val_loss: 0.6015 - val_auroc: 0.5694    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 7s - loss: 0.5894 - auroc: 0.5829 - val_loss: 0.6010 - val_auroc: 0.5721    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 7s - loss: 0.5892 - auroc: 0.6004 - val_loss: 0.6003 - val_auroc: 0.6007    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 6s - loss: 0.5866 - auroc: 0.6195 - val_loss: 0.5973 - val_auroc: 0.6076    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 6s - loss: 0.5842 - auroc: 0.6245 - val_loss: 0.5948 - val_auroc: 0.6180    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 7s - loss: 0.5835 - auroc: 0.6259 - val_loss: 0.5977 - val_auroc: 0.6106    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 7s - loss: 0.5841 - auroc: 0.6269 - val_loss: 0.5963 - val_auroc: 0.6158    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 7s - loss: 0.5822 - auroc: 0.6308 - val_loss: 0.5957 - val_auroc: 0.6145    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.6179552674293518                                                             \n",
      "100%|██████████| 20/20 [16:25<00:00, 62.31s/it, best loss: -0.6477921009063721]\n",
      "Evalutation of best performing model:\n",
      "10000/10000 [==============================] - 1s 75us/step\n",
      "[0.5900108000636101, 0.6616052389144897]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'batch_size': 1, 'layers': 1, 'layers_1': 0.016156149080679794, 'layers_2': 0.07571845542061423, 'layers_3': 0.03973044032865311, 'lr': 0}\n"
     ]
    }
   ],
   "source": [
    "# new parameters to compare based on last results - see below\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=20,\n",
    "                                      trials=Trials(), \n",
    "                                      notebook_name='04_NeuMF_hyperparameter_tuning')\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test, batch_size = 100))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load( open(\"data/models/tuning/test_2.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of parameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualization of parameter combinationsbest_auc_1 = []\n",
    "\n",
    "for i in range(0, len(results)):\n",
    "    best_auc_1.append(results[i]['best_val_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6222817897796631,\n",
       " 0.6390849351882935,\n",
       " 0.6196146607398987,\n",
       " 0.5887677669525146,\n",
       " 0.6398872137069702,\n",
       " 0.6356604099273682,\n",
       " 0.6477921009063721,\n",
       " 0.6247851252555847,\n",
       " 0.6368501782417297,\n",
       " 0.5888638496398926,\n",
       " 0.572520911693573,\n",
       " 0.5740381479263306,\n",
       " 0.6167516708374023,\n",
       " 0.6424117088317871,\n",
       " 0.5743148326873779,\n",
       " 0.5781593918800354,\n",
       " 0.6178091764450073,\n",
       " 0.6441203951835632,\n",
       " 0.6335114240646362,\n",
       " 0.6179552674293518]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_auc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 256,\n",
       " 'layers': (256, 128, 64, 32),\n",
       " 'layers_1': 0.016156149080679794,\n",
       " 'layers_2': 0.07571845542061423,\n",
       " 'layers_3': 0.03973044032865311,\n",
       " 'lr': 0.001,\n",
       " 'val_loss': 0.5909921708901723,\n",
       " 'best_val_auc': 0.6477921009063721}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from numpy.random import seed\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow import set_random_seed\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pickle\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.sequence import pad_sequences\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import initializers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Embedding, Input, Dense, merge, Flatten, concatenate, multiply, dot, Reshape, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras.callbacks\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from time import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pdb\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from scipy import sparse\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sqlalchemy as db\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sqlalchemy import create_engine\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import psycopg2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import roc_auc_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'layers': hp.choice('layers', [[128,64,32,16] , [256, 128,64,32]]),\n",
      "        'reg_mlp': hp.uniform('reg_mlp', 0, 0.1),\n",
      "        'reg_mlp_1': hp.uniform('reg_mlp_1', 0, 0.1),\n",
      "        'reg_mlp_2': hp.uniform('reg_mlp_2', 0, 0.1),\n",
      "        'lr': hp.choice('lr', [0.001, 0.01, 0.0001]),\n",
      "        'batch_size': hp.choice('batch_size', [64, 128, 256]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: data_bundle = pd.read_pickle(\"training_data/sample_100000.pkl\")\n",
      "   3: max_length = 200\n",
      "   4: #data_bundle = data_bundle.dropna().reset_index()\n",
      "   5: # context\n",
      "   6: month = np.nan_to_num(data_bundle.month_enc.values.reshape(-1,1))\n",
      "   7: days_online = np.nan_to_num(data_bundle.days_online_log_std.values.reshape(-1,1))#, dtype = \"float32\")\n",
      "   8: # item\n",
      "   9: item_anbieter = np.nan_to_num(data_bundle.anbieterid_enc.values.reshape(-1,1)) # nur weil es noch Nan gab - fix ich noch\n",
      "  10: item_mkt= np.nan_to_num(data_bundle.anbietermarktplatz_enc.values.reshape(-1,1))\n",
      "  11: item_wg = np.nan_to_num(data_bundle.warengruppe_enc.values.reshape(-1,1))\n",
      "  12: item_preis = np.nan_to_num(data_bundle.preis_log_std.values.reshape(-1,1))\n",
      "  13: item_ve = np.nan_to_num(data_bundle.minve_log_std.values.reshape(-1,1))\n",
      "  14: list_text = []\n",
      "  15: list_text_user = []\n",
      "  16: for i in range(len(data_bundle)):\n",
      "  17:     list_text.append(data_bundle.text_vec[i])\n",
      "  18:     list_text_user.append(data_bundle.text_vec_user[i])\n",
      "  19: item_text = np.array(list_text, ndmin = 2)\n",
      "  20: \n",
      "  21: # user\n",
      "  22: user_text = np.array(list_text_user, ndmin = 2)\n",
      "  23: user_mkt = np.nan_to_num(data_bundle.usermkt_enc.values.reshape(-1,1))\n",
      "  24: \n",
      "  25: user_anbieter = pad_sequences(data_bundle.anbieterid_enc_user, maxlen = max_length, padding = \"pre\")\n",
      "  26: user_anbietermkt = pad_sequences(data_bundle.anbietermarktplatz_enc_user, maxlen = max_length, padding = \"pre\")\n",
      "  27: user_wg = pad_sequences(data_bundle.warengruppe_enc_user, maxlen = max_length, padding = \"pre\")\n",
      "  28: user_preis = np.nan_to_num(data_bundle.preis_log_std_user.values.reshape(-1,1))\n",
      "  29: user_ve = np.nan_to_num(data_bundle.minve_log_std_user.values.reshape(-1,1))\n",
      "  30: \n",
      "  31: # features \n",
      "  32: x_train = [month[:90000], days_online[:90000], item_anbieter[:90000], item_mkt[:90000], item_wg[:90000], item_preis[:90000], item_ve[:90000], item_text[:90000], user_mkt[:90000], user_anbietermkt[:90000], user_wg[:90000], user_anbieter[:90000], user_preis[:90000], user_ve[:90000], user_text[:90000]]\n",
      "  33: x_test = [month[90000:], days_online[90000:], item_anbieter[90000:], item_mkt[90000:], item_wg[90000:], item_preis[90000:], item_ve[90000:], item_text[90000:], user_mkt[90000:], user_anbietermkt[90000:], user_wg[90000:], user_anbieter[90000:], user_preis[90000:], user_ve[90000:], user_text[90000:]]\n",
      "  34: # target\n",
      "  35: y_train = data_bundle.pick.values.reshape(-1,1)[:90000]\n",
      "  36: y_test = data_bundle.pick.values.reshape(-1,1)[90000:]\n",
      "  37: \n",
      "  38: \n",
      "  39: \n",
      "  40: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "    1: def keras_fmin_fnct(space):\n",
      "    2: \n",
      "    3:     layers = space['layers']\n",
      "    4:     reg_mlp = space['reg_mlp']\n",
      "    5:     reg_lay = space['reg_mlp_1']\n",
      "    6:     reg_layers = [reg_mlp, reg_lay, reg_lay, reg_lay]\n",
      "    7:     reg_mf = space['reg_mlp_2']\n",
      "    8:     \n",
      "    9:     def mask_aware_mean(x):\n",
      "   10:     # recreate the masks - all zero rows have been masked\n",
      "   11:         mask = K.not_equal(K.sum(K.abs(x), axis=2, keepdims=True), 0)\n",
      "   12:         # number of that rows are not all zeros\n",
      "   13:         n = K.sum(K.cast(mask, 'float32'), axis=1, keepdims=False)\n",
      "   14:         # compute mask-aware mean of x\n",
      "   15:         x_mean = K.sum(x, axis=1, keepdims=False) / n\n",
      "   16:         return x_mean \n",
      "   17: \n",
      "   18:     lrelu = lambda x: tf.keras.activations.relu(x, alpha=0.1)\n",
      "   19:     \n",
      "   20:     def get_model(layers, reg_layers, reg_mf):\n",
      "   21:         assert len(layers) == len(reg_layers)\n",
      "   22:         num_layer = len(layers) #Number of layers in the MLP\n",
      "   23:         ### Input variables\n",
      "   24:         max_length = 200\n",
      "   25:         months = 13\n",
      "   26:         supplier = 563\n",
      "   27:         wgs = 230\n",
      "   28:         mkt = 9\n",
      "   29: \n",
      "   30:         months_emb = round(months ** 0.25)\n",
      "   31:         supplier_emb = round(supplier ** 0.25)\n",
      "   32:         wgs_emb = round(wgs ** 0.25)\n",
      "   33:         mkt_emb = round(mkt ** 0.25)\n",
      "   34:         \n",
      "   35:         # Context\n",
      "   36:         month = Input(shape = (1,), dtype = \"float32\", name = \"month\")\n",
      "   37:         day_online = Input(shape = (1,), dtype = \"float32\", name = \"days_online\")\n",
      "   38:         # Item\n",
      "   39:         item_anbieter = Input(shape = (1,), dtype = \"float32\", name = \"item_anbieter\")\n",
      "   40:         item_mkt = Input(shape = (1,), dtype = \"float32\", name = \"item_mkt\")\n",
      "   41:         item_wg = Input(shape = (1,), dtype = \"float32\", name = \"item_wg\")\n",
      "   42:         item_preis = Input(shape = (1,), dtype = \"float32\", name = \"item_preis\")\n",
      "   43:         item_ve = Input(shape = (1,), dtype = \"float32\", name = \"item_ve\")\n",
      "   44:         item_text = Input(shape = (150,), dtype = \"float32\", name = \"item_text\")\n",
      "   45: \n",
      "   46:         # User\n",
      "   47:         user_mkt = Input(shape = (1,), dtype = \"float32\", name = \"user_mkt\")\n",
      "   48:         user_anbieter = Input(shape = (max_length,), dtype = \"float32\", name = \"user_anbieter\")\n",
      "   49:         user_anbietermkt = Input(shape = (max_length,), dtype = \"float32\", name = \"user_anbietermkt\")\n",
      "   50:         user_wg = Input(shape = (max_length,), dtype = \"float32\", name = \"user_wg\")\n",
      "   51:         user_preis = Input(shape = (1,), dtype = \"float32\", name = \"user_preis\")\n",
      "   52:         user_ve = Input(shape = (1,), dtype = \"float32\", name = \"user_ve\")\n",
      "   53:         user_text = Input(shape = (150,), dtype = \"float32\", name = \"user_text\")\n",
      "   54: \n",
      "   55: \n",
      "   56: \n",
      "   57:         ### Embedding layer\n",
      "   58:         # MF\n",
      "   59:         # Context\n",
      "   60:         MF_Embedding_Context_month = Embedding(input_dim=months, output_dim=months_emb, name='mf_embedding_month',\n",
      "   61:                                       embeddings_initializer=initializers.random_normal(),\n",
      "   62:                                       embeddings_regularizer=l2(reg_mf), input_length=1)\n",
      "   63: \n",
      "   64:         # Item\n",
      "   65: \n",
      "   66:         MF_Embedding_Item_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mf_embedding_item_anbieter',\n",
      "   67:                                       embeddings_initializer=initializers.random_normal(),\n",
      "   68:                                       embeddings_regularizer=l2(reg_mf),input_length=1)\n",
      "   69:         MF_Embedding_Item_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mf_embedding_item_mkt',\n",
      "   70:                                       embeddings_initializer=initializers.random_normal(),\n",
      "   71:                                       embeddings_regularizer=l2(reg_mf), input_length=1)\n",
      "   72: \n",
      "   73:         MF_Embedding_Item_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mf_embedding_item_wg',\n",
      "   74:                                       embeddings_initializer=initializers.random_normal(),\n",
      "   75:                                       embeddings_regularizer=l2(reg_mf), input_length=1)\n",
      "   76: \n",
      "   77: \n",
      "   78:         # User\n",
      "   79: \n",
      "   80:         MF_Embedding_User_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mf_embedding_user_mkt',\n",
      "   81:                                           embeddings_initializer=initializers.random_normal(),\n",
      "   82:                                           embeddings_regularizer=l2(reg_mf),\n",
      "   83:                                           #mask_zero = True,\n",
      "   84:                                           input_length=1)\n",
      "   85:         MF_Embedding_User_anbietermkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mf_embedding_user_anbietermkt',\n",
      "   86:                                                   embeddings_initializer=initializers.random_normal(),\n",
      "   87:                                                   embeddings_regularizer=l2(reg_mf),\n",
      "   88:                                                   mask_zero = True,\n",
      "   89:                                                   input_length=max_length)\n",
      "   90:         MF_Embedding_User_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mf_embedding_user_wg',\n",
      "   91:                                          embeddings_initializer=initializers.random_normal(),\n",
      "   92:                                          embeddings_regularizer=l2(reg_mf),\n",
      "   93:                                          mask_zero = True,\n",
      "   94:                                          input_length=max_length)\n",
      "   95:         MF_Embedding_User_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mf_embedding_user_anbieter',\n",
      "   96:                                                embeddings_initializer=initializers.random_normal(),\n",
      "   97:                                                embeddings_regularizer=l2(reg_mf),\n",
      "   98:                                                mask_zero = True,\n",
      "   99:                                                input_length=max_length)\n",
      "  100: \n",
      "  101: \n",
      "  102:         # MLP\n",
      "  103:         #Context\n",
      "  104: \n",
      "  105:         MLP_Embedding_Context_month = Embedding(input_dim=months, output_dim=months_emb, name='mlp_embedding_month',\n",
      "  106:                                       embeddings_initializer=initializers.random_normal(),\n",
      "  107:                                       embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
      "  108: \n",
      "  109: \n",
      "  110:         # Item\n",
      "  111: \n",
      "  112:         MLP_Embedding_Item_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mlp_embedding_item_anbieter',\n",
      "  113:                                       embeddings_initializer=initializers.random_normal(),\n",
      "  114:                                       embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
      "  115:         MLP_Embedding_Item_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mlp_embedding_item_mkt',\n",
      "  116:                                       embeddings_initializer=initializers.random_normal(),\n",
      "  117:                                       embeddings_regularizer=l2(reg_layers[0]),  \n",
      "  118:                                            input_length=1)\n",
      "  119: \n",
      "  120:         MLP_Embedding_Item_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mlp_embedding_item_wg',\n",
      "  121:                                       embeddings_initializer=initializers.random_normal(),\n",
      "  122:                                       embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
      "  123: \n",
      "  124: \n",
      "  125:         # User\n",
      "  126: \n",
      "  127:         MLP_Embedding_User_mkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mlp_embedding_user_mkt',\n",
      "  128:                                           embeddings_initializer=initializers.random_normal(),\n",
      "  129:                                           embeddings_regularizer=l2(reg_layers[0]),\n",
      "  130:                                           #mask_zero = True,\n",
      "  131:                                           input_length=1)\n",
      "  132:         MLP_Embedding_User_anbietermkt = Embedding(input_dim=mkt, output_dim=mkt_emb, name='mlp_embedding_user_anbietermkt',\n",
      "  133:                                                   embeddings_initializer=initializers.random_normal(),\n",
      "  134:                                                   embeddings_regularizer=l2(reg_layers[0]),\n",
      "  135:                                                   mask_zero = True,\n",
      "  136:                                                   input_length=max_length)\n",
      "  137:         MLP_Embedding_User_wg = Embedding(input_dim=wgs, output_dim=wgs_emb, name='mlp_embedding_user_wg',\n",
      "  138:                                          embeddings_initializer=initializers.random_normal(),\n",
      "  139:                                          embeddings_regularizer=l2(reg_layers[0]),\n",
      "  140:                                          mask_zero = True,\n",
      "  141:                                          input_length=max_length)\n",
      "  142:         MLP_Embedding_User_anbieter = Embedding(input_dim=supplier, output_dim=supplier_emb, name='mlp_embedding_user_anbieter',\n",
      "  143:                                                embeddings_initializer=initializers.random_normal(),\n",
      "  144:                                                embeddings_regularizer=l2(reg_layers[0]),\n",
      "  145:                                                #mask_zero = True,\n",
      "  146:                                                input_length=max_length)\n",
      "  147: \n",
      "  148:         ## all user ones zb. and then concenate / multiply in MF \n",
      "  149:         # MF part\n",
      "  150:         ## context \n",
      "  151:         emb_item_month = Flatten(name = \"mf_flat_month\")(MF_Embedding_Context_month(month))\n",
      "  152: \n",
      "  153:         ## user\n",
      "  154:         emb_user_mkt = Flatten(name =\"mf_flat_user_mkt\")(MF_Embedding_User_mkt(user_mkt))\n",
      "  155:         #     emb_user_anbietermkt = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mf_avg_user_anbietermkt\"))(MF_Embedding_User_anbietermkt(user_anbietermkt))\n",
      "  156:         emb_user_wg = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mf_avg_user_wg\"))(MF_Embedding_User_wg(user_wg))\n",
      "  157:         emb_user_anbieter = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mf_avg_user_anbieter\"))(MF_Embedding_User_anbieter(user_anbieter))\n",
      "  158: \n",
      "  159:         ## item\n",
      "  160:         emb_item_anbieter = Flatten(name = \"mf_flat_item_anbieter\")(MF_Embedding_Item_anbieter(item_anbieter))\n",
      "  161:         emb_item_mkt = Flatten(name = \"mf_flat_item_mkt\")(MF_Embedding_Item_mkt(item_mkt))\n",
      "  162:         emb_item_wg = Flatten(name = \"mf_flat_item_wg\")(MF_Embedding_Item_wg(item_wg))\n",
      "  163: \n",
      "  164:         # MF connect\n",
      "  165:         mf_vector = multiply([concatenate([emb_user_mkt, emb_user_wg, emb_user_anbieter, user_preis, user_ve, user_text,\n",
      "  166:                                            day_online, emb_item_month], name = \"mf_user\"), \n",
      "  167:                               concatenate([emb_item_mkt, emb_item_wg, emb_item_anbieter, item_preis, item_ve, item_text,\n",
      "  168:                                            day_online, emb_item_month], name = \"mf_item\")], \n",
      "  169:                              name = \"mf_multiply\")\n",
      "  170: \n",
      "  171: \n",
      "  172:         # MLP part\n",
      "  173:         ## context \n",
      "  174:         emb_item_month2 = Flatten(name = \"mlp_flat_month\")(MLP_Embedding_Context_month(month))\n",
      "  175:         ## user\n",
      "  176:         emb_user_mkt2 = Flatten(name =\"mlp_flat_user_mkt\")(MLP_Embedding_User_mkt(user_mkt))\n",
      "  177:         emb_user_anbietermkt2 = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mlp_avg_user_anbietermkt\"))(MLP_Embedding_User_anbietermkt(user_anbietermkt))\n",
      "  178:         emb_user_wg2 = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mlp_avg_user_wg\"))(MLP_Embedding_User_wg(user_wg))\n",
      "  179:         emb_user_anbieter2 = (keras.layers.Lambda(lambda x: mask_aware_mean(x), name = \"mlp_avg_user_anbieten\"))(MLP_Embedding_User_anbieter(user_anbieter))\n",
      "  180: \n",
      "  181:         ## item\n",
      "  182:         emb_item_anbieter2 = Flatten(name = \"mlp_flat_item_anbieter\")(MLP_Embedding_Item_anbieter(item_anbieter))\n",
      "  183:         emb_item_mkt2 = Flatten(name = \"mlp_flat_item_mkt\")(MLP_Embedding_Item_mkt(item_mkt))\n",
      "  184:         emb_item_wg2 = Flatten(name = \"mlp_flat_item_wg\")(MLP_Embedding_Item_wg(item_wg))\n",
      "  185: \n",
      "  186:         mlp_vector = concatenate([emb_item_month2, day_online,\n",
      "  187:                                   emb_user_mkt2, emb_user_anbietermkt2, emb_user_wg2, emb_user_anbieter2, user_preis, user_ve, user_text,\n",
      "  188:                                   emb_item_anbieter2, emb_item_mkt2, emb_item_wg2, item_preis, item_ve, item_text], name = \"mlp_conc\")\n",
      "  189: \n",
      "  190:         for idx in range(1, num_layer):\n",
      "  191:             layer = Dense(layers[idx], kernel_regularizer=l2(reg_layers[idx]), activation=lrelu, name=\"layer%d\" % idx)\n",
      "  192:             mlp_vector = layer(mlp_vector)\n",
      "  193: \n",
      "  194:         # Concatenate MF and MLP parts\n",
      "  195:         predict_vector = concatenate([mf_vector, mlp_vector])\n",
      "  196: \n",
      "  197:         # Final prediction layer\n",
      "  198:         prediction = Dense(1, activation='sigmoid', kernel_initializer=initializers.lecun_normal(),\n",
      "  199:                            name=\"prediction\")(predict_vector)\n",
      "  200: \n",
      "  201:         model_ = Model(inputs=[month, day_online,\n",
      "  202:                                item_anbieter, item_mkt, item_wg, item_preis, item_ve, user_text,\n",
      "  203:                                user_mkt, user_anbietermkt, user_wg, user_anbieter, user_preis, user_ve, item_text],\n",
      "  204:                        outputs=prediction)\n",
      "  205: \n",
      "  206:         return model_\n",
      "  207: \n",
      "  208:     \n",
      "  209:     # load functino to evaluate performance      \n",
      "  210:     def auroc(y_true, y_pred):\n",
      "  211:         return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)\n",
      "  212:                   \n",
      "  213: \n",
      "  214: \n",
      "  215:     # create Model \n",
      "  216:     \n",
      "  217:     if 'results' not in globals():\n",
      "  218:         global results\n",
      "  219:         results = []\n",
      "  220:         \n",
      "  221:     model = get_model(layers, reg_layers, reg_mf)\n",
      "  222:     model.compile(optimizer=Adam(lr=space['lr']), loss='binary_crossentropy', metrics = [auroc])\n",
      "  223:     result = model.fit(x_train, y_train, batch_size = space['batch_size'], epochs = 10, verbose = 2, validation_split=0.1)\n",
      "  224:     validation_auc =np.amax(result.history['val_auroc'])\n",
      "  225:     \n",
      "  226:     valLoss = result.history['val_loss'][-1]\n",
      "  227:     parameters = space\n",
      "  228:     parameters[\"val_loss\"] = valLoss\n",
      "  229:     parameters[\"best_val_auc\"] = validation_auc\n",
      "  230:     results.append(parameters)\n",
      "  231:     with open(\"models/tuning/test_1.pkl\", 'wb') as fp:\n",
      "  232:         pickle.dump(results, fp)\n",
      "  233:     print('Best validation auc of epoch:', validation_auc)\n",
      "  234:     return {'loss': -validation_auc, 'status': STATUS_OK, 'model': model}\n",
      "  235: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 81000 samples, validate on 9000 samples    \n",
      "Epoch 1/10                                          \n",
      " - 4s - loss: 0.9583 - auroc: 0.5731 - val_loss: 0.5989 - val_auroc: 0.5723\n",
      "\n",
      "Epoch 2/10                                          \n",
      " - 3s - loss: 0.5870 - auroc: 0.5881 - val_loss: 0.5994 - val_auroc: 0.5693\n",
      "\n",
      "Epoch 3/10                                          \n",
      " - 3s - loss: 0.5876 - auroc: 0.5861 - val_loss: 0.6010 - val_auroc: 0.5714\n",
      "\n",
      "Epoch 4/10                                          \n",
      " - 3s - loss: 0.5872 - auroc: 0.5863 - val_loss: 0.6013 - val_auroc: 0.5674\n",
      "\n",
      "Epoch 5/10                                          \n",
      " - 3s - loss: 0.5874 - auroc: 0.5854 - val_loss: 0.6009 - val_auroc: 0.5723\n",
      "\n",
      "Epoch 6/10                                          \n",
      " - 4s - loss: 0.5873 - auroc: 0.5856 - val_loss: 0.6008 - val_auroc: 0.5683\n",
      "\n",
      "Epoch 7/10                                          \n",
      " - 4s - loss: 0.5875 - auroc: 0.5858 - val_loss: 0.5988 - val_auroc: 0.5756\n",
      "\n",
      "Epoch 8/10                                          \n",
      " - 4s - loss: 0.5871 - auroc: 0.5856 - val_loss: 0.6003 - val_auroc: 0.5718\n",
      "\n",
      "Epoch 9/10                                          \n",
      " - 4s - loss: 0.5874 - auroc: 0.5859 - val_loss: 0.5995 - val_auroc: 0.5708\n",
      "\n",
      "Epoch 10/10                                         \n",
      " - 3s - loss: 0.5876 - auroc: 0.5878 - val_loss: 0.5989 - val_auroc: 0.5730\n",
      "\n",
      "Best validation auc of epoch:                       \n",
      "0.5755667090415955                                  \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 4s - loss: 13.8816 - auroc: 0.4976 - val_loss: 10.0514 - val_auroc: 0.5034 \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 3s - loss: 7.6476 - auroc: 0.4898 - val_loss: 5.7079 - val_auroc: 0.5029   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 3s - loss: 4.4629 - auroc: 0.4918 - val_loss: 3.4552 - val_auroc: 0.5074   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 3s - loss: 2.7959 - auroc: 0.4973 - val_loss: 2.2613 - val_auroc: 0.5134   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 3s - loss: 1.8980 - auroc: 0.5045 - val_loss: 1.6034 - val_auroc: 0.5217   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 3s - loss: 1.3902 - auroc: 0.5138 - val_loss: 1.2192 - val_auroc: 0.5289   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 3s - loss: 1.0853 - auroc: 0.5233 - val_loss: 0.9821 - val_auroc: 0.5356   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 3s - loss: 0.8938 - auroc: 0.5321 - val_loss: 0.8315 - val_auroc: 0.5417   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 3s - loss: 0.7719 - auroc: 0.5412 - val_loss: 0.7362 - val_auroc: 0.5460   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 3s - loss: 0.6952 - auroc: 0.5485 - val_loss: 0.6773 - val_auroc: 0.5503   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.5502917170524597                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 5s - loss: 4.5922 - auroc: 0.4862 - val_loss: 2.4744 - val_auroc: 0.5267   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 5s - loss: 1.6696 - auroc: 0.5300 - val_loss: 1.1630 - val_auroc: 0.5610   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 5s - loss: 0.9429 - auroc: 0.5589 - val_loss: 0.8036 - val_auroc: 0.5743   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 5s - loss: 0.7238 - auroc: 0.5677 - val_loss: 0.6778 - val_auroc: 0.5724   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 0.6406 - auroc: 0.5662 - val_loss: 0.6271 - val_auroc: 0.5666   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 5s - loss: 0.6067 - auroc: 0.5674 - val_loss: 0.6070 - val_auroc: 0.5661   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 5s - loss: 0.5936 - auroc: 0.5727 - val_loss: 0.6001 - val_auroc: 0.5684   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 5s - loss: 0.5889 - auroc: 0.5779 - val_loss: 0.5979 - val_auroc: 0.5706   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 5s - loss: 0.5871 - auroc: 0.5817 - val_loss: 0.5972 - val_auroc: 0.5719   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 0.5863 - auroc: 0.5841 - val_loss: 0.5969 - val_auroc: 0.5733   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.5742740631103516                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 8s - loss: 0.9703 - auroc: 0.5614 - val_loss: 0.5973 - val_auroc: 0.5720   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 7s - loss: 0.5848 - auroc: 0.5912 - val_loss: 0.5971 - val_auroc: 0.5734   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 8s - loss: 0.5842 - auroc: 0.5923 - val_loss: 0.5969 - val_auroc: 0.5725   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 8s - loss: 0.5840 - auroc: 0.5950 - val_loss: 0.5976 - val_auroc: 0.5710   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 8s - loss: 0.5839 - auroc: 0.5949 - val_loss: 0.5970 - val_auroc: 0.5729   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 8s - loss: 0.5840 - auroc: 0.5937 - val_loss: 0.5979 - val_auroc: 0.5714   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 8s - loss: 0.5839 - auroc: 0.5947 - val_loss: 0.5975 - val_auroc: 0.5715   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 8s - loss: 0.5839 - auroc: 0.5957 - val_loss: 0.5971 - val_auroc: 0.5730   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 8s - loss: 0.5839 - auroc: 0.5946 - val_loss: 0.5973 - val_auroc: 0.5739   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 8s - loss: 0.5839 - auroc: 0.5949 - val_loss: 0.5969 - val_auroc: 0.5714   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.5738552212715149                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 5s - loss: 15.3528 - auroc: 0.4928 - val_loss: 10.6376 - val_auroc: 0.4897 \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 4s - loss: 7.7417 - auroc: 0.4966 - val_loss: 5.4578 - val_auroc: 0.5005   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 4s - loss: 4.0439 - auroc: 0.5059 - val_loss: 2.9423 - val_auroc: 0.5076   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 4s - loss: 2.2580 - auroc: 0.5144 - val_loss: 1.7356 - val_auroc: 0.5134   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 1.4027 - auroc: 0.5230 - val_loss: 1.1570 - val_auroc: 0.5190   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 4s - loss: 0.9900 - auroc: 0.5329 - val_loss: 0.8748 - val_auroc: 0.5267   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 4s - loss: 0.7864 - auroc: 0.5427 - val_loss: 0.7341 - val_auroc: 0.5343   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 4s - loss: 0.6843 - auroc: 0.5522 - val_loss: 0.6635 - val_auroc: 0.5405   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 4s - loss: 0.6331 - auroc: 0.5603 - val_loss: 0.6286 - val_auroc: 0.5462   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 0.6079 - auroc: 0.5662 - val_loss: 0.6119 - val_auroc: 0.5528   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.5528298616409302                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 9s - loss: 1.2103 - auroc: 0.5604 - val_loss: 0.5971 - val_auroc: 0.5694   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 9s - loss: 0.5848 - auroc: 0.5915 - val_loss: 0.5964 - val_auroc: 0.5739   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 8s - loss: 0.5841 - auroc: 0.5935 - val_loss: 0.5970 - val_auroc: 0.5718   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 8s - loss: 0.5840 - auroc: 0.5930 - val_loss: 0.5967 - val_auroc: 0.5740   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 8s - loss: 0.5839 - auroc: 0.5957 - val_loss: 0.5970 - val_auroc: 0.5712   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 8s - loss: 0.5839 - auroc: 0.5943 - val_loss: 0.5975 - val_auroc: 0.5698   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 8s - loss: 0.5839 - auroc: 0.5941 - val_loss: 0.5979 - val_auroc: 0.5715   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 8s - loss: 0.5838 - auroc: 0.5942 - val_loss: 0.5971 - val_auroc: 0.5739   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 8s - loss: 0.5840 - auroc: 0.5951 - val_loss: 0.5973 - val_auroc: 0.5726   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 8s - loss: 0.5840 - auroc: 0.5952 - val_loss: 0.5974 - val_auroc: 0.5721   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.5739582180976868                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 10s - loss: 2.1735 - auroc: 0.5517 - val_loss: 0.8795 - val_auroc: 0.6019  \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 9s - loss: 0.6912 - auroc: 0.6259 - val_loss: 0.6251 - val_auroc: 0.6245   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 9s - loss: 0.5999 - auroc: 0.6395 - val_loss: 0.6054 - val_auroc: 0.6273   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 9s - loss: 0.5907 - auroc: 0.6446 - val_loss: 0.6012 - val_auroc: 0.6300   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 9s - loss: 0.5880 - auroc: 0.6465 - val_loss: 0.5993 - val_auroc: 0.6309   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 9s - loss: 0.5862 - auroc: 0.6474 - val_loss: 0.5976 - val_auroc: 0.6310   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 9s - loss: 0.5848 - auroc: 0.6486 - val_loss: 0.5973 - val_auroc: 0.6300   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 9s - loss: 0.5838 - auroc: 0.6503 - val_loss: 0.5960 - val_auroc: 0.6321   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 9s - loss: 0.5827 - auroc: 0.6514 - val_loss: 0.5956 - val_auroc: 0.6318   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 9s - loss: 0.5820 - auroc: 0.6513 - val_loss: 0.5942 - val_auroc: 0.6374   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6373609304428101                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 5s - loss: 1.0146 - auroc: 0.6029 - val_loss: 0.6038 - val_auroc: 0.6229   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 4s - loss: 0.5867 - auroc: 0.6415 - val_loss: 0.5958 - val_auroc: 0.6292   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 4s - loss: 0.5827 - auroc: 0.6466 - val_loss: 0.5946 - val_auroc: 0.6379   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 4s - loss: 0.5810 - auroc: 0.6491 - val_loss: 0.5927 - val_auroc: 0.6353   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 0.5797 - auroc: 0.6503 - val_loss: 0.5922 - val_auroc: 0.6349   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 4s - loss: 0.5788 - auroc: 0.6520 - val_loss: 0.5916 - val_auroc: 0.6381   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 4s - loss: 0.5776 - auroc: 0.6539 - val_loss: 0.5919 - val_auroc: 0.6342   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 4s - loss: 0.5776 - auroc: 0.6533 - val_loss: 0.5885 - val_auroc: 0.6440   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 4s - loss: 0.5764 - auroc: 0.6551 - val_loss: 0.5892 - val_auroc: 0.6423   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 0.5755 - auroc: 0.6554 - val_loss: 0.5898 - val_auroc: 0.6407   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6440442800521851                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 7s - loss: 16.4943 - auroc: 0.5127 - val_loss: 7.7739 - val_auroc: 0.5074  \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 6s - loss: 4.4031 - auroc: 0.5062 - val_loss: 2.3108 - val_auroc: 0.5107   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 6s - loss: 1.5105 - auroc: 0.5149 - val_loss: 1.0203 - val_auroc: 0.5222   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 6s - loss: 0.8154 - auroc: 0.5316 - val_loss: 0.6989 - val_auroc: 0.5338   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 5s - loss: 0.6403 - auroc: 0.5472 - val_loss: 0.6199 - val_auroc: 0.5439   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 6s - loss: 0.5986 - auroc: 0.5593 - val_loss: 0.6030 - val_auroc: 0.5513   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 6s - loss: 0.5898 - auroc: 0.5682 - val_loss: 0.5999 - val_auroc: 0.5554   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10                                                                    \n",
      " - 6s - loss: 0.5878 - auroc: 0.5743 - val_loss: 0.5992 - val_auroc: 0.5579   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 6s - loss: 0.5869 - auroc: 0.5786 - val_loss: 0.5987 - val_auroc: 0.5602   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 5s - loss: 0.5863 - auroc: 0.5822 - val_loss: 0.5984 - val_auroc: 0.5618   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.5617768168449402                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                              \n",
      "Epoch 1/10                                                                    \n",
      " - 5s - loss: 4.1881 - auroc: 0.5080 - val_loss: 3.1008 - val_auroc: 0.5254   \n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " - 4s - loss: 2.4388 - auroc: 0.5232 - val_loss: 1.9142 - val_auroc: 0.5486   \n",
      "\n",
      "Epoch 3/10                                                                    \n",
      " - 4s - loss: 1.5812 - auroc: 0.5481 - val_loss: 1.3195 - val_auroc: 0.5734   \n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " - 4s - loss: 1.1453 - auroc: 0.5727 - val_loss: 1.0132 - val_auroc: 0.5889   \n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " - 4s - loss: 0.9169 - auroc: 0.5901 - val_loss: 0.8489 - val_auroc: 0.5997   \n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " - 4s - loss: 0.7909 - auroc: 0.6003 - val_loss: 0.7550 - val_auroc: 0.6052   \n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " - 4s - loss: 0.7167 - auroc: 0.6059 - val_loss: 0.6979 - val_auroc: 0.6056   \n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " - 4s - loss: 0.6705 - auroc: 0.6078 - val_loss: 0.6619 - val_auroc: 0.6036   \n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " - 4s - loss: 0.6409 - auroc: 0.6071 - val_loss: 0.6385 - val_auroc: 0.6022   \n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " - 4s - loss: 0.6219 - auroc: 0.6059 - val_loss: 0.6236 - val_auroc: 0.5990   \n",
      "\n",
      "Best validation auc of epoch:                                                 \n",
      "0.6056258678436279                                                            \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 5s - loss: 1.9784 - auroc: 0.5365 - val_loss: 0.6118 - val_auroc: 0.5528    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 4s - loss: 0.5886 - auroc: 0.5804 - val_loss: 0.5979 - val_auroc: 0.5665    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 4s - loss: 0.5847 - auroc: 0.5910 - val_loss: 0.5977 - val_auroc: 0.5702    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 4s - loss: 0.5841 - auroc: 0.5924 - val_loss: 0.5973 - val_auroc: 0.5741    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 4s - loss: 0.5836 - auroc: 0.5945 - val_loss: 0.5971 - val_auroc: 0.5747    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 4s - loss: 0.5835 - auroc: 0.5948 - val_loss: 0.5971 - val_auroc: 0.5756    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 4s - loss: 0.5835 - auroc: 0.5952 - val_loss: 0.5970 - val_auroc: 0.5742    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 4s - loss: 0.5834 - auroc: 0.5961 - val_loss: 0.5967 - val_auroc: 0.5774    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 4s - loss: 0.5835 - auroc: 0.5955 - val_loss: 0.5968 - val_auroc: 0.5754    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 4s - loss: 0.5834 - auroc: 0.5950 - val_loss: 0.5973 - val_auroc: 0.5759    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5773842334747314                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 7s - loss: 0.7352 - auroc: 0.5747 - val_loss: 0.6011 - val_auroc: 0.5616    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 5s - loss: 0.5891 - auroc: 0.5819 - val_loss: 0.6006 - val_auroc: 0.5694    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 6s - loss: 0.5888 - auroc: 0.5821 - val_loss: 0.6004 - val_auroc: 0.5716    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 6s - loss: 0.5892 - auroc: 0.5826 - val_loss: 0.5981 - val_auroc: 0.5697    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 6s - loss: 0.5893 - auroc: 0.5847 - val_loss: 0.6000 - val_auroc: 0.5732    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 6s - loss: 0.5893 - auroc: 0.5830 - val_loss: 0.6012 - val_auroc: 0.5714    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 6s - loss: 0.5889 - auroc: 0.5828 - val_loss: 0.6022 - val_auroc: 0.5656    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 6s - loss: 0.5890 - auroc: 0.5825 - val_loss: 0.6014 - val_auroc: 0.5642    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 6s - loss: 0.5892 - auroc: 0.5828 - val_loss: 0.5999 - val_auroc: 0.5717    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 6s - loss: 0.5894 - auroc: 0.5822 - val_loss: 0.6021 - val_auroc: 0.5565    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5731910467147827                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 7s - loss: 8.6226 - auroc: 0.4969 - val_loss: 4.6052 - val_auroc: 0.5067    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 6s - loss: 2.9622 - auroc: 0.4990 - val_loss: 1.8868 - val_auroc: 0.5183    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 6s - loss: 1.4054 - auroc: 0.5139 - val_loss: 1.0771 - val_auroc: 0.5304    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 6s - loss: 0.8990 - auroc: 0.5301 - val_loss: 0.7787 - val_auroc: 0.5412    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 6s - loss: 0.7021 - auroc: 0.5470 - val_loss: 0.6603 - val_auroc: 0.5489    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 6s - loss: 0.6258 - auroc: 0.5593 - val_loss: 0.6174 - val_auroc: 0.5551    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 6s - loss: 0.5990 - auroc: 0.5690 - val_loss: 0.6035 - val_auroc: 0.5582    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 6s - loss: 0.5904 - auroc: 0.5751 - val_loss: 0.5996 - val_auroc: 0.5594    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 5s - loss: 0.5877 - auroc: 0.5796 - val_loss: 0.5985 - val_auroc: 0.5618    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 6s - loss: 0.5868 - auroc: 0.5822 - val_loss: 0.5982 - val_auroc: 0.5632    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.563234269618988                                                              \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 12s - loss: 6.2640 - auroc: 0.4920 - val_loss: 1.5332 - val_auroc: 0.5056   \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 10s - loss: 0.9043 - auroc: 0.5072 - val_loss: 0.6549 - val_auroc: 0.5231   \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 10s - loss: 0.6093 - auroc: 0.5361 - val_loss: 0.6029 - val_auroc: 0.5406   \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 10s - loss: 0.5906 - auroc: 0.5582 - val_loss: 0.6000 - val_auroc: 0.5512   \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 9s - loss: 0.5882 - auroc: 0.5722 - val_loss: 0.5992 - val_auroc: 0.5583    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 9s - loss: 0.5870 - auroc: 0.5786 - val_loss: 0.5988 - val_auroc: 0.5627    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 10s - loss: 0.5862 - auroc: 0.5828 - val_loss: 0.5983 - val_auroc: 0.5646   \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 10s - loss: 0.5856 - auroc: 0.5867 - val_loss: 0.5980 - val_auroc: 0.5670   \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 10s - loss: 0.5851 - auroc: 0.5879 - val_loss: 0.5977 - val_auroc: 0.5684   \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 10s - loss: 0.5847 - auroc: 0.5901 - val_loss: 0.5975 - val_auroc: 0.5694   \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5693525671958923                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 6s - loss: 0.6691 - auroc: 0.5984 - val_loss: 0.6035 - val_auroc: 0.5897    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 5s - loss: 0.5895 - auroc: 0.6148 - val_loss: 0.5976 - val_auroc: 0.6220    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 5s - loss: 0.5928 - auroc: 0.6283 - val_loss: 0.6075 - val_auroc: 0.6314    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 5s - loss: 0.5899 - auroc: 0.6331 - val_loss: 0.6023 - val_auroc: 0.6244    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 4s - loss: 0.5884 - auroc: 0.6355 - val_loss: 0.6091 - val_auroc: 0.6292    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 4s - loss: 0.5842 - auroc: 0.6358 - val_loss: 0.5947 - val_auroc: 0.6319    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 4s - loss: 0.5852 - auroc: 0.6369 - val_loss: 0.5960 - val_auroc: 0.6237    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 4s - loss: 0.5865 - auroc: 0.6388 - val_loss: 0.5948 - val_auroc: 0.6290    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 4s - loss: 0.5872 - auroc: 0.6391 - val_loss: 0.5928 - val_auroc: 0.6344    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 4s - loss: 0.5851 - auroc: 0.6386 - val_loss: 0.5941 - val_auroc: 0.6338    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.6344467997550964                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 8s - loss: 1.0248 - auroc: 0.5740 - val_loss: 0.6009 - val_auroc: 0.5683    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 6s - loss: 0.5864 - auroc: 0.5864 - val_loss: 0.5970 - val_auroc: 0.5727    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 6s - loss: 0.5843 - auroc: 0.5938 - val_loss: 0.5963 - val_auroc: 0.5744    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 6s - loss: 0.5839 - auroc: 0.5937 - val_loss: 0.5967 - val_auroc: 0.5753    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 6s - loss: 0.5838 - auroc: 0.5964 - val_loss: 0.5967 - val_auroc: 0.5737    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 6s - loss: 0.5837 - auroc: 0.5946 - val_loss: 0.5972 - val_auroc: 0.5743    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 6s - loss: 0.5836 - auroc: 0.5947 - val_loss: 0.5971 - val_auroc: 0.5755    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 6s - loss: 0.5835 - auroc: 0.5963 - val_loss: 0.5970 - val_auroc: 0.5733    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 6s - loss: 0.5836 - auroc: 0.5952 - val_loss: 0.5971 - val_auroc: 0.5742    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 6s - loss: 0.5836 - auroc: 0.5947 - val_loss: 0.5973 - val_auroc: 0.5731    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5755028128623962                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 8s - loss: 2.5891 - auroc: 0.5354 - val_loss: 0.5971 - val_auroc: 0.5717    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 6s - loss: 0.5855 - auroc: 0.5858 - val_loss: 0.5964 - val_auroc: 0.5720    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 6s - loss: 0.5843 - auroc: 0.5910 - val_loss: 0.5964 - val_auroc: 0.5770    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 6s - loss: 0.5839 - auroc: 0.5937 - val_loss: 0.5964 - val_auroc: 0.5753    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 6s - loss: 0.5837 - auroc: 0.5941 - val_loss: 0.5969 - val_auroc: 0.5740    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 6s - loss: 0.5836 - auroc: 0.5945 - val_loss: 0.5966 - val_auroc: 0.5750    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 6s - loss: 0.5836 - auroc: 0.5947 - val_loss: 0.5969 - val_auroc: 0.5736    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 6s - loss: 0.5837 - auroc: 0.5951 - val_loss: 0.5970 - val_auroc: 0.5738    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 6s - loss: 0.5836 - auroc: 0.5954 - val_loss: 0.5975 - val_auroc: 0.5721    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 6s - loss: 0.5836 - auroc: 0.5951 - val_loss: 0.5972 - val_auroc: 0.5722    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5770454406738281                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 6s - loss: 0.8829 - auroc: 0.5737 - val_loss: 0.5982 - val_auroc: 0.5769    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 4s - loss: 0.5871 - auroc: 0.5855 - val_loss: 0.5993 - val_auroc: 0.5758    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 4s - loss: 0.5871 - auroc: 0.5872 - val_loss: 0.5995 - val_auroc: 0.5669    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 4s - loss: 0.5872 - auroc: 0.5860 - val_loss: 0.5975 - val_auroc: 0.5748    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 4s - loss: 0.5877 - auroc: 0.5845 - val_loss: 0.5982 - val_auroc: 0.5717    \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10                                                                     \n",
      " - 4s - loss: 0.5869 - auroc: 0.5856 - val_loss: 0.6012 - val_auroc: 0.5672    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 4s - loss: 0.5870 - auroc: 0.5870 - val_loss: 0.6010 - val_auroc: 0.5694    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 4s - loss: 0.5869 - auroc: 0.5862 - val_loss: 0.5985 - val_auroc: 0.5732    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 4s - loss: 0.5871 - auroc: 0.5861 - val_loss: 0.5988 - val_auroc: 0.5722    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 4s - loss: 0.5873 - auroc: 0.5869 - val_loss: 0.5989 - val_auroc: 0.5764    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5768840312957764                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 6s - loss: 0.7277 - auroc: 0.5777 - val_loss: 0.5990 - val_auroc: 0.5727    \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 4s - loss: 0.5870 - auroc: 0.5860 - val_loss: 0.5997 - val_auroc: 0.5671    \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 4s - loss: 0.5870 - auroc: 0.5860 - val_loss: 0.5994 - val_auroc: 0.5736    \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 4s - loss: 0.5875 - auroc: 0.5860 - val_loss: 0.5987 - val_auroc: 0.5702    \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 4s - loss: 0.5873 - auroc: 0.5849 - val_loss: 0.5983 - val_auroc: 0.5717    \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 4s - loss: 0.5877 - auroc: 0.5854 - val_loss: 0.6002 - val_auroc: 0.5719    \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 4s - loss: 0.5871 - auroc: 0.5861 - val_loss: 0.5981 - val_auroc: 0.5729    \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 4s - loss: 0.5876 - auroc: 0.5854 - val_loss: 0.6017 - val_auroc: 0.5693    \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 4s - loss: 0.5879 - auroc: 0.5855 - val_loss: 0.5983 - val_auroc: 0.5734    \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 4s - loss: 0.5870 - auroc: 0.5865 - val_loss: 0.6024 - val_auroc: 0.5675    \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5735912919044495                                                             \n",
      "Train on 81000 samples, validate on 9000 samples                               \n",
      "Epoch 1/10                                                                     \n",
      " - 13s - loss: 0.6676 - auroc: 0.5748 - val_loss: 0.6148 - val_auroc: 0.5646   \n",
      "\n",
      "Epoch 2/10                                                                     \n",
      " - 12s - loss: 0.5925 - auroc: 0.5794 - val_loss: 0.6063 - val_auroc: 0.5549   \n",
      "\n",
      "Epoch 3/10                                                                     \n",
      " - 11s - loss: 0.5916 - auroc: 0.5814 - val_loss: 0.6050 - val_auroc: 0.5666   \n",
      "\n",
      "Epoch 4/10                                                                     \n",
      " - 11s - loss: 0.5920 - auroc: 0.5794 - val_loss: 0.6042 - val_auroc: 0.5640   \n",
      "\n",
      "Epoch 5/10                                                                     \n",
      " - 11s - loss: 0.5917 - auroc: 0.5777 - val_loss: 0.6054 - val_auroc: 0.5596   \n",
      "\n",
      "Epoch 6/10                                                                     \n",
      " - 11s - loss: 0.5916 - auroc: 0.5775 - val_loss: 0.6036 - val_auroc: 0.5664   \n",
      "\n",
      "Epoch 7/10                                                                     \n",
      " - 11s - loss: 0.5914 - auroc: 0.5796 - val_loss: 0.6004 - val_auroc: 0.5737   \n",
      "\n",
      "Epoch 8/10                                                                     \n",
      " - 11s - loss: 0.5908 - auroc: 0.5800 - val_loss: 0.6078 - val_auroc: 0.5622   \n",
      "\n",
      "Epoch 9/10                                                                     \n",
      " - 12s - loss: 0.5919 - auroc: 0.5778 - val_loss: 0.6045 - val_auroc: 0.5643   \n",
      "\n",
      "Epoch 10/10                                                                    \n",
      " - 12s - loss: 0.5904 - auroc: 0.5803 - val_loss: 0.6040 - val_auroc: 0.5666   \n",
      "\n",
      "Best validation auc of epoch:                                                  \n",
      "0.5737336277961731                                                             \n",
      "100%|██████████| 20/20 [21:05<00:00, 78.38s/it, best loss: -0.6440442800521851]\n",
      "Evalutation of best performing model:\n",
      "10000/10000 [==============================] - 1s 71us/step\n",
      "[0.5886389726400375, 0.6554206609725952]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'batch_size': 2, 'layers': 1, 'lr': 0, 'reg_mlp': 0.025898591582998898, 'reg_mlp_1': 0.00821336141287018, 'reg_mlp_2': 0.0019408999555612595}\n"
     ]
    }
   ],
   "source": [
    "# new parameters to compare based on last results - see below\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=20,\n",
    "                                      trials=Trials(), \n",
    "                                      notebook_name='04_NeuMF_hyperparameter_tuning')\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test, batch_size = 100))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load( open(\"data/models/tuning/test_1.pkl\", \"rb\" ) ) # didnt save.. :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 256,\n",
       " 'layers': (256, 128, 64, 32),\n",
       " 'lr': 0.001,\n",
       " 'reg_mlp': 0.025898591582998898,\n",
       " 'reg_mlp_1': 0.00821336141287018,\n",
       " 'reg_mlp_2': 0.0019408999555612595,\n",
       " 'val_loss': 0.5897670127285851,\n",
       " 'best_val_auc': 0.6440442800521851}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of parameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc = []\n",
    "for i in range(0, len(results)):\n",
    "    best_auc.append(results[i]['best_val_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = []\n",
    "for i in range(0, len(results)):\n",
    "    learning_rate.append(results[i]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = []\n",
    "for i in range(0, len(results)):\n",
    "    batch_size.append(results[i]['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_mlp = []\n",
    "for i in range(0, len(results)):\n",
    "    reg_mlp.append(results[i]['reg_mlp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('experiment')\n",
    "ax1.set_ylabel('auc', color=color)\n",
    "ax1.plot( best_auc, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('compare', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(batch_size, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is sghtly clipped\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.plot(best_auc)\n",
    "plt.plot(learning_rate)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.plot(best_auc)\n",
    "plt.plot(learning_rate)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(batch_size)\n",
    "#plt.plot(reg_mlp)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
